{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Informe_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2I49tQOnnz8"
      },
      "source": [
        "#Informe final de carrera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS4uYMZzoT0T"
      },
      "source": [
        "*** Santiago Restrepo Alzate - DS-ONLINE-20 ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHskN9-7sPQm"
      },
      "source": [
        "*** Este notebook se puede acceder en github: DS_Informe_final.ipynb ***\n",
        "---\n",
        "https://github.com/santyres/Acamica-DS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf6M_MlBobiZ"
      },
      "source": [
        "Aplicación de NLP para predicción de sentimientos.\n",
        "\n",
        "Este informe final tiene como objetivo investigar y experimentar técnicas alternativas para abordar el tema de clasificación de las reviews de productos en positivas o negativas (clasificación binaria) y responder a las preguntas que nos plantearemos para desarrollar el informe.\n",
        "\n",
        "Tomaremos como punto de partida el modelo entregado en el proyecto 3 del curso, en el cual usamos algoritmos de Machine Learning para entrenar un clasificador de sentimientos para las reviews de productos.  A partir de este punto evaluaremos nuevas aproximaciones para dar respuesta a las siguientes preguntas:\n",
        "\n",
        "1. Cuáles otras transformaciones o estrategias de preprocesamiento de datos podrían mejorar el desempeño de nuestro modelo?\n",
        "\n",
        "2. Este problema de clasificación se puede abordar con otras técnicas como son las redes neuronales o los word embeddings?\n",
        "\n",
        "Lo que esperamos es que, resolviendo las dos preguntas anteriores, podamos obtener un modelo equivalente o mejor que el modelo obtenido en el proyecto 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjgFccG4UaW7"
      },
      "source": [
        "#Primer modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H-6ARoHU_ox"
      },
      "source": [
        "Partiremos del modelo obtenido en el proyecto 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw-8yd0Onccp",
        "outputId": "2518e7b0-a3ee-426a-b201-bec8d9127c5e"
      },
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "from random import randint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import gensim\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO60SV2rVQN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9e0ab1-87e3-4ad6-9a60-9fac3b686da7"
      },
      "source": [
        "#Instalar paquete de lenguaje español para la librería spacy\n",
        "!python -m spacy download es_core_news_md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.2.5/es_core_news_md-2.2.5.tar.gz#egg=es_core_news_md==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rik5XEf-VdLZ"
      },
      "source": [
        "#Importar paquete de lenguaje en español para la librería spacy\n",
        "import es_core_news_md\n",
        "nlp = es_core_news_md.load()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7C1WAkZVpnq",
        "outputId": "6e50307a-cb28-4e9d-c1da-a98576c1bae0"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "S0CrjYm3VyDT",
        "outputId": "6a6eb970-821b-46d0-f165-a0c7ee004ecb"
      },
      "source": [
        "#Cargar datos de entrenamiento\n",
        "df = pd.read_json(\"dataset_es_train.json\", lines=True)\n",
        "\n",
        "#reagrupar las estrellas (calificaciones) para generar las etiquetas positivas y negativas\n",
        "df = df[df.stars != 3]\n",
        "\n",
        "#Crear una nueva columna con el sentimiento (1:positivo o 0:negativo) para redefinir la etiqueta para el modelo\n",
        "df[\"sentimiento\"] = df[\"stars\"].map(lambda x : \"0\" if (x <= 2) else \"1\",na_action=None)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>reviewer_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_title</th>\n",
              "      <th>language</th>\n",
              "      <th>product_category</th>\n",
              "      <th>sentimiento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>es_0491108</td>\n",
              "      <td>product_es_0296024</td>\n",
              "      <td>reviewer_es_0999081</td>\n",
              "      <td>1</td>\n",
              "      <td>Nada bueno se me fue ka pantalla en menos de 8...</td>\n",
              "      <td>television Nevir</td>\n",
              "      <td>es</td>\n",
              "      <td>electronics</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>es_0869872</td>\n",
              "      <td>product_es_0922286</td>\n",
              "      <td>reviewer_es_0216771</td>\n",
              "      <td>1</td>\n",
              "      <td>Horrible, nos tuvimos que comprar otro porque ...</td>\n",
              "      <td>Dinero tirado a la basura con esta compra</td>\n",
              "      <td>es</td>\n",
              "      <td>electronics</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>es_0811721</td>\n",
              "      <td>product_es_0474543</td>\n",
              "      <td>reviewer_es_0929213</td>\n",
              "      <td>1</td>\n",
              "      <td>Te obligan a comprar dos unidades y te llega s...</td>\n",
              "      <td>solo llega una unidad cuando te obligan a comp...</td>\n",
              "      <td>es</td>\n",
              "      <td>drugstore</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>es_0359921</td>\n",
              "      <td>product_es_0656090</td>\n",
              "      <td>reviewer_es_0224702</td>\n",
              "      <td>1</td>\n",
              "      <td>No entro en descalificar al vendedor, solo pue...</td>\n",
              "      <td>PRODUCTO NO RECIBIDO.</td>\n",
              "      <td>es</td>\n",
              "      <td>wireless</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>es_0068940</td>\n",
              "      <td>product_es_0662544</td>\n",
              "      <td>reviewer_es_0224827</td>\n",
              "      <td>1</td>\n",
              "      <td>Llega tarde y co la talla equivocada</td>\n",
              "      <td>Devuelto</td>\n",
              "      <td>es</td>\n",
              "      <td>shoes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    review_id          product_id  ... product_category  sentimiento\n",
              "0  es_0491108  product_es_0296024  ...      electronics            0\n",
              "1  es_0869872  product_es_0922286  ...      electronics            0\n",
              "2  es_0811721  product_es_0474543  ...        drugstore            0\n",
              "3  es_0359921  product_es_0656090  ...         wireless            0\n",
              "4  es_0068940  product_es_0662544  ...            shoes            0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaXJi-vWnVMQ"
      },
      "source": [
        "#método para generar la matriz de confusión\n",
        "def confusion(ytest,y_pred):\n",
        "    names=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
        "    cm=confusion_matrix(ytest,y_pred)\n",
        "    f,ax=plt.subplots(figsize=(5,5))\n",
        "    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n",
        "    plt.xlabel(\"y_pred\")\n",
        "    plt.ylabel(\"y_true\")\n",
        "    ax.set_xticklabels(names)\n",
        "    ax.set_yticklabels(names)\n",
        "    plt.show()\n",
        "\n",
        "#Metodo para normalizar los datos\n",
        "def normalizar_datos(df, stopwords_exceptions, num_filtered_chars):\n",
        "  stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "  # stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "\n",
        "  #Elimina las excepciones que se quieren hacer sobre la lista de stopwords\n",
        "  for w in stopwords_exceptions:\n",
        "    stopwords.remove(w)\n",
        "    print(f\"palabra '{w}' eliminada de la lista de stopwords!\")\n",
        "\n",
        "  palabras_lemma = []\n",
        "\n",
        "  #Realizar tokenizado y lemmatizado para cada review, se optimiza el pipeline de spacy para mejorar el performance.\n",
        "  for doc in nlp.pipe(list(df.review_body), disable=[\"tagger\", \"parser\",\"ner\",\"textcat\",\"custom\"]): #recorre reviews\n",
        "    #recorre los tokens para extraer lemmas de las palabras\n",
        "    lemma = [token.lemma_.lower() for token in doc if token.text not in stopwords #elimina stopwords\n",
        "                                                  if token.text.isalpha() #elimina numeros y puntuación\n",
        "                                                  if len(token.text) > num_filtered_chars] #elimina palabras con menos de 3 caracteres\n",
        "    lemma = \" \".join(lemma)\n",
        "    palabras_lemma.append(lemma)\n",
        "\n",
        "  df[\"review_body_lemma\"] = palabras_lemma\n",
        "  return df\n",
        "\n",
        "#Metodo para vectorizar los datos y entregar los set de datos de train y test\n",
        "def train_test_vectorized(df):\n",
        "  # Usaremos solo las 2000 palabras con mas frecuencia en todo el corpus para generar los vectores, para optimizar la memoria\n",
        "  max_features=2000\n",
        "  vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "  reviews = list(df.review_body_lemma.values)\n",
        "  stars = df.sentimiento.values\n",
        "\n",
        "  X = vectorizer.fit_transform(reviews)\n",
        "\n",
        "  x = X.toarray()\n",
        "  y = stars\n",
        "\n",
        "  #Creamos los conjuntos de train y test para el entrenamiento del modelo\n",
        "  return train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)\n",
        "\n",
        "#Metodo para vectorizar los datos usando 1-gram y 2-gram y entregar los set de datos de train y test\n",
        "def train_test_vectorized_2grams(df):\n",
        "  # Usaremos solo las 2000 palabras con mas frecuencia en todo el corpus para generar los vectores, para optimizar la memoria\n",
        "  max_features=2000\n",
        "  vectorizer = TfidfVectorizer(max_features=max_features, analyzer='word', ngram_range=(1, 2)) #generar vectores con 1 gram y 2 gram\n",
        "\n",
        "  reviews = list(df.review_body_lemma.values)\n",
        "  stars = df.sentimiento.values\n",
        "\n",
        "  X = vectorizer.fit_transform(reviews)\n",
        "\n",
        "  x = X.toarray()\n",
        "  y = stars\n",
        "\n",
        "  #Creamos los conjuntos de train y test para el entrenamiento del modelo\n",
        "  return train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "48tXgAoz6NhE",
        "outputId": "cec41d65-aaf7-4037-b06f-1b94a420af41"
      },
      "source": [
        "#Normalizar datos (sin exlcuir palabras del conjunto de stopwords y sacando las palabras que además tengas 3 o menos caracteres)\n",
        "df = normalizar_datos(df, \"\", 3)\n",
        "#Vectorizar datos y generar el set de datos de train y test\n",
        "xtrain, xtest, ytrain, ytest = train_test_vectorized(df)\n",
        "\n",
        "#Entrenar modelo\n",
        "from sklearn.svm import LinearSVC\n",
        "svc = LinearSVC(C = 0.1, random_state=42, loss='squared_hinge')\n",
        "svc.fit(xtrain,ytrain)\n",
        "print(\"Score: \"+str(svc.score(xtest,ytest))+'\\n')\n",
        "y_pred = svc.predict(xtest)\n",
        "confusion(ytest,y_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.8618125\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mumsqWA8D8fu"
      },
      "source": [
        "Score del modelo seleccionado en el proyecto 3: 0.861"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QgD0NNEHAZ"
      },
      "source": [
        "#Pregunta 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHx1qTkVQxjW"
      },
      "source": [
        "1. Cuáles otras transformaciones o estrategias de preprocesamiento de datos podrían mejorar el desempeño de nuestro modelo?\n",
        "\n",
        "  1.1 **Adicionar el título al texto de cada review**. Los titulos de los reviews pueden contener palabras claves para el clasificador, pueden ser mas concretos a la hora de describir el sentimiento. Si agregamos estas palabras al corpus de los reviews, el clasificador podría ajustar mejor sus patrones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "cCceVT2tgoOc",
        "outputId": "18fd46b3-f8b1-4cba-9d32-89c2a1a8972a"
      },
      "source": [
        "reviews_sin_titulo = df.review_body.copy()\n",
        "\n",
        "#Adicionar el titulo al cuerpo de cada revisión.\n",
        "df[\"review_body\"] = df[\"review_title\"]  + \", \" + df[\"review_body\"]\n",
        "\n",
        "reviews_con_titulo = df.review_body\n",
        "\n",
        "#Normalizar datos (sin exlcuir palabras del conjunto de stopwords y sacando las palabras que además tengan 3 o menos caracteres)\n",
        "df = normalizar_datos(df, \"\", 3)\n",
        "xtrain, xtest, ytrain, ytest = train_test_vectorized(df)\n",
        "\n",
        "#Entrenamos nuevamente el modelo LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "svc = LinearSVC(C = 0.1, random_state=42, loss='squared_hinge')\n",
        "svc.fit(xtrain,ytrain)\n",
        "print(\"Score: \"+str(svc.score(xtest,ytest))+'\\n')\n",
        "y_pred = svc.predict(xtest)\n",
        "confusion(ytest,y_pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.8961875\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7C-l56-DO7"
      },
      "source": [
        "Nota: logramos subir unos puntos en el accuracy (de 0.86 a 0.89) del modelo agregando los títulos a cada review, miremos cuantas palabras nuevas agregamos al modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcSI65uK-Pyw",
        "outputId": "43183735-a456-40b2-d0b1-b62892fe7d03"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "#Calculemos las frecencias para las reviews sin el titulo agregado\n",
        "palabras_reviews_sin_titulo = []\n",
        "lemmas = []\n",
        "\n",
        "#Tokenizar y lemmatizar\n",
        "for doc in nlp.pipe(list(reviews_sin_titulo), disable=[\"tagger\", \"parser\",\"ner\",\"textcat\",\"custom\"]): #recorre reviews\n",
        "  lemma = [token.lemma_.lower() for token in doc if token.text not in stopwords #elimina stopwords\n",
        "                                                  if token.text.isalpha() #elimina numeros y puntuación\n",
        "                                                  if len(token.text) > 3] #elimina palabras con menos de 3 caracteres\n",
        "  lemmas.append(lemma)\n",
        "palabras_reviews_sin_titulo = list(itertools.chain(*lemmas))\n",
        "\n",
        "#calcula la frecuencia de aparición de cada palabra\n",
        "freq = nltk.FreqDist(palabras_reviews_sin_titulo)\n",
        "df_palabras_reviews_sin_titulo = pd.DataFrame(list(freq.items()), columns = [\"palabras\",\"frecuencia\"])\n",
        "df_palabras_reviews_sin_titulo.sort_values(\"frecuencia\",ascending=False,inplace=True)\n",
        "\n",
        "#Calculemos las frecuencias para las reviews con el titulo agregado\n",
        "palabras_reviews_con_titulo = []\n",
        "lemmas = []\n",
        "\n",
        "#Tokenizar y lemmatizar\n",
        "for doc in nlp.pipe(list(reviews_con_titulo), disable=[\"tagger\", \"parser\",\"ner\",\"textcat\",\"custom\"]): #recorre reviews\n",
        "  #recorre los tokens para extraer lemmas de las palabras\n",
        "  lemma = [token.lemma_.lower() for token in doc if token.text not in stopwords #elimina stopwords\n",
        "                                                  if token.text.isalpha() #elimina numeros y puntuación\n",
        "                                                  if len(token.text) > 3] #elimina palabras con menos de 3 caracteres\n",
        "  lemmas.append(lemma)\n",
        "palabras_reviews_con_titulo = list(itertools.chain(*lemmas))\n",
        "\n",
        "#calcula la frecuencia de aparición de cada palabra\n",
        "freq = nltk.FreqDist(palabras_reviews_con_titulo)\n",
        "df_palabras_reviews_con_titulo = pd.DataFrame(list(freq.items()), columns = [\"palabras\",\"frecuencia\"])\n",
        "df_palabras_reviews_con_titulo.sort_values(\"frecuencia\",ascending=False,inplace=True)\n",
        "\n",
        "print(f\"Con la inclusión del título en el texto del review logramos incrementar el corpus de palabras en {len(df_palabras_reviews_con_titulo) - len(df_palabras_reviews_sin_titulo)}\")\n",
        "\n",
        "# palabras_nuevas = []\n",
        "# revisiones_sin_titulo = list(df_palabras_reviews_sin_titulo.palabras.head(50))\n",
        "# for p in list(df_palabras_reviews_con_titulo.palabras.head(50)):\n",
        "#   if p not in revisiones_sin_titulo:\n",
        "#     palabras_nuevas.append(p)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Con la inclusión del título en el texto del review logramos incrementar el corpus de palabras en 3496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyDGuflBacXv"
      },
      "source": [
        "1.2 **Remover la palabra \"No\" de la lista de stopwords**.   Esto podría ayudar a mejorar la conformación de patrones del algoritmo, ya que las negaciones, en muchos casos, pueden dar el sentido negativo o positivo en una revisión.  Ejemplo: \"*producto recibido*\" podría configurar una revisión como positiva, por el contrario, \"*producto no recibido*\" podría configurar una revisión como negativa.\n",
        "\n",
        "Para esto también tenemos que tomar la decisión de conservar las palabras de 2 o mas caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "eskQB4iobQVW",
        "outputId": "c2258c86-0c58-4433-b8b5-b3ecd71e33c6"
      },
      "source": [
        "#Normalizar datos (exlcuir la palabra \"no\" del conjunto de stopwords y conservar sólo las palabras que tengan 2 o más caracteres)\n",
        "df = normalizar_datos(df, [\"no\"], 1)\n",
        "xtrain, xtest, ytrain, ytest = train_test_vectorized(df)\n",
        "\n",
        "#Entrenamos nuevamente el modelo LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "svc = LinearSVC(C = 0.1, random_state=42, loss='squared_hinge')\n",
        "svc.fit(xtrain,ytrain)\n",
        "print(\"Score: \"+str(svc.score(xtest,ytest))+'\\n')\n",
        "y_pred = svc.predict(xtest)\n",
        "confusion(ytest,y_pred)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "palabra 'no' eliminada de la lista de stopwords!\n",
            "Score: 0.91309375\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njiSeAQvmY5W"
      },
      "source": [
        "Nota: conservando la palabra \"No\" en las revisiones logramos subir un poco el accuracy del modelo (de 0.89 a 0.91)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJJnDRNumo8D"
      },
      "source": [
        "1.3 **Vectorizar las palabras por pares (bigramas)** podría darle mas contexto a los reviews.   por ejemplo:\n",
        "\n",
        "La palabra \"calidad\" por si sola podría ser ambigua a la hora de clasificar un review como negativo o positivo, pero si la combinamos con otra palabra como \"discutible\" tendríamos \"calidad discutible\" que podría encajar dentro de las expresiones comunes para reviews negativas o si la combinamos con la palabra \"garantizada\" tendríamos \"calidad garantizada\" que podría encajar dentro de las expresiones comunes para reviews positivas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "qH4BAmbtt4oS",
        "outputId": "77eb1eaa-45df-4c43-a882-4df52b91e00a"
      },
      "source": [
        "#Normalizar datos (exlcuir la palabra \"no\" del conjunto de stopwords y conservar sólo las palabras que tengan 2 o más caracteres)\n",
        "df = normalizar_datos(df, [\"no\"], 1)\n",
        "#Crear los set de entrenamiento y prueba con bigramas (configurado para realizar 1gram y 2gram)\n",
        "xtrain, xtest, ytrain, ytest = train_test_vectorized_2grams(df)\n",
        "\n",
        "#Entrenamos nuevamente el modelo LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "svc = LinearSVC(C = 0.1, random_state=42, loss='squared_hinge')\n",
        "svc.fit(xtrain,ytrain)\n",
        "print(\"Score: \"+str(svc.score(xtest,ytest))+'\\n')\n",
        "y_pred = svc.predict(xtest)\n",
        "confusion(ytest,y_pred)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "palabra 'no' eliminada de la lista de stopwords!\n",
            "Score: 0.9215\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKCxaN1dxOLS"
      },
      "source": [
        "Nota: Al incluir vectores conformados por palabras por pares (bigramas) mejoró  un poco la metrica del modelo (de 0.913 a 0.921), podemos concluir que el argumento expuesto para sustentar esta estrategia es válido en este contexto y para este caso de uso en particular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ6NMHE7EXfy"
      },
      "source": [
        "#Respuesta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbqacwtUEaSQ"
      },
      "source": [
        "Despues de explorar 3 nuevas estrategias en el preprocesamiento de los datos, podemos decir que todas nos ayudaron a mejorar el score del modelo, pasando de 0.861 a 0.921.  Las estrategias fueron: adicionar el titulo del texto a cada review, remover la palabra \"no\" de la lista de stopwords (con esta última tambien tomamos la decisión de conservar las palabras con 2 o mas caracteres de longitud) e incluir vectores conformados con pares de palabras (bigrams). Esto nos permitió aumentar la cantidad de palabras y expresiones para que el algoritmo tuviera mejores posibilidades de aprender a cerca de los reviews positivos y los reviews negativos y mejorar su desempeño."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKrfbXfpG7yK"
      },
      "source": [
        "#Pregunta 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYRrjWuYG-mq"
      },
      "source": [
        "2. Este problema de clasificación se puede abordar con otras técnicas como son las redes neuronales o los word embeddings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fsBTbHaHHbH"
      },
      "source": [
        "  2.1 **Consideremos el uso de los word embeddings** como una alternativa al TfidfVectorizer. En las incrustaciones, cada palabra está representada por un vector de valor real, que generalmente tiene decenas o cientos de dimensiones a diferencia de las miles o millones de dimensiones necesarias para las representaciones de palabras en vectores sparce como ocurre cuando usamos \n",
        "OneHotEncoder. Las incrustaciones de palabras pueden reducir drásticamente el número de dimensiones necesarias para representar un documento de texto, además, éstas capturan el contexto y la semántica de las oraciones, ya que cada representación de vector de palabra se basa en su significado contextual.\n",
        "\n",
        "  Para el problema de clasificación que nos ocupa, usaremos un word embedding pre-entrenado y veremos si las incrustaciones de palabras son aplicables a nuestro caso de uso y cuál será el efecto sobre el modelo de aprendizaje.\n",
        "\n",
        "  El embedding usado puede descargase del siguiente repositorio:\n",
        "  https://eafit-my.sharepoint.com/:u:/g/personal/srestr63_eafit_edu_co/ERktMgNfWAdDsLuFT9qEDCgBUEYPldH0ZudsKNvFbjL2ig?e=xeGmJh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVeMYym8lLs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdb5f6d-ad28-4693-b974-bfd9b815ab30"
      },
      "source": [
        "#Con respecto al embedding que usaremos:\n",
        "#https://github.com/dccuchile/spanish-word-embeddings\n",
        "#http://crscardellino.github.io/SBWCE/\n",
        "\n",
        "# cargar el word embedding pre-entrenado en español. Cada palabra está representada por un vector de 300 posiciones.\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    os.path.join(os.getcwd(), 'SBW-vectors-300-min5.txt'),\n",
        "    binary=False,\n",
        "    unicode_errors='ignore'\n",
        ")\n",
        "\n",
        "#validamos el tamaño de la dimension para una palabra en el embedding\n",
        "# len(model['comida'])\n",
        "model.vector_size\n",
        "#este script puede tomar 5 minutos para procesar"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X29QthD3mmed"
      },
      "source": [
        "Ahora tenemos vectores de 300 posiciones para representar palabras en lugar de vectores de 2000 posiciones.  Vamos a usar este embedding para vectorizar nuestras reviews y obtener los features que usaremos para entrenar de nuevo nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNOXcRaLmmee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b482943a-e0b4-4321-b8d6-78dff1a42142"
      },
      "source": [
        "#Normalizar datos (exlcuir la palabra \"no\" del conjunto de stopwords y conservar sólo las palabras que tengan 2 o más caracteres)\n",
        "df = normalizar_datos(df, [\"no\"], 1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "palabra 'no' eliminada de la lista de stopwords!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JA0bNAcmmef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbf7675-a7a0-4745-aa62-7f0dba2b5a10"
      },
      "source": [
        "import numpy as np\n",
        "#Para encontrar el vector de una revisión completa, obtenemos el vector de cada palabra de la revisión por separado y tomamos un promedio simple. \n",
        "review_embeddings = []\n",
        "\n",
        "for doc in nlp.pipe(list(df.review_body_lemma), disable=[\"tagger\", \"parser\",\"ner\",\"textcat\",\"custom\"]):\n",
        "    review_average = np.zeros(model.vector_size)\n",
        "    count_val = 0\n",
        "\n",
        "    for token in doc:\n",
        "        if(token.text.lower() in model):\n",
        "            review_average += model[token.text.lower()]\n",
        "            count_val += 1\n",
        "    review_embeddings.append(list(review_average/count_val))\n",
        "    \n",
        "print(f\"Cantidad de reviews en el embedding: {len(review_embeddings)}\")\n",
        "print(f\"Longitud de cada review en el embedding: {len(review_embeddings[0])}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad de reviews en el embedding: 160000\n",
            "Longitud de cada review en el embedding: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK7-_aBmmmef"
      },
      "source": [
        "#Crear un dataframe con los vectores que representan cada review y rellenar las posiciones que puedan estar vacías\n",
        "# embedding_data.isnull().sum()\n",
        "embedding_data = pd.DataFrame(review_embeddings)\n",
        "embedding_data = embedding_data.fillna(0)\n",
        "\n",
        "#obtener los set de datos de entreniamiento y prueba\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(embedding_data,df.sentimiento,test_size=0.2,random_state=42,stratify=df.sentimiento)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5qEnRbjmmeh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e16e0732-3718-4499-c853-1573447c049b"
      },
      "source": [
        "#Entrenamos nuevamente el modelo LinearSVC\n",
        "from sklearn.svm import LinearSVC\n",
        "svc = LinearSVC(C = 0.1, random_state=42, loss='squared_hinge')\n",
        "svc.fit(xtrain,ytrain)\n",
        "print(\"Score: \"+str(svc.score(xtest,ytest))+'\\n')\n",
        "y_pred = svc.predict(xtest)\n",
        "confusion(ytest,y_pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.88103125\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KCZ2gtQmmei"
      },
      "source": [
        "Nota: Desafortunadamente, obtuvimos un score ligeramente menor a nuestro modelo vectorizado con TfidfVectorizer, aunque las incrustaciones de palabras fueron realmente efectivas para reducir el número total de dimensiones, no fueron útiles para obtener un mejor modelo, además no nos facilitan interpretar y diagnosticar qué está causando su desempeño por debajo del deseado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LCKThHaqSfv"
      },
      "source": [
        "2.2 Consideremos el uso de una red neuronal para el problema de clasificación. Usaremos el perceptrón multicapa implementado con sklearn.  Es un modelo mas antigüo y simple que los modelos complejos de la era actual, aunque suficientemente complejo para distinguir datos que no son linealmente separables.  Lo seleccionamos como punto de partida para esta aproximación, para evitar profundizar en conceptos mas complejos que tendrían que considerarse a la hora de escoger otro modelo de red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRh898HS6IaQ"
      },
      "source": [
        "Perceptron multicapa (MLP: multi layer perceptron) es un tipo de red neuronal donde varias capas de un grupo de perceptrones se apilan juntas para hacer un modelo. (Recordemos que un perceptron representa una neurona, y una neurona la podemos asimilar como un modelo lineal que toma múltiples entradas y produce una salida). En nuestro caso, perceptron es un modelo lineal que toma un montón de entradas, las multiplica por valores llamados pesos y añade un término de sesgo para generar una salida.\n",
        "\n",
        "El perceptrón multicapa es una red neuronal artificial (RNA) formada por múltiples capas. Dado que combinar funciones lineales sólo da como resultado otra salida lineal, las RNA incluyen (en sus capas ocultas) una función no lineal para que el MLP sea flexible y aprenda límites de decisión no lineales. Esta función es conocida como la función de activación.\n",
        "\n",
        "Las capas se pueden clasificar en:\n",
        "\n",
        "Capa de entrada: Constituida por aquellas neuronas que introducen los patrones de entrada en la red. En estas neuronas no se produce procesamiento.\n",
        "\n",
        "Capas ocultas: Formada por aquellas neuronas cuyas entradas provienen de capas anteriores y cuyas salidas pasan a neuronas de capas posteriores.\n",
        "\n",
        "Capa de salida: Neuronas cuyos valores de salida se corresponden con las salidas de toda la red.\n",
        "\n",
        "Entrenemos un MLPClassifier con sus parámetros por definición:\n",
        "- Activación: Relu (función de activación)\n",
        "- Alpha: 0.0001 (Regularización L2)\n",
        "- hidden_layer_sizes: 100 (una capa oculta de 100 neuronas)\n",
        "- Solver: adam (adam: stochastic gradient-based optimizer, para la optimización de los pesos)\n",
        "- Learning rate: constante, inicializado en 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WNm06YROqbFS",
        "outputId": "95a6581a-7104-49f2-bcfa-fa501512526f"
      },
      "source": [
        "#Normalizar datos (exlcuir la palabra \"no\" del conjunto de stopwords y conservar sólo las palabras que tengan 2 o más caracteres)\n",
        "df = normalizar_datos(df, [\"no\"], 1)\n",
        "xtrain, xtest, ytrain, ytest = train_test_vectorized_2grams(df)\n",
        "\n",
        "#Entrenamos el modelo MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(random_state=42, verbose=True)\n",
        "mlp.fit(xtrain,ytrain)\n",
        "print(\"Score: \"+str(mlp.score(xtest,ytest))+'\\n')\n",
        "y_pred = mlp.predict(xtest)\n",
        "confusion(ytest,y_pred)\n",
        "#Esta celda puede demorarse hasta 15 minutos en procesar"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "palabra 'no' eliminada de la lista de stopwords!\n",
            "Iteration 1, loss = 0.26629385\n",
            "Iteration 2, loss = 0.20016812\n",
            "Iteration 3, loss = 0.19589085\n",
            "Iteration 4, loss = 0.19283500\n",
            "Iteration 5, loss = 0.18978366\n",
            "Iteration 6, loss = 0.18661938\n",
            "Iteration 7, loss = 0.18349304\n",
            "Iteration 8, loss = 0.17996549\n",
            "Iteration 9, loss = 0.17621993\n",
            "Iteration 10, loss = 0.17220463\n",
            "Iteration 11, loss = 0.16718220\n",
            "Iteration 12, loss = 0.16148380\n",
            "Iteration 13, loss = 0.15504873\n",
            "Iteration 14, loss = 0.14742716\n",
            "Iteration 15, loss = 0.13928722\n",
            "Iteration 16, loss = 0.12991880\n",
            "Iteration 17, loss = 0.12025287\n",
            "Iteration 18, loss = 0.10968439\n",
            "Iteration 19, loss = 0.09897960\n",
            "Iteration 20, loss = 0.08830487\n",
            "Iteration 21, loss = 0.07773695\n",
            "Iteration 22, loss = 0.06801493\n",
            "Iteration 23, loss = 0.05869203\n",
            "Iteration 24, loss = 0.05049238\n",
            "Iteration 25, loss = 0.04296968\n",
            "Iteration 26, loss = 0.03631899\n",
            "Iteration 27, loss = 0.03068573\n",
            "Iteration 29, loss = 0.02182245\n",
            "Iteration 30, loss = 0.01856906\n",
            "Iteration 31, loss = 0.01588731\n",
            "Iteration 32, loss = 0.01374014\n",
            "Iteration 33, loss = 0.01205651\n",
            "Iteration 34, loss = 0.01079305\n",
            "Iteration 35, loss = 0.00960397\n",
            "Iteration 36, loss = 0.00878078\n",
            "Iteration 37, loss = 0.00816883\n",
            "Iteration 38, loss = 0.00775142\n",
            "Iteration 39, loss = 0.00715089\n",
            "Iteration 40, loss = 0.00688716\n",
            "Iteration 41, loss = 0.00673172\n",
            "Iteration 42, loss = 0.00642778\n",
            "Iteration 43, loss = 0.00618044\n",
            "Iteration 44, loss = 0.00629121\n",
            "Iteration 45, loss = 0.00612669\n",
            "Iteration 46, loss = 0.00590552\n",
            "Iteration 47, loss = 0.00590193\n",
            "Iteration 48, loss = 0.00594649\n",
            "Iteration 49, loss = 0.00570092\n",
            "Iteration 50, loss = 0.00566024\n",
            "Iteration 51, loss = 0.00566371\n",
            "Iteration 52, loss = 0.00546505\n",
            "Iteration 53, loss = 0.00547582\n",
            "Iteration 54, loss = 0.00576493\n",
            "Iteration 55, loss = 0.00550619\n",
            "Iteration 56, loss = 0.00536398\n",
            "Iteration 57, loss = 0.00521292\n",
            "Iteration 58, loss = 0.00532412\n",
            "Iteration 59, loss = 0.00532741\n",
            "Iteration 60, loss = 0.00543260\n",
            "Iteration 61, loss = 0.00509115\n",
            "Iteration 62, loss = 0.00523622\n",
            "Iteration 63, loss = 0.00516682\n",
            "Iteration 64, loss = 0.00523668\n",
            "Iteration 65, loss = 0.00540563\n",
            "Iteration 66, loss = 0.00508159\n",
            "Iteration 67, loss = 0.00504883\n",
            "Iteration 68, loss = 0.00501077\n",
            "Iteration 69, loss = 0.00505257\n",
            "Iteration 70, loss = 0.00489917\n",
            "Iteration 71, loss = 0.00511515\n",
            "Iteration 72, loss = 0.00515672\n",
            "Iteration 73, loss = 0.00503221\n",
            "Iteration 74, loss = 0.00490171\n",
            "Iteration 75, loss = 0.00479253\n",
            "Iteration 76, loss = 0.00482454\n",
            "Iteration 77, loss = 0.00500912\n",
            "Iteration 78, loss = 0.00495732\n",
            "Iteration 79, loss = 0.00487379\n",
            "Iteration 80, loss = 0.00478248\n",
            "Iteration 81, loss = 0.00482182\n",
            "Iteration 82, loss = 0.00473952\n",
            "Iteration 83, loss = 0.00472550\n",
            "Iteration 84, loss = 0.00490937\n",
            "Iteration 85, loss = 0.00468713\n",
            "Iteration 86, loss = 0.00485041\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Score: 0.91215625\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NT-nj-IXdNQ"
      },
      "source": [
        "Muy bien!! con la red neuronal perceptrón multicapa obtuvimos un score de 0.912, prácticamente, logramos igualar el modelo lineal LinearSVC que entrenamos arriba con el cual obtuvimos un score de 0.921 -  Intentemos buscar mejores hiperparámetros para este modelo usando randomgridsearch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0lD4AF5YcjW",
        "outputId": "4f8cd991-d29d-41bf-e2e1-ef5dfa3c1c18"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "parameters = {\n",
        "            'hidden_layer_sizes':[(10,),(5,5),(100,),(50,50)],\n",
        "            'alpha':10.0 ** -np.arange(1, 10)\n",
        "            #  solver=['lbfgs','adam')\n",
        "             }\n",
        "\n",
        "mlp = MLPClassifier(random_state=42, verbose=True)\n",
        "mlp_optimo = RandomizedSearchCV(mlp, parameters, scoring='accuracy', cv=3)\n",
        "mlp_optimo.fit(xtrain,ytrain)\n",
        "# mejor_modelo = RandomizedSearchCV(rfmodel, param_dist, n_iter=100, random_state=42, cv=5)\n",
        "print(\"Mejores parametros: \"+str(mlp_optimo.best_params_))\n",
        "print(\"Mejor Score: \"+str(mlp_optimo.best_score_)+'\\n')\n",
        "\n",
        "#*** Este script se demoró 5 horas en procesar ****"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45848482\n",
            "Iteration 2, loss = 0.25626376\n",
            "Iteration 3, loss = 0.22572295\n",
            "Iteration 4, loss = 0.21686561\n",
            "Iteration 5, loss = 0.21349849\n",
            "Iteration 6, loss = 0.21173407\n",
            "Iteration 7, loss = 0.21064822\n",
            "Iteration 8, loss = 0.20983975\n",
            "Iteration 9, loss = 0.20918793\n",
            "Iteration 10, loss = 0.20838810\n",
            "Iteration 11, loss = 0.20758148\n",
            "Iteration 12, loss = 0.20680769\n",
            "Iteration 13, loss = 0.20595102\n",
            "Iteration 14, loss = 0.20509288\n",
            "Iteration 15, loss = 0.20402998\n",
            "Iteration 16, loss = 0.20307067\n",
            "Iteration 17, loss = 0.20194743\n",
            "Iteration 18, loss = 0.20083389\n",
            "Iteration 19, loss = 0.19985142\n",
            "Iteration 20, loss = 0.19885201\n",
            "Iteration 21, loss = 0.19785088\n",
            "Iteration 22, loss = 0.19689570\n",
            "Iteration 23, loss = 0.19599833\n",
            "Iteration 24, loss = 0.19510831\n",
            "Iteration 25, loss = 0.19420157\n",
            "Iteration 26, loss = 0.19331868\n",
            "Iteration 27, loss = 0.19240735\n",
            "Iteration 28, loss = 0.19156984\n",
            "Iteration 29, loss = 0.19085943\n",
            "Iteration 30, loss = 0.18995532\n",
            "Iteration 31, loss = 0.18918160\n",
            "Iteration 32, loss = 0.18842631\n",
            "Iteration 33, loss = 0.18760782\n",
            "Iteration 34, loss = 0.18690070\n",
            "Iteration 35, loss = 0.18605743\n",
            "Iteration 36, loss = 0.18526443\n",
            "Iteration 37, loss = 0.18465370\n",
            "Iteration 38, loss = 0.18386133\n",
            "Iteration 39, loss = 0.18306697\n",
            "Iteration 40, loss = 0.18238384\n",
            "Iteration 41, loss = 0.18157074\n",
            "Iteration 42, loss = 0.18093455\n",
            "Iteration 43, loss = 0.18011621\n",
            "Iteration 44, loss = 0.17945571\n",
            "Iteration 45, loss = 0.17866908\n",
            "Iteration 46, loss = 0.17795556\n",
            "Iteration 47, loss = 0.17725928\n",
            "Iteration 48, loss = 0.17660594\n",
            "Iteration 49, loss = 0.17592703\n",
            "Iteration 50, loss = 0.17503784\n",
            "Iteration 51, loss = 0.17425702\n",
            "Iteration 52, loss = 0.17361883\n",
            "Iteration 53, loss = 0.17275570\n",
            "Iteration 54, loss = 0.17207080\n",
            "Iteration 55, loss = 0.17124146\n",
            "Iteration 56, loss = 0.17049009\n",
            "Iteration 57, loss = 0.16962215\n",
            "Iteration 58, loss = 0.16890278\n",
            "Iteration 59, loss = 0.16798225\n",
            "Iteration 60, loss = 0.16709637\n",
            "Iteration 61, loss = 0.16615073\n",
            "Iteration 62, loss = 0.16528846\n",
            "Iteration 63, loss = 0.16433973\n",
            "Iteration 64, loss = 0.16357935\n",
            "Iteration 65, loss = 0.16262193\n",
            "Iteration 66, loss = 0.16161837\n",
            "Iteration 67, loss = 0.16072071\n",
            "Iteration 68, loss = 0.15970663\n",
            "Iteration 69, loss = 0.15871817\n",
            "Iteration 70, loss = 0.15786777\n",
            "Iteration 71, loss = 0.15673346\n",
            "Iteration 72, loss = 0.15581236\n",
            "Iteration 73, loss = 0.15481831\n",
            "Iteration 74, loss = 0.15373410\n",
            "Iteration 75, loss = 0.15266469\n",
            "Iteration 76, loss = 0.15170173\n",
            "Iteration 77, loss = 0.15062642\n",
            "Iteration 78, loss = 0.14957456\n",
            "Iteration 79, loss = 0.14837163\n",
            "Iteration 80, loss = 0.14739230\n",
            "Iteration 81, loss = 0.14629354\n",
            "Iteration 82, loss = 0.14521349\n",
            "Iteration 83, loss = 0.14411088\n",
            "Iteration 84, loss = 0.14296020\n",
            "Iteration 85, loss = 0.14191241\n",
            "Iteration 86, loss = 0.14088007\n",
            "Iteration 87, loss = 0.13968081\n",
            "Iteration 88, loss = 0.13860175\n",
            "Iteration 89, loss = 0.13751422\n",
            "Iteration 90, loss = 0.13638224\n",
            "Iteration 91, loss = 0.13529085\n",
            "Iteration 92, loss = 0.13425426\n",
            "Iteration 93, loss = 0.13312588\n",
            "Iteration 94, loss = 0.13197268\n",
            "Iteration 95, loss = 0.13097506\n",
            "Iteration 96, loss = 0.12987943\n",
            "Iteration 97, loss = 0.12872495\n",
            "Iteration 98, loss = 0.12765151\n",
            "Iteration 99, loss = 0.12665151\n",
            "Iteration 100, loss = 0.12557530\n",
            "Iteration 101, loss = 0.12445038\n",
            "Iteration 102, loss = 0.12331406\n",
            "Iteration 103, loss = 0.12218693\n",
            "Iteration 104, loss = 0.12122219\n",
            "Iteration 105, loss = 0.12012516\n",
            "Iteration 106, loss = 0.11907313\n",
            "Iteration 107, loss = 0.11804236\n",
            "Iteration 108, loss = 0.11708566\n",
            "Iteration 109, loss = 0.11594147\n",
            "Iteration 110, loss = 0.11487734\n",
            "Iteration 111, loss = 0.11381985\n",
            "Iteration 112, loss = 0.11280539\n",
            "Iteration 113, loss = 0.11181858\n",
            "Iteration 114, loss = 0.11099223\n",
            "Iteration 115, loss = 0.11000324\n",
            "Iteration 116, loss = 0.10901454\n",
            "Iteration 117, loss = 0.10802736\n",
            "Iteration 118, loss = 0.10710451\n",
            "Iteration 119, loss = 0.10623870\n",
            "Iteration 120, loss = 0.10523495\n",
            "Iteration 121, loss = 0.10436219\n",
            "Iteration 122, loss = 0.10347441\n",
            "Iteration 123, loss = 0.10263620\n",
            "Iteration 124, loss = 0.10164600\n",
            "Iteration 125, loss = 0.10074831\n",
            "Iteration 126, loss = 0.09999241\n",
            "Iteration 127, loss = 0.09913341\n",
            "Iteration 128, loss = 0.09824284\n",
            "Iteration 129, loss = 0.09741838\n",
            "Iteration 130, loss = 0.09656690\n",
            "Iteration 131, loss = 0.09574407\n",
            "Iteration 132, loss = 0.09503868\n",
            "Iteration 133, loss = 0.09420222\n",
            "Iteration 134, loss = 0.09343340\n",
            "Iteration 135, loss = 0.09272612\n",
            "Iteration 136, loss = 0.09193565\n",
            "Iteration 137, loss = 0.09120914\n",
            "Iteration 138, loss = 0.09040216\n",
            "Iteration 139, loss = 0.08979414\n",
            "Iteration 140, loss = 0.08896455\n",
            "Iteration 141, loss = 0.08831091\n",
            "Iteration 142, loss = 0.08763697\n",
            "Iteration 143, loss = 0.08692787\n",
            "Iteration 144, loss = 0.08629406\n",
            "Iteration 145, loss = 0.08559604\n",
            "Iteration 146, loss = 0.08496486\n",
            "Iteration 147, loss = 0.08428934\n",
            "Iteration 148, loss = 0.08348121\n",
            "Iteration 149, loss = 0.08287629\n",
            "Iteration 150, loss = 0.08224700\n",
            "Iteration 151, loss = 0.08157978\n",
            "Iteration 152, loss = 0.08097623\n",
            "Iteration 153, loss = 0.08029396\n",
            "Iteration 154, loss = 0.07981948\n",
            "Iteration 155, loss = 0.07914096\n",
            "Iteration 156, loss = 0.07870656\n",
            "Iteration 157, loss = 0.07798213\n",
            "Iteration 158, loss = 0.07754295\n",
            "Iteration 159, loss = 0.07681488\n",
            "Iteration 160, loss = 0.07624926\n",
            "Iteration 161, loss = 0.07573538\n",
            "Iteration 162, loss = 0.07505585\n",
            "Iteration 163, loss = 0.07464997\n",
            "Iteration 164, loss = 0.07397510\n",
            "Iteration 165, loss = 0.07381860\n",
            "Iteration 166, loss = 0.07304276\n",
            "Iteration 167, loss = 0.07247279\n",
            "Iteration 168, loss = 0.07201225\n",
            "Iteration 169, loss = 0.07150180\n",
            "Iteration 170, loss = 0.07105582\n",
            "Iteration 171, loss = 0.07051768\n",
            "Iteration 172, loss = 0.06998141\n",
            "Iteration 173, loss = 0.06951419\n",
            "Iteration 174, loss = 0.06907139\n",
            "Iteration 175, loss = 0.06848495\n",
            "Iteration 176, loss = 0.06799562\n",
            "Iteration 177, loss = 0.06765122\n",
            "Iteration 178, loss = 0.06714014\n",
            "Iteration 179, loss = 0.06670012\n",
            "Iteration 180, loss = 0.06616279\n",
            "Iteration 181, loss = 0.06572166\n",
            "Iteration 182, loss = 0.06534293\n",
            "Iteration 183, loss = 0.06482837\n",
            "Iteration 184, loss = 0.06437439\n",
            "Iteration 185, loss = 0.06398118\n",
            "Iteration 186, loss = 0.06351697\n",
            "Iteration 187, loss = 0.06314083\n",
            "Iteration 188, loss = 0.06271679\n",
            "Iteration 189, loss = 0.06229683\n",
            "Iteration 190, loss = 0.06190779\n",
            "Iteration 191, loss = 0.06137356\n",
            "Iteration 192, loss = 0.06110293\n",
            "Iteration 193, loss = 0.06066700\n",
            "Iteration 194, loss = 0.06024390\n",
            "Iteration 195, loss = 0.05986767\n",
            "Iteration 196, loss = 0.05964846\n",
            "Iteration 197, loss = 0.05911378\n",
            "Iteration 198, loss = 0.05886579\n",
            "Iteration 199, loss = 0.05834352\n",
            "Iteration 200, loss = 0.05801770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45770816\n",
            "Iteration 2, loss = 0.25448271\n",
            "Iteration 3, loss = 0.22414330\n",
            "Iteration 4, loss = 0.21539144\n",
            "Iteration 5, loss = 0.21197100\n",
            "Iteration 6, loss = 0.21030795\n",
            "Iteration 7, loss = 0.20926525\n",
            "Iteration 8, loss = 0.20861954\n",
            "Iteration 9, loss = 0.20782158\n",
            "Iteration 10, loss = 0.20712922\n",
            "Iteration 11, loss = 0.20630661\n",
            "Iteration 12, loss = 0.20559516\n",
            "Iteration 13, loss = 0.20478001\n",
            "Iteration 14, loss = 0.20388211\n",
            "Iteration 15, loss = 0.20282961\n",
            "Iteration 16, loss = 0.20185322\n",
            "Iteration 17, loss = 0.20075302\n",
            "Iteration 18, loss = 0.19963580\n",
            "Iteration 19, loss = 0.19861981\n",
            "Iteration 20, loss = 0.19754180\n",
            "Iteration 21, loss = 0.19646222\n",
            "Iteration 22, loss = 0.19548336\n",
            "Iteration 23, loss = 0.19445917\n",
            "Iteration 24, loss = 0.19354627\n",
            "Iteration 25, loss = 0.19254870\n",
            "Iteration 26, loss = 0.19156999\n",
            "Iteration 27, loss = 0.19069876\n",
            "Iteration 28, loss = 0.18984022\n",
            "Iteration 29, loss = 0.18898550\n",
            "Iteration 30, loss = 0.18809817\n",
            "Iteration 31, loss = 0.18728941\n",
            "Iteration 32, loss = 0.18649972\n",
            "Iteration 33, loss = 0.18563778\n",
            "Iteration 34, loss = 0.18489160\n",
            "Iteration 35, loss = 0.18405205\n",
            "Iteration 36, loss = 0.18321845\n",
            "Iteration 37, loss = 0.18257610\n",
            "Iteration 38, loss = 0.18185042\n",
            "Iteration 39, loss = 0.18103997\n",
            "Iteration 40, loss = 0.18034540\n",
            "Iteration 41, loss = 0.17964436\n",
            "Iteration 42, loss = 0.17893331\n",
            "Iteration 43, loss = 0.17824448\n",
            "Iteration 44, loss = 0.17748319\n",
            "Iteration 45, loss = 0.17679281\n",
            "Iteration 46, loss = 0.17615109\n",
            "Iteration 47, loss = 0.17546869\n",
            "Iteration 48, loss = 0.17478746\n",
            "Iteration 49, loss = 0.17398493\n",
            "Iteration 50, loss = 0.17326496\n",
            "Iteration 51, loss = 0.17257100\n",
            "Iteration 52, loss = 0.17188696\n",
            "Iteration 53, loss = 0.17119869\n",
            "Iteration 54, loss = 0.17045744\n",
            "Iteration 55, loss = 0.16973047\n",
            "Iteration 56, loss = 0.16903492\n",
            "Iteration 57, loss = 0.16823058\n",
            "Iteration 58, loss = 0.16758271\n",
            "Iteration 59, loss = 0.16676420\n",
            "Iteration 60, loss = 0.16608214\n",
            "Iteration 61, loss = 0.16539156\n",
            "Iteration 62, loss = 0.16465700\n",
            "Iteration 63, loss = 0.16385489\n",
            "Iteration 64, loss = 0.16306464\n",
            "Iteration 65, loss = 0.16228541\n",
            "Iteration 66, loss = 0.16150433\n",
            "Iteration 67, loss = 0.16081281\n",
            "Iteration 68, loss = 0.16000904\n",
            "Iteration 69, loss = 0.15913911\n",
            "Iteration 70, loss = 0.15837472\n",
            "Iteration 71, loss = 0.15751964\n",
            "Iteration 72, loss = 0.15669932\n",
            "Iteration 73, loss = 0.15577861\n",
            "Iteration 74, loss = 0.15499695\n",
            "Iteration 75, loss = 0.15417573\n",
            "Iteration 76, loss = 0.15330145\n",
            "Iteration 77, loss = 0.15241207\n",
            "Iteration 78, loss = 0.15159041\n",
            "Iteration 79, loss = 0.15070472\n",
            "Iteration 80, loss = 0.14970501\n",
            "Iteration 81, loss = 0.14884586\n",
            "Iteration 82, loss = 0.14780585\n",
            "Iteration 83, loss = 0.14706549\n",
            "Iteration 84, loss = 0.14588007\n",
            "Iteration 85, loss = 0.14504043\n",
            "Iteration 86, loss = 0.14413942\n",
            "Iteration 87, loss = 0.14299156\n",
            "Iteration 88, loss = 0.14205661\n",
            "Iteration 89, loss = 0.14113295\n",
            "Iteration 90, loss = 0.14019243\n",
            "Iteration 91, loss = 0.13912046\n",
            "Iteration 92, loss = 0.13814098\n",
            "Iteration 93, loss = 0.13714897\n",
            "Iteration 94, loss = 0.13608529\n",
            "Iteration 95, loss = 0.13517712\n",
            "Iteration 96, loss = 0.13404641\n",
            "Iteration 97, loss = 0.13316237\n",
            "Iteration 98, loss = 0.13209095\n",
            "Iteration 99, loss = 0.13118090\n",
            "Iteration 100, loss = 0.13005376\n",
            "Iteration 101, loss = 0.12903191\n",
            "Iteration 102, loss = 0.12805774\n",
            "Iteration 103, loss = 0.12700560\n",
            "Iteration 104, loss = 0.12599922\n",
            "Iteration 105, loss = 0.12508070\n",
            "Iteration 106, loss = 0.12405589\n",
            "Iteration 107, loss = 0.12306692\n",
            "Iteration 108, loss = 0.12203026\n",
            "Iteration 109, loss = 0.12094341\n",
            "Iteration 110, loss = 0.11997124\n",
            "Iteration 111, loss = 0.11906832\n",
            "Iteration 112, loss = 0.11801901\n",
            "Iteration 113, loss = 0.11708284\n",
            "Iteration 114, loss = 0.11589644\n",
            "Iteration 115, loss = 0.11501583\n",
            "Iteration 116, loss = 0.11413272\n",
            "Iteration 117, loss = 0.11320835\n",
            "Iteration 118, loss = 0.11220921\n",
            "Iteration 119, loss = 0.11124461\n",
            "Iteration 120, loss = 0.11031264\n",
            "Iteration 121, loss = 0.10932338\n",
            "Iteration 122, loss = 0.10842205\n",
            "Iteration 123, loss = 0.10749346\n",
            "Iteration 124, loss = 0.10655151\n",
            "Iteration 125, loss = 0.10568342\n",
            "Iteration 126, loss = 0.10479524\n",
            "Iteration 127, loss = 0.10384517\n",
            "Iteration 128, loss = 0.10304846\n",
            "Iteration 129, loss = 0.10221326\n",
            "Iteration 130, loss = 0.10131043\n",
            "Iteration 131, loss = 0.10049233\n",
            "Iteration 132, loss = 0.09957977\n",
            "Iteration 133, loss = 0.09878998\n",
            "Iteration 134, loss = 0.09800785\n",
            "Iteration 135, loss = 0.09712622\n",
            "Iteration 136, loss = 0.09635906\n",
            "Iteration 137, loss = 0.09548416\n",
            "Iteration 138, loss = 0.09476099\n",
            "Iteration 139, loss = 0.09406130\n",
            "Iteration 140, loss = 0.09323427\n",
            "Iteration 141, loss = 0.09248804\n",
            "Iteration 142, loss = 0.09171240\n",
            "Iteration 143, loss = 0.09107010\n",
            "Iteration 144, loss = 0.09028957\n",
            "Iteration 145, loss = 0.08944861\n",
            "Iteration 146, loss = 0.08868920\n",
            "Iteration 147, loss = 0.08798842\n",
            "Iteration 148, loss = 0.08738988\n",
            "Iteration 149, loss = 0.08676930\n",
            "Iteration 150, loss = 0.08603026\n",
            "Iteration 151, loss = 0.08522854\n",
            "Iteration 152, loss = 0.08466949\n",
            "Iteration 153, loss = 0.08403522\n",
            "Iteration 154, loss = 0.08325152\n",
            "Iteration 155, loss = 0.08260521\n",
            "Iteration 156, loss = 0.08208292\n",
            "Iteration 157, loss = 0.08146450\n",
            "Iteration 158, loss = 0.08086573\n",
            "Iteration 159, loss = 0.08020556\n",
            "Iteration 160, loss = 0.07958871\n",
            "Iteration 161, loss = 0.07897389\n",
            "Iteration 162, loss = 0.07836326\n",
            "Iteration 163, loss = 0.07776722\n",
            "Iteration 164, loss = 0.07707909\n",
            "Iteration 165, loss = 0.07656968\n",
            "Iteration 166, loss = 0.07597231\n",
            "Iteration 167, loss = 0.07532160\n",
            "Iteration 168, loss = 0.07485510\n",
            "Iteration 169, loss = 0.07434436\n",
            "Iteration 170, loss = 0.07390689\n",
            "Iteration 171, loss = 0.07321262\n",
            "Iteration 172, loss = 0.07264763\n",
            "Iteration 173, loss = 0.07208020\n",
            "Iteration 174, loss = 0.07168171\n",
            "Iteration 175, loss = 0.07112125\n",
            "Iteration 176, loss = 0.07051193\n",
            "Iteration 177, loss = 0.07009669\n",
            "Iteration 178, loss = 0.06955440\n",
            "Iteration 179, loss = 0.06903972\n",
            "Iteration 180, loss = 0.06856738\n",
            "Iteration 181, loss = 0.06809253\n",
            "Iteration 182, loss = 0.06761847\n",
            "Iteration 183, loss = 0.06706980\n",
            "Iteration 184, loss = 0.06647986\n",
            "Iteration 185, loss = 0.06610174\n",
            "Iteration 186, loss = 0.06558689\n",
            "Iteration 187, loss = 0.06511711\n",
            "Iteration 188, loss = 0.06477407\n",
            "Iteration 189, loss = 0.06420803\n",
            "Iteration 190, loss = 0.06379357\n",
            "Iteration 191, loss = 0.06335560\n",
            "Iteration 192, loss = 0.06290276\n",
            "Iteration 193, loss = 0.06257445\n",
            "Iteration 194, loss = 0.06203595\n",
            "Iteration 195, loss = 0.06152176\n",
            "Iteration 196, loss = 0.06115464\n",
            "Iteration 197, loss = 0.06075508\n",
            "Iteration 198, loss = 0.06045358\n",
            "Iteration 199, loss = 0.05989566\n",
            "Iteration 200, loss = 0.05953099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45779813\n",
            "Iteration 2, loss = 0.25390040\n",
            "Iteration 3, loss = 0.22334502\n",
            "Iteration 4, loss = 0.21463799\n",
            "Iteration 5, loss = 0.21122086\n",
            "Iteration 6, loss = 0.20960585\n",
            "Iteration 7, loss = 0.20854787\n",
            "Iteration 8, loss = 0.20794047\n",
            "Iteration 9, loss = 0.20737984\n",
            "Iteration 10, loss = 0.20673842\n",
            "Iteration 11, loss = 0.20606620\n",
            "Iteration 12, loss = 0.20540371\n",
            "Iteration 13, loss = 0.20467999\n",
            "Iteration 14, loss = 0.20386258\n",
            "Iteration 15, loss = 0.20321999\n",
            "Iteration 16, loss = 0.20227644\n",
            "Iteration 17, loss = 0.20133942\n",
            "Iteration 18, loss = 0.20041504\n",
            "Iteration 19, loss = 0.19932980\n",
            "Iteration 20, loss = 0.19832815\n",
            "Iteration 21, loss = 0.19723673\n",
            "Iteration 22, loss = 0.19626131\n",
            "Iteration 23, loss = 0.19524855\n",
            "Iteration 24, loss = 0.19427337\n",
            "Iteration 25, loss = 0.19318948\n",
            "Iteration 26, loss = 0.19232807\n",
            "Iteration 27, loss = 0.19135490\n",
            "Iteration 28, loss = 0.19044166\n",
            "Iteration 29, loss = 0.18950079\n",
            "Iteration 30, loss = 0.18854352\n",
            "Iteration 31, loss = 0.18774308\n",
            "Iteration 32, loss = 0.18681734\n",
            "Iteration 33, loss = 0.18594066\n",
            "Iteration 34, loss = 0.18522124\n",
            "Iteration 35, loss = 0.18431548\n",
            "Iteration 36, loss = 0.18349873\n",
            "Iteration 37, loss = 0.18277466\n",
            "Iteration 38, loss = 0.18197538\n",
            "Iteration 39, loss = 0.18111446\n",
            "Iteration 40, loss = 0.18041956\n",
            "Iteration 41, loss = 0.17960871\n",
            "Iteration 42, loss = 0.17889682\n",
            "Iteration 43, loss = 0.17820297\n",
            "Iteration 44, loss = 0.17745881\n",
            "Iteration 45, loss = 0.17666117\n",
            "Iteration 46, loss = 0.17598416\n",
            "Iteration 47, loss = 0.17529210\n",
            "Iteration 48, loss = 0.17455781\n",
            "Iteration 49, loss = 0.17391910\n",
            "Iteration 50, loss = 0.17314188\n",
            "Iteration 51, loss = 0.17243663\n",
            "Iteration 52, loss = 0.17184152\n",
            "Iteration 53, loss = 0.17113111\n",
            "Iteration 54, loss = 0.17034121\n",
            "Iteration 55, loss = 0.16972552\n",
            "Iteration 56, loss = 0.16895811\n",
            "Iteration 57, loss = 0.16832017\n",
            "Iteration 58, loss = 0.16752192\n",
            "Iteration 59, loss = 0.16685515\n",
            "Iteration 60, loss = 0.16605857\n",
            "Iteration 61, loss = 0.16544509\n",
            "Iteration 62, loss = 0.16465916\n",
            "Iteration 63, loss = 0.16388323\n",
            "Iteration 64, loss = 0.16323252\n",
            "Iteration 65, loss = 0.16243003\n",
            "Iteration 66, loss = 0.16169955\n",
            "Iteration 67, loss = 0.16090849\n",
            "Iteration 68, loss = 0.16014624\n",
            "Iteration 69, loss = 0.15939146\n",
            "Iteration 70, loss = 0.15850531\n",
            "Iteration 71, loss = 0.15782694\n",
            "Iteration 72, loss = 0.15709881\n",
            "Iteration 73, loss = 0.15624383\n",
            "Iteration 74, loss = 0.15547193\n",
            "Iteration 75, loss = 0.15472416\n",
            "Iteration 76, loss = 0.15380118\n",
            "Iteration 77, loss = 0.15299756\n",
            "Iteration 78, loss = 0.15219547\n",
            "Iteration 79, loss = 0.15142243\n",
            "Iteration 80, loss = 0.15057649\n",
            "Iteration 81, loss = 0.14979055\n",
            "Iteration 82, loss = 0.14898623\n",
            "Iteration 83, loss = 0.14813904\n",
            "Iteration 84, loss = 0.14728778\n",
            "Iteration 85, loss = 0.14649174\n",
            "Iteration 86, loss = 0.14559470\n",
            "Iteration 87, loss = 0.14472548\n",
            "Iteration 88, loss = 0.14389980\n",
            "Iteration 89, loss = 0.14304437\n",
            "Iteration 90, loss = 0.14213656\n",
            "Iteration 91, loss = 0.14130593\n",
            "Iteration 92, loss = 0.14039464\n",
            "Iteration 93, loss = 0.13964162\n",
            "Iteration 94, loss = 0.13877926\n",
            "Iteration 95, loss = 0.13777787\n",
            "Iteration 96, loss = 0.13697723\n",
            "Iteration 97, loss = 0.13596841\n",
            "Iteration 98, loss = 0.13510506\n",
            "Iteration 99, loss = 0.13427389\n",
            "Iteration 100, loss = 0.13340489\n",
            "Iteration 101, loss = 0.13255269\n",
            "Iteration 102, loss = 0.13171220\n",
            "Iteration 103, loss = 0.13075349\n",
            "Iteration 104, loss = 0.12994406\n",
            "Iteration 105, loss = 0.12916034\n",
            "Iteration 106, loss = 0.12809632\n",
            "Iteration 107, loss = 0.12725440\n",
            "Iteration 108, loss = 0.12642794\n",
            "Iteration 109, loss = 0.12557999\n",
            "Iteration 110, loss = 0.12463106\n",
            "Iteration 111, loss = 0.12377576\n",
            "Iteration 112, loss = 0.12288992\n",
            "Iteration 113, loss = 0.12205609\n",
            "Iteration 114, loss = 0.12118835\n",
            "Iteration 115, loss = 0.12032090\n",
            "Iteration 116, loss = 0.11938517\n",
            "Iteration 117, loss = 0.11855400\n",
            "Iteration 118, loss = 0.11766174\n",
            "Iteration 119, loss = 0.11673664\n",
            "Iteration 120, loss = 0.11597658\n",
            "Iteration 121, loss = 0.11506937\n",
            "Iteration 122, loss = 0.11431114\n",
            "Iteration 123, loss = 0.11333978\n",
            "Iteration 124, loss = 0.11252500\n",
            "Iteration 125, loss = 0.11177034\n",
            "Iteration 126, loss = 0.11092014\n",
            "Iteration 127, loss = 0.11000252\n",
            "Iteration 128, loss = 0.10931371\n",
            "Iteration 129, loss = 0.10852771\n",
            "Iteration 130, loss = 0.10769336\n",
            "Iteration 131, loss = 0.10672549\n",
            "Iteration 132, loss = 0.10592342\n",
            "Iteration 133, loss = 0.10522307\n",
            "Iteration 134, loss = 0.10450809\n",
            "Iteration 135, loss = 0.10367283\n",
            "Iteration 136, loss = 0.10294915\n",
            "Iteration 137, loss = 0.10220602\n",
            "Iteration 138, loss = 0.10143253\n",
            "Iteration 139, loss = 0.10063990\n",
            "Iteration 140, loss = 0.09996282\n",
            "Iteration 141, loss = 0.09917728\n",
            "Iteration 142, loss = 0.09844344\n",
            "Iteration 143, loss = 0.09772395\n",
            "Iteration 144, loss = 0.09702163\n",
            "Iteration 145, loss = 0.09626095\n",
            "Iteration 146, loss = 0.09584969\n",
            "Iteration 147, loss = 0.09488108\n",
            "Iteration 148, loss = 0.09419279\n",
            "Iteration 149, loss = 0.09353086\n",
            "Iteration 150, loss = 0.09293500\n",
            "Iteration 151, loss = 0.09225003\n",
            "Iteration 152, loss = 0.09171728\n",
            "Iteration 153, loss = 0.09094571\n",
            "Iteration 154, loss = 0.09015843\n",
            "Iteration 155, loss = 0.08976473\n",
            "Iteration 156, loss = 0.08909250\n",
            "Iteration 157, loss = 0.08844707\n",
            "Iteration 158, loss = 0.08789825\n",
            "Iteration 159, loss = 0.08719541\n",
            "Iteration 160, loss = 0.08657319\n",
            "Iteration 161, loss = 0.08599792\n",
            "Iteration 162, loss = 0.08542344\n",
            "Iteration 163, loss = 0.08477133\n",
            "Iteration 164, loss = 0.08429128\n",
            "Iteration 165, loss = 0.08382510\n",
            "Iteration 166, loss = 0.08305480\n",
            "Iteration 167, loss = 0.08274086\n",
            "Iteration 168, loss = 0.08203453\n",
            "Iteration 169, loss = 0.08151006\n",
            "Iteration 170, loss = 0.08086538\n",
            "Iteration 171, loss = 0.08055456\n",
            "Iteration 172, loss = 0.07981025\n",
            "Iteration 173, loss = 0.07938575\n",
            "Iteration 174, loss = 0.07896186\n",
            "Iteration 175, loss = 0.07836361\n",
            "Iteration 176, loss = 0.07775330\n",
            "Iteration 177, loss = 0.07739337\n",
            "Iteration 178, loss = 0.07692859\n",
            "Iteration 179, loss = 0.07636702\n",
            "Iteration 180, loss = 0.07592800\n",
            "Iteration 181, loss = 0.07534786\n",
            "Iteration 182, loss = 0.07494155\n",
            "Iteration 183, loss = 0.07445185\n",
            "Iteration 184, loss = 0.07396669\n",
            "Iteration 185, loss = 0.07345132\n",
            "Iteration 186, loss = 0.07305299\n",
            "Iteration 187, loss = 0.07242741\n",
            "Iteration 188, loss = 0.07202450\n",
            "Iteration 189, loss = 0.07168815\n",
            "Iteration 190, loss = 0.07115359\n",
            "Iteration 191, loss = 0.07079129\n",
            "Iteration 192, loss = 0.07026178\n",
            "Iteration 193, loss = 0.06979466\n",
            "Iteration 194, loss = 0.06951617\n",
            "Iteration 195, loss = 0.06891625\n",
            "Iteration 196, loss = 0.06859418\n",
            "Iteration 197, loss = 0.06809992\n",
            "Iteration 198, loss = 0.06756366\n",
            "Iteration 199, loss = 0.06720197\n",
            "Iteration 200, loss = 0.06701819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.31845014\n",
            "Iteration 2, loss = 0.21972550\n",
            "Iteration 3, loss = 0.21398210\n",
            "Iteration 4, loss = 0.20991860\n",
            "Iteration 5, loss = 0.20636983\n",
            "Iteration 6, loss = 0.20262781\n",
            "Iteration 7, loss = 0.19885386\n",
            "Iteration 8, loss = 0.19513696\n",
            "Iteration 9, loss = 0.19081375\n",
            "Iteration 10, loss = 0.18636296\n",
            "Iteration 11, loss = 0.18138782\n",
            "Iteration 12, loss = 0.17527516\n",
            "Iteration 13, loss = 0.16887464\n",
            "Iteration 14, loss = 0.16121712\n",
            "Iteration 15, loss = 0.15287029\n",
            "Iteration 16, loss = 0.14365604\n",
            "Iteration 17, loss = 0.13352856\n",
            "Iteration 18, loss = 0.12325486\n",
            "Iteration 19, loss = 0.11210238\n",
            "Iteration 20, loss = 0.10120484\n",
            "Iteration 21, loss = 0.09022470\n",
            "Iteration 22, loss = 0.07979640\n",
            "Iteration 23, loss = 0.07008193\n",
            "Iteration 24, loss = 0.06100545\n",
            "Iteration 25, loss = 0.05287603\n",
            "Iteration 26, loss = 0.04531140\n",
            "Iteration 27, loss = 0.03894931\n",
            "Iteration 28, loss = 0.03336205\n",
            "Iteration 29, loss = 0.02849493\n",
            "Iteration 30, loss = 0.02426906\n",
            "Iteration 31, loss = 0.02084746\n",
            "Iteration 32, loss = 0.01769974\n",
            "Iteration 33, loss = 0.01506485\n",
            "Iteration 34, loss = 0.01294091\n",
            "Iteration 35, loss = 0.01114505\n",
            "Iteration 36, loss = 0.00958232\n",
            "Iteration 37, loss = 0.00829664\n",
            "Iteration 38, loss = 0.00719824\n",
            "Iteration 39, loss = 0.00626250\n",
            "Iteration 40, loss = 0.00555736\n",
            "Iteration 41, loss = 0.00483248\n",
            "Iteration 42, loss = 0.00426838\n",
            "Iteration 43, loss = 0.00381774\n",
            "Iteration 44, loss = 0.00336296\n",
            "Iteration 45, loss = 0.00302627\n",
            "Iteration 46, loss = 0.00269551\n",
            "Iteration 47, loss = 0.00246666\n",
            "Iteration 48, loss = 0.00223384\n",
            "Iteration 49, loss = 0.00216124\n",
            "Iteration 50, loss = 0.00193558\n",
            "Iteration 51, loss = 0.00180915\n",
            "Iteration 52, loss = 0.00165473\n",
            "Iteration 53, loss = 0.00154675\n",
            "Iteration 54, loss = 0.00152800\n",
            "Iteration 55, loss = 0.00145246\n",
            "Iteration 56, loss = 0.00141086\n",
            "Iteration 57, loss = 0.00132187\n",
            "Iteration 58, loss = 0.00123935\n",
            "Iteration 59, loss = 0.00125943\n",
            "Iteration 60, loss = 0.00121517\n",
            "Iteration 61, loss = 0.00113713\n",
            "Iteration 62, loss = 0.00112346\n",
            "Iteration 63, loss = 0.00113332\n",
            "Iteration 64, loss = 0.00106441\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.31768077\n",
            "Iteration 2, loss = 0.21814165\n",
            "Iteration 3, loss = 0.21220029\n",
            "Iteration 4, loss = 0.20816400\n",
            "Iteration 5, loss = 0.20427825\n",
            "Iteration 6, loss = 0.20039692\n",
            "Iteration 7, loss = 0.19654384\n",
            "Iteration 8, loss = 0.19214461\n",
            "Iteration 9, loss = 0.18798035\n",
            "Iteration 10, loss = 0.18310484\n",
            "Iteration 11, loss = 0.17757286\n",
            "Iteration 12, loss = 0.17150625\n",
            "Iteration 13, loss = 0.16430916\n",
            "Iteration 14, loss = 0.15653718\n",
            "Iteration 15, loss = 0.14783992\n",
            "Iteration 16, loss = 0.13837527\n",
            "Iteration 17, loss = 0.12817564\n",
            "Iteration 18, loss = 0.11736022\n",
            "Iteration 19, loss = 0.10623784\n",
            "Iteration 20, loss = 0.09545578\n",
            "Iteration 21, loss = 0.08472987\n",
            "Iteration 22, loss = 0.07468053\n",
            "Iteration 23, loss = 0.06483566\n",
            "Iteration 24, loss = 0.05646108\n",
            "Iteration 25, loss = 0.04862091\n",
            "Iteration 26, loss = 0.04168323\n",
            "Iteration 27, loss = 0.03568056\n",
            "Iteration 28, loss = 0.03025169\n",
            "Iteration 29, loss = 0.02576522\n",
            "Iteration 30, loss = 0.02187459\n",
            "Iteration 31, loss = 0.01849356\n",
            "Iteration 32, loss = 0.01569936\n",
            "Iteration 33, loss = 0.01329953\n",
            "Iteration 34, loss = 0.01127209\n",
            "Iteration 35, loss = 0.00963249\n",
            "Iteration 36, loss = 0.00824228\n",
            "Iteration 37, loss = 0.00706279\n",
            "Iteration 38, loss = 0.00601274\n",
            "Iteration 39, loss = 0.00517375\n",
            "Iteration 40, loss = 0.00449457\n",
            "Iteration 41, loss = 0.00398545\n",
            "Iteration 42, loss = 0.00357413\n",
            "Iteration 43, loss = 0.00310594\n",
            "Iteration 44, loss = 0.00283003\n",
            "Iteration 45, loss = 0.00256561\n",
            "Iteration 46, loss = 0.00225310\n",
            "Iteration 47, loss = 0.00206401\n",
            "Iteration 48, loss = 0.00192550\n",
            "Iteration 49, loss = 0.00183737\n",
            "Iteration 50, loss = 0.00165127\n",
            "Iteration 51, loss = 0.00153557\n",
            "Iteration 52, loss = 0.00136097\n",
            "Iteration 53, loss = 0.00128058\n",
            "Iteration 54, loss = 0.00132944\n",
            "Iteration 55, loss = 0.00121662\n",
            "Iteration 56, loss = 0.00111355\n",
            "Iteration 57, loss = 0.00115305\n",
            "Iteration 58, loss = 0.00103183\n",
            "Iteration 59, loss = 0.00096847\n",
            "Iteration 60, loss = 0.00096251\n",
            "Iteration 61, loss = 0.00099629\n",
            "Iteration 62, loss = 0.00094233\n",
            "Iteration 63, loss = 0.00085282\n",
            "Iteration 64, loss = 0.00084064\n",
            "Iteration 65, loss = 0.00089861\n",
            "Iteration 66, loss = 0.00080452\n",
            "Iteration 67, loss = 0.00083942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.31833250\n",
            "Iteration 2, loss = 0.21774925\n",
            "Iteration 3, loss = 0.21213160\n",
            "Iteration 4, loss = 0.20842943\n",
            "Iteration 5, loss = 0.20499213\n",
            "Iteration 6, loss = 0.20141336\n",
            "Iteration 7, loss = 0.19750409\n",
            "Iteration 8, loss = 0.19382979\n",
            "Iteration 9, loss = 0.18963302\n",
            "Iteration 10, loss = 0.18498423\n",
            "Iteration 11, loss = 0.18006468\n",
            "Iteration 12, loss = 0.17435754\n",
            "Iteration 13, loss = 0.16791535\n",
            "Iteration 14, loss = 0.16051188\n",
            "Iteration 15, loss = 0.15200800\n",
            "Iteration 16, loss = 0.14300553\n",
            "Iteration 17, loss = 0.13311602\n",
            "Iteration 18, loss = 0.12259744\n",
            "Iteration 19, loss = 0.11137478\n",
            "Iteration 20, loss = 0.10058990\n",
            "Iteration 21, loss = 0.08973489\n",
            "Iteration 22, loss = 0.07934788\n",
            "Iteration 23, loss = 0.06932389\n",
            "Iteration 24, loss = 0.06037076\n",
            "Iteration 25, loss = 0.05235021\n",
            "Iteration 26, loss = 0.04515461\n",
            "Iteration 27, loss = 0.03861531\n",
            "Iteration 28, loss = 0.03313843\n",
            "Iteration 29, loss = 0.02813555\n",
            "Iteration 30, loss = 0.02409637\n",
            "Iteration 31, loss = 0.02036445\n",
            "Iteration 32, loss = 0.01741967\n",
            "Iteration 33, loss = 0.01488739\n",
            "Iteration 34, loss = 0.01261695\n",
            "Iteration 35, loss = 0.01084398\n",
            "Iteration 36, loss = 0.00931223\n",
            "Iteration 37, loss = 0.00793519\n",
            "Iteration 38, loss = 0.00683060\n",
            "Iteration 39, loss = 0.00593623\n",
            "Iteration 40, loss = 0.00517948\n",
            "Iteration 41, loss = 0.00449732\n",
            "Iteration 42, loss = 0.00394408\n",
            "Iteration 43, loss = 0.00345798\n",
            "Iteration 44, loss = 0.00299125\n",
            "Iteration 45, loss = 0.00264910\n",
            "Iteration 46, loss = 0.00237483\n",
            "Iteration 47, loss = 0.00214450\n",
            "Iteration 48, loss = 0.00192638\n",
            "Iteration 49, loss = 0.00176383\n",
            "Iteration 50, loss = 0.00167491\n",
            "Iteration 51, loss = 0.00148915\n",
            "Iteration 52, loss = 0.00138503\n",
            "Iteration 53, loss = 0.00124826\n",
            "Iteration 54, loss = 0.00120186\n",
            "Iteration 55, loss = 0.00114568\n",
            "Iteration 56, loss = 0.00113374\n",
            "Iteration 57, loss = 0.00108725\n",
            "Iteration 58, loss = 0.00099310\n",
            "Iteration 59, loss = 0.00095717\n",
            "Iteration 60, loss = 0.00092247\n",
            "Iteration 61, loss = 0.00089700\n",
            "Iteration 62, loss = 0.00092758\n",
            "Iteration 63, loss = 0.00081177\n",
            "Iteration 64, loss = 0.00080074\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.45846013\n",
            "Iteration 2, loss = 0.25610494\n",
            "Iteration 3, loss = 0.22558227\n",
            "Iteration 4, loss = 0.21672355\n",
            "Iteration 5, loss = 0.21334959\n",
            "Iteration 6, loss = 0.21158018\n",
            "Iteration 7, loss = 0.21048552\n",
            "Iteration 8, loss = 0.20967139\n",
            "Iteration 9, loss = 0.20901563\n",
            "Iteration 10, loss = 0.20821226\n",
            "Iteration 11, loss = 0.20740771\n",
            "Iteration 12, loss = 0.20662360\n",
            "Iteration 13, loss = 0.20575748\n",
            "Iteration 14, loss = 0.20488452\n",
            "Iteration 15, loss = 0.20380790\n",
            "Iteration 16, loss = 0.20284651\n",
            "Iteration 17, loss = 0.20170017\n",
            "Iteration 18, loss = 0.20056379\n",
            "Iteration 19, loss = 0.19956684\n",
            "Iteration 20, loss = 0.19856156\n",
            "Iteration 21, loss = 0.19755337\n",
            "Iteration 22, loss = 0.19657665\n",
            "Iteration 23, loss = 0.19564994\n",
            "Iteration 24, loss = 0.19473480\n",
            "Iteration 25, loss = 0.19381187\n",
            "Iteration 26, loss = 0.19292643\n",
            "Iteration 27, loss = 0.19198567\n",
            "Iteration 28, loss = 0.19113099\n",
            "Iteration 29, loss = 0.19040922\n",
            "Iteration 30, loss = 0.18948758\n",
            "Iteration 31, loss = 0.18870705\n",
            "Iteration 32, loss = 0.18793910\n",
            "Iteration 33, loss = 0.18710193\n",
            "Iteration 34, loss = 0.18637110\n",
            "Iteration 35, loss = 0.18552314\n",
            "Iteration 36, loss = 0.18471362\n",
            "Iteration 37, loss = 0.18407568\n",
            "Iteration 38, loss = 0.18327263\n",
            "Iteration 39, loss = 0.18247010\n",
            "Iteration 40, loss = 0.18175857\n",
            "Iteration 41, loss = 0.18093956\n",
            "Iteration 42, loss = 0.18028300\n",
            "Iteration 43, loss = 0.17946087\n",
            "Iteration 44, loss = 0.17879553\n",
            "Iteration 45, loss = 0.17799092\n",
            "Iteration 46, loss = 0.17727881\n",
            "Iteration 47, loss = 0.17656338\n",
            "Iteration 48, loss = 0.17592059\n",
            "Iteration 49, loss = 0.17522321\n",
            "Iteration 50, loss = 0.17435655\n",
            "Iteration 51, loss = 0.17357917\n",
            "Iteration 52, loss = 0.17294748\n",
            "Iteration 53, loss = 0.17206763\n",
            "Iteration 54, loss = 0.17140156\n",
            "Iteration 55, loss = 0.17058723\n",
            "Iteration 56, loss = 0.16980465\n",
            "Iteration 57, loss = 0.16894255\n",
            "Iteration 58, loss = 0.16824855\n",
            "Iteration 59, loss = 0.16731518\n",
            "Iteration 60, loss = 0.16646862\n",
            "Iteration 61, loss = 0.16553592\n",
            "Iteration 62, loss = 0.16470971\n",
            "Iteration 63, loss = 0.16378118\n",
            "Iteration 64, loss = 0.16301818\n",
            "Iteration 65, loss = 0.16207572\n",
            "Iteration 66, loss = 0.16107621\n",
            "Iteration 67, loss = 0.16022591\n",
            "Iteration 68, loss = 0.15918383\n",
            "Iteration 69, loss = 0.15818318\n",
            "Iteration 70, loss = 0.15735535\n",
            "Iteration 71, loss = 0.15622120\n",
            "Iteration 72, loss = 0.15531011\n",
            "Iteration 73, loss = 0.15426982\n",
            "Iteration 74, loss = 0.15319552\n",
            "Iteration 75, loss = 0.15215614\n",
            "Iteration 76, loss = 0.15114793\n",
            "Iteration 77, loss = 0.15008263\n",
            "Iteration 78, loss = 0.14902843\n",
            "Iteration 79, loss = 0.14781795\n",
            "Iteration 80, loss = 0.14679358\n",
            "Iteration 81, loss = 0.14570970\n",
            "Iteration 82, loss = 0.14462623\n",
            "Iteration 83, loss = 0.14349672\n",
            "Iteration 84, loss = 0.14234672\n",
            "Iteration 85, loss = 0.14127431\n",
            "Iteration 86, loss = 0.14023116\n",
            "Iteration 87, loss = 0.13900420\n",
            "Iteration 88, loss = 0.13790011\n",
            "Iteration 89, loss = 0.13681021\n",
            "Iteration 90, loss = 0.13564036\n",
            "Iteration 91, loss = 0.13460013\n",
            "Iteration 92, loss = 0.13348695\n",
            "Iteration 93, loss = 0.13234754\n",
            "Iteration 94, loss = 0.13114887\n",
            "Iteration 95, loss = 0.13011794\n",
            "Iteration 96, loss = 0.12895341\n",
            "Iteration 97, loss = 0.12778551\n",
            "Iteration 98, loss = 0.12673831\n",
            "Iteration 99, loss = 0.12577548\n",
            "Iteration 100, loss = 0.12462282\n",
            "Iteration 101, loss = 0.12347053\n",
            "Iteration 102, loss = 0.12236582\n",
            "Iteration 103, loss = 0.12122254\n",
            "Iteration 104, loss = 0.12022165\n",
            "Iteration 105, loss = 0.11910725\n",
            "Iteration 106, loss = 0.11806735\n",
            "Iteration 107, loss = 0.11707908\n",
            "Iteration 108, loss = 0.11607122\n",
            "Iteration 109, loss = 0.11499119\n",
            "Iteration 110, loss = 0.11389916\n",
            "Iteration 111, loss = 0.11296156\n",
            "Iteration 112, loss = 0.11187223\n",
            "Iteration 113, loss = 0.11093984\n",
            "Iteration 114, loss = 0.11003827\n",
            "Iteration 115, loss = 0.10905282\n",
            "Iteration 116, loss = 0.10811399\n",
            "Iteration 117, loss = 0.10712538\n",
            "Iteration 118, loss = 0.10612613\n",
            "Iteration 119, loss = 0.10524699\n",
            "Iteration 120, loss = 0.10415984\n",
            "Iteration 121, loss = 0.10328334\n",
            "Iteration 122, loss = 0.10235604\n",
            "Iteration 123, loss = 0.10151495\n",
            "Iteration 124, loss = 0.10046952\n",
            "Iteration 125, loss = 0.09963136\n",
            "Iteration 126, loss = 0.09871956\n",
            "Iteration 127, loss = 0.09788710\n",
            "Iteration 128, loss = 0.09692577\n",
            "Iteration 129, loss = 0.09607151\n",
            "Iteration 130, loss = 0.09523039\n",
            "Iteration 131, loss = 0.09438013\n",
            "Iteration 132, loss = 0.09359776\n",
            "Iteration 133, loss = 0.09268246\n",
            "Iteration 134, loss = 0.09194876\n",
            "Iteration 135, loss = 0.09113190\n",
            "Iteration 136, loss = 0.09039918\n",
            "Iteration 137, loss = 0.08959856\n",
            "Iteration 138, loss = 0.08878599\n",
            "Iteration 139, loss = 0.08814038\n",
            "Iteration 140, loss = 0.08726343\n",
            "Iteration 141, loss = 0.08651192\n",
            "Iteration 142, loss = 0.08580995\n",
            "Iteration 143, loss = 0.08509017\n",
            "Iteration 144, loss = 0.08444529\n",
            "Iteration 145, loss = 0.08365944\n",
            "Iteration 146, loss = 0.08311766\n",
            "Iteration 147, loss = 0.08226723\n",
            "Iteration 148, loss = 0.08159430\n",
            "Iteration 149, loss = 0.08087666\n",
            "Iteration 150, loss = 0.08023891\n",
            "Iteration 151, loss = 0.07952739\n",
            "Iteration 152, loss = 0.07889952\n",
            "Iteration 153, loss = 0.07830671\n",
            "Iteration 154, loss = 0.07773931\n",
            "Iteration 155, loss = 0.07709135\n",
            "Iteration 156, loss = 0.07647088\n",
            "Iteration 157, loss = 0.07575861\n",
            "Iteration 158, loss = 0.07537088\n",
            "Iteration 159, loss = 0.07469809\n",
            "Iteration 160, loss = 0.07395003\n",
            "Iteration 161, loss = 0.07341457\n",
            "Iteration 162, loss = 0.07285057\n",
            "Iteration 163, loss = 0.07245843\n",
            "Iteration 164, loss = 0.07176927\n",
            "Iteration 165, loss = 0.07138089\n",
            "Iteration 166, loss = 0.07066822\n",
            "Iteration 167, loss = 0.07022636\n",
            "Iteration 168, loss = 0.06965238\n",
            "Iteration 169, loss = 0.06911603\n",
            "Iteration 170, loss = 0.06854028\n",
            "Iteration 171, loss = 0.06810911\n",
            "Iteration 172, loss = 0.06756612\n",
            "Iteration 173, loss = 0.06707615\n",
            "Iteration 174, loss = 0.06660764\n",
            "Iteration 175, loss = 0.06612257\n",
            "Iteration 176, loss = 0.06554959\n",
            "Iteration 177, loss = 0.06510447\n",
            "Iteration 178, loss = 0.06478791\n",
            "Iteration 179, loss = 0.06428886\n",
            "Iteration 180, loss = 0.06376696\n",
            "Iteration 181, loss = 0.06331362\n",
            "Iteration 182, loss = 0.06288795\n",
            "Iteration 183, loss = 0.06245911\n",
            "Iteration 184, loss = 0.06185030\n",
            "Iteration 185, loss = 0.06159450\n",
            "Iteration 186, loss = 0.06105044\n",
            "Iteration 187, loss = 0.06079863\n",
            "Iteration 188, loss = 0.06033578\n",
            "Iteration 189, loss = 0.05982136\n",
            "Iteration 190, loss = 0.05949098\n",
            "Iteration 191, loss = 0.05891166\n",
            "Iteration 192, loss = 0.05865435\n",
            "Iteration 193, loss = 0.05822870\n",
            "Iteration 194, loss = 0.05790118\n",
            "Iteration 195, loss = 0.05744784\n",
            "Iteration 196, loss = 0.05706950\n",
            "Iteration 197, loss = 0.05660093\n",
            "Iteration 198, loss = 0.05629194\n",
            "Iteration 199, loss = 0.05578408\n",
            "Iteration 200, loss = 0.05552445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45773881\n",
            "Iteration 2, loss = 0.25438383\n",
            "Iteration 3, loss = 0.22402782\n",
            "Iteration 4, loss = 0.21526428\n",
            "Iteration 5, loss = 0.21183397\n",
            "Iteration 6, loss = 0.21016182\n",
            "Iteration 7, loss = 0.20910943\n",
            "Iteration 8, loss = 0.20845961\n",
            "Iteration 9, loss = 0.20766014\n",
            "Iteration 10, loss = 0.20697046\n",
            "Iteration 11, loss = 0.20613164\n",
            "Iteration 12, loss = 0.20540459\n",
            "Iteration 13, loss = 0.20457438\n",
            "Iteration 14, loss = 0.20367641\n",
            "Iteration 15, loss = 0.20261034\n",
            "Iteration 16, loss = 0.20162248\n",
            "Iteration 17, loss = 0.20052334\n",
            "Iteration 18, loss = 0.19940460\n",
            "Iteration 19, loss = 0.19836607\n",
            "Iteration 20, loss = 0.19726500\n",
            "Iteration 21, loss = 0.19618142\n",
            "Iteration 22, loss = 0.19520127\n",
            "Iteration 23, loss = 0.19417783\n",
            "Iteration 24, loss = 0.19326053\n",
            "Iteration 25, loss = 0.19225519\n",
            "Iteration 26, loss = 0.19126949\n",
            "Iteration 27, loss = 0.19038669\n",
            "Iteration 28, loss = 0.18950560\n",
            "Iteration 29, loss = 0.18864006\n",
            "Iteration 30, loss = 0.18774031\n",
            "Iteration 31, loss = 0.18690727\n",
            "Iteration 32, loss = 0.18611345\n",
            "Iteration 33, loss = 0.18524138\n",
            "Iteration 34, loss = 0.18448270\n",
            "Iteration 35, loss = 0.18363306\n",
            "Iteration 36, loss = 0.18279528\n",
            "Iteration 37, loss = 0.18216075\n",
            "Iteration 38, loss = 0.18141911\n",
            "Iteration 39, loss = 0.18060765\n",
            "Iteration 40, loss = 0.17991793\n",
            "Iteration 41, loss = 0.17919992\n",
            "Iteration 42, loss = 0.17848164\n",
            "Iteration 43, loss = 0.17778666\n",
            "Iteration 44, loss = 0.17699919\n",
            "Iteration 45, loss = 0.17630690\n",
            "Iteration 46, loss = 0.17565590\n",
            "Iteration 47, loss = 0.17496324\n",
            "Iteration 48, loss = 0.17426742\n",
            "Iteration 49, loss = 0.17349081\n",
            "Iteration 50, loss = 0.17277015\n",
            "Iteration 51, loss = 0.17205750\n",
            "Iteration 52, loss = 0.17137014\n",
            "Iteration 53, loss = 0.17068135\n",
            "Iteration 54, loss = 0.16993255\n",
            "Iteration 55, loss = 0.16921029\n",
            "Iteration 56, loss = 0.16850281\n",
            "Iteration 57, loss = 0.16769201\n",
            "Iteration 58, loss = 0.16700864\n",
            "Iteration 59, loss = 0.16620510\n",
            "Iteration 60, loss = 0.16549247\n",
            "Iteration 61, loss = 0.16478005\n",
            "Iteration 62, loss = 0.16402908\n",
            "Iteration 63, loss = 0.16323526\n",
            "Iteration 64, loss = 0.16240404\n",
            "Iteration 65, loss = 0.16162200\n",
            "Iteration 66, loss = 0.16085028\n",
            "Iteration 67, loss = 0.16015085\n",
            "Iteration 68, loss = 0.15934574\n",
            "Iteration 69, loss = 0.15844886\n",
            "Iteration 70, loss = 0.15767676\n",
            "Iteration 71, loss = 0.15683327\n",
            "Iteration 72, loss = 0.15601893\n",
            "Iteration 73, loss = 0.15512894\n",
            "Iteration 74, loss = 0.15435364\n",
            "Iteration 75, loss = 0.15354394\n",
            "Iteration 76, loss = 0.15266025\n",
            "Iteration 77, loss = 0.15185113\n",
            "Iteration 78, loss = 0.15103329\n",
            "Iteration 79, loss = 0.15018526\n",
            "Iteration 80, loss = 0.14923252\n",
            "Iteration 81, loss = 0.14839010\n",
            "Iteration 82, loss = 0.14739549\n",
            "Iteration 83, loss = 0.14667341\n",
            "Iteration 84, loss = 0.14551178\n",
            "Iteration 85, loss = 0.14471992\n",
            "Iteration 86, loss = 0.14388343\n",
            "Iteration 87, loss = 0.14274345\n",
            "Iteration 88, loss = 0.14186349\n",
            "Iteration 89, loss = 0.14095882\n",
            "Iteration 90, loss = 0.14006706\n",
            "Iteration 91, loss = 0.13905795\n",
            "Iteration 92, loss = 0.13808983\n",
            "Iteration 93, loss = 0.13714195\n",
            "Iteration 94, loss = 0.13609092\n",
            "Iteration 95, loss = 0.13522374\n",
            "Iteration 96, loss = 0.13415216\n",
            "Iteration 97, loss = 0.13324314\n",
            "Iteration 98, loss = 0.13228560\n",
            "Iteration 99, loss = 0.13140285\n",
            "Iteration 100, loss = 0.13032957\n",
            "Iteration 101, loss = 0.12938185\n",
            "Iteration 102, loss = 0.12846577\n",
            "Iteration 103, loss = 0.12738785\n",
            "Iteration 104, loss = 0.12648101\n",
            "Iteration 105, loss = 0.12558453\n",
            "Iteration 106, loss = 0.12465620\n",
            "Iteration 107, loss = 0.12373704\n",
            "Iteration 108, loss = 0.12273163\n",
            "Iteration 109, loss = 0.12174082\n",
            "Iteration 110, loss = 0.12083295\n",
            "Iteration 111, loss = 0.12002190\n",
            "Iteration 112, loss = 0.11906324\n",
            "Iteration 113, loss = 0.11818592\n",
            "Iteration 114, loss = 0.11715539\n",
            "Iteration 115, loss = 0.11625573\n",
            "Iteration 116, loss = 0.11551113\n",
            "Iteration 117, loss = 0.11462832\n",
            "Iteration 118, loss = 0.11366355\n",
            "Iteration 119, loss = 0.11278756\n",
            "Iteration 120, loss = 0.11200509\n",
            "Iteration 121, loss = 0.11102285\n",
            "Iteration 122, loss = 0.11021246\n",
            "Iteration 123, loss = 0.10933889\n",
            "Iteration 124, loss = 0.10845119\n",
            "Iteration 125, loss = 0.10760553\n",
            "Iteration 126, loss = 0.10681130\n",
            "Iteration 127, loss = 0.10593481\n",
            "Iteration 128, loss = 0.10521252\n",
            "Iteration 129, loss = 0.10438661\n",
            "Iteration 130, loss = 0.10345468\n",
            "Iteration 131, loss = 0.10275870\n",
            "Iteration 132, loss = 0.10185898\n",
            "Iteration 133, loss = 0.10107471\n",
            "Iteration 134, loss = 0.10031820\n",
            "Iteration 135, loss = 0.09947032\n",
            "Iteration 136, loss = 0.09875283\n",
            "Iteration 137, loss = 0.09796080\n",
            "Iteration 138, loss = 0.09710032\n",
            "Iteration 139, loss = 0.09651308\n",
            "Iteration 140, loss = 0.09570390\n",
            "Iteration 141, loss = 0.09493226\n",
            "Iteration 142, loss = 0.09414719\n",
            "Iteration 143, loss = 0.09353171\n",
            "Iteration 144, loss = 0.09278921\n",
            "Iteration 145, loss = 0.09188562\n",
            "Iteration 146, loss = 0.09127502\n",
            "Iteration 147, loss = 0.09045326\n",
            "Iteration 148, loss = 0.08989729\n",
            "Iteration 149, loss = 0.08916008\n",
            "Iteration 150, loss = 0.08856623\n",
            "Iteration 151, loss = 0.08776162\n",
            "Iteration 152, loss = 0.08711032\n",
            "Iteration 153, loss = 0.08651692\n",
            "Iteration 154, loss = 0.08576833\n",
            "Iteration 155, loss = 0.08517900\n",
            "Iteration 156, loss = 0.08453712\n",
            "Iteration 157, loss = 0.08390924\n",
            "Iteration 158, loss = 0.08331802\n",
            "Iteration 159, loss = 0.08264069\n",
            "Iteration 160, loss = 0.08201781\n",
            "Iteration 161, loss = 0.08138556\n",
            "Iteration 162, loss = 0.08083048\n",
            "Iteration 163, loss = 0.08022032\n",
            "Iteration 164, loss = 0.07957834\n",
            "Iteration 165, loss = 0.07903903\n",
            "Iteration 166, loss = 0.07841142\n",
            "Iteration 167, loss = 0.07783992\n",
            "Iteration 168, loss = 0.07727879\n",
            "Iteration 169, loss = 0.07674583\n",
            "Iteration 170, loss = 0.07641529\n",
            "Iteration 171, loss = 0.07556677\n",
            "Iteration 172, loss = 0.07502070\n",
            "Iteration 173, loss = 0.07451213\n",
            "Iteration 174, loss = 0.07402949\n",
            "Iteration 175, loss = 0.07350541\n",
            "Iteration 176, loss = 0.07298467\n",
            "Iteration 177, loss = 0.07241659\n",
            "Iteration 178, loss = 0.07185866\n",
            "Iteration 179, loss = 0.07136916\n",
            "Iteration 180, loss = 0.07092765\n",
            "Iteration 181, loss = 0.07046597\n",
            "Iteration 182, loss = 0.06998609\n",
            "Iteration 183, loss = 0.06944348\n",
            "Iteration 184, loss = 0.06889068\n",
            "Iteration 185, loss = 0.06852043\n",
            "Iteration 186, loss = 0.06791585\n",
            "Iteration 187, loss = 0.06740823\n",
            "Iteration 188, loss = 0.06700457\n",
            "Iteration 189, loss = 0.06648509\n",
            "Iteration 190, loss = 0.06610126\n",
            "Iteration 191, loss = 0.06558503\n",
            "Iteration 192, loss = 0.06512989\n",
            "Iteration 193, loss = 0.06476435\n",
            "Iteration 194, loss = 0.06421964\n",
            "Iteration 195, loss = 0.06382379\n",
            "Iteration 196, loss = 0.06335620\n",
            "Iteration 197, loss = 0.06308587\n",
            "Iteration 198, loss = 0.06266502\n",
            "Iteration 199, loss = 0.06219557\n",
            "Iteration 200, loss = 0.06173984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45782341\n",
            "Iteration 2, loss = 0.25379548\n",
            "Iteration 3, loss = 0.22322685\n",
            "Iteration 4, loss = 0.21450502\n",
            "Iteration 5, loss = 0.21107294\n",
            "Iteration 6, loss = 0.20944596\n",
            "Iteration 7, loss = 0.20837525\n",
            "Iteration 8, loss = 0.20775540\n",
            "Iteration 9, loss = 0.20718173\n",
            "Iteration 10, loss = 0.20652744\n",
            "Iteration 11, loss = 0.20582844\n",
            "Iteration 12, loss = 0.20513807\n",
            "Iteration 13, loss = 0.20439834\n",
            "Iteration 14, loss = 0.20357279\n",
            "Iteration 15, loss = 0.20291304\n",
            "Iteration 16, loss = 0.20194257\n",
            "Iteration 17, loss = 0.20096296\n",
            "Iteration 18, loss = 0.19999564\n",
            "Iteration 19, loss = 0.19890741\n",
            "Iteration 20, loss = 0.19789644\n",
            "Iteration 21, loss = 0.19679511\n",
            "Iteration 22, loss = 0.19582266\n",
            "Iteration 23, loss = 0.19480042\n",
            "Iteration 24, loss = 0.19382007\n",
            "Iteration 25, loss = 0.19271975\n",
            "Iteration 26, loss = 0.19183461\n",
            "Iteration 27, loss = 0.19084461\n",
            "Iteration 28, loss = 0.18992225\n",
            "Iteration 29, loss = 0.18896119\n",
            "Iteration 30, loss = 0.18799910\n",
            "Iteration 31, loss = 0.18717961\n",
            "Iteration 32, loss = 0.18624899\n",
            "Iteration 33, loss = 0.18535254\n",
            "Iteration 34, loss = 0.18462131\n",
            "Iteration 35, loss = 0.18375340\n",
            "Iteration 36, loss = 0.18294707\n",
            "Iteration 37, loss = 0.18222477\n",
            "Iteration 38, loss = 0.18144343\n",
            "Iteration 39, loss = 0.18059617\n",
            "Iteration 40, loss = 0.17992675\n",
            "Iteration 41, loss = 0.17913913\n",
            "Iteration 42, loss = 0.17846866\n",
            "Iteration 43, loss = 0.17779286\n",
            "Iteration 44, loss = 0.17709152\n",
            "Iteration 45, loss = 0.17633535\n",
            "Iteration 46, loss = 0.17570204\n",
            "Iteration 47, loss = 0.17504719\n",
            "Iteration 48, loss = 0.17434474\n",
            "Iteration 49, loss = 0.17374182\n",
            "Iteration 50, loss = 0.17298590\n",
            "Iteration 51, loss = 0.17229947\n",
            "Iteration 52, loss = 0.17174522\n",
            "Iteration 53, loss = 0.17102112\n",
            "Iteration 54, loss = 0.17025311\n",
            "Iteration 55, loss = 0.16966185\n",
            "Iteration 56, loss = 0.16891854\n",
            "Iteration 57, loss = 0.16828706\n",
            "Iteration 58, loss = 0.16754656\n",
            "Iteration 59, loss = 0.16689802\n",
            "Iteration 60, loss = 0.16611740\n",
            "Iteration 61, loss = 0.16554732\n",
            "Iteration 62, loss = 0.16477842\n",
            "Iteration 63, loss = 0.16404416\n",
            "Iteration 64, loss = 0.16342295\n",
            "Iteration 65, loss = 0.16267465\n",
            "Iteration 66, loss = 0.16197890\n",
            "Iteration 67, loss = 0.16121746\n",
            "Iteration 68, loss = 0.16049328\n",
            "Iteration 69, loss = 0.15979791\n",
            "Iteration 70, loss = 0.15892603\n",
            "Iteration 71, loss = 0.15826590\n",
            "Iteration 72, loss = 0.15756261\n",
            "Iteration 73, loss = 0.15672741\n",
            "Iteration 74, loss = 0.15593837\n",
            "Iteration 75, loss = 0.15522732\n",
            "Iteration 76, loss = 0.15432238\n",
            "Iteration 77, loss = 0.15350967\n",
            "Iteration 78, loss = 0.15273414\n",
            "Iteration 79, loss = 0.15196799\n",
            "Iteration 80, loss = 0.15114343\n",
            "Iteration 81, loss = 0.15033282\n",
            "Iteration 82, loss = 0.14951717\n",
            "Iteration 83, loss = 0.14862503\n",
            "Iteration 84, loss = 0.14782578\n",
            "Iteration 85, loss = 0.14700346\n",
            "Iteration 86, loss = 0.14612605\n",
            "Iteration 87, loss = 0.14525262\n",
            "Iteration 88, loss = 0.14444274\n",
            "Iteration 89, loss = 0.14358473\n",
            "Iteration 90, loss = 0.14271881\n",
            "Iteration 91, loss = 0.14189583\n",
            "Iteration 92, loss = 0.14097782\n",
            "Iteration 93, loss = 0.14025042\n",
            "Iteration 94, loss = 0.13933715\n",
            "Iteration 95, loss = 0.13839596\n",
            "Iteration 96, loss = 0.13751268\n",
            "Iteration 97, loss = 0.13657385\n",
            "Iteration 98, loss = 0.13566697\n",
            "Iteration 99, loss = 0.13483568\n",
            "Iteration 100, loss = 0.13388919\n",
            "Iteration 101, loss = 0.13301137\n",
            "Iteration 102, loss = 0.13211749\n",
            "Iteration 103, loss = 0.13112323\n",
            "Iteration 104, loss = 0.13023991\n",
            "Iteration 105, loss = 0.12951930\n",
            "Iteration 106, loss = 0.12841474\n",
            "Iteration 107, loss = 0.12753992\n",
            "Iteration 108, loss = 0.12666554\n",
            "Iteration 109, loss = 0.12583957\n",
            "Iteration 110, loss = 0.12480098\n",
            "Iteration 111, loss = 0.12397779\n",
            "Iteration 112, loss = 0.12302487\n",
            "Iteration 113, loss = 0.12221735\n",
            "Iteration 114, loss = 0.12131800\n",
            "Iteration 115, loss = 0.12040803\n",
            "Iteration 116, loss = 0.11949426\n",
            "Iteration 117, loss = 0.11865418\n",
            "Iteration 118, loss = 0.11776278\n",
            "Iteration 119, loss = 0.11679242\n",
            "Iteration 120, loss = 0.11600645\n",
            "Iteration 121, loss = 0.11496556\n",
            "Iteration 122, loss = 0.11431982\n",
            "Iteration 123, loss = 0.11336080\n",
            "Iteration 124, loss = 0.11245261\n",
            "Iteration 125, loss = 0.11166665\n",
            "Iteration 126, loss = 0.11077973\n",
            "Iteration 127, loss = 0.10987439\n",
            "Iteration 128, loss = 0.10915572\n",
            "Iteration 129, loss = 0.10828221\n",
            "Iteration 130, loss = 0.10744517\n",
            "Iteration 131, loss = 0.10649316\n",
            "Iteration 132, loss = 0.10566755\n",
            "Iteration 133, loss = 0.10493283\n",
            "Iteration 134, loss = 0.10419327\n",
            "Iteration 135, loss = 0.10333130\n",
            "Iteration 136, loss = 0.10252637\n",
            "Iteration 137, loss = 0.10180519\n",
            "Iteration 138, loss = 0.10093011\n",
            "Iteration 139, loss = 0.10011958\n",
            "Iteration 140, loss = 0.09936951\n",
            "Iteration 141, loss = 0.09866595\n",
            "Iteration 142, loss = 0.09786061\n",
            "Iteration 143, loss = 0.09714528\n",
            "Iteration 144, loss = 0.09642772\n",
            "Iteration 145, loss = 0.09558592\n",
            "Iteration 146, loss = 0.09509918\n",
            "Iteration 147, loss = 0.09423206\n",
            "Iteration 148, loss = 0.09349345\n",
            "Iteration 149, loss = 0.09286194\n",
            "Iteration 150, loss = 0.09204528\n",
            "Iteration 151, loss = 0.09145802\n",
            "Iteration 152, loss = 0.09077296\n",
            "Iteration 153, loss = 0.09015832\n",
            "Iteration 154, loss = 0.08933977\n",
            "Iteration 155, loss = 0.08888949\n",
            "Iteration 156, loss = 0.08814279\n",
            "Iteration 157, loss = 0.08751209\n",
            "Iteration 158, loss = 0.08683570\n",
            "Iteration 159, loss = 0.08616106\n",
            "Iteration 160, loss = 0.08556147\n",
            "Iteration 161, loss = 0.08489536\n",
            "Iteration 162, loss = 0.08436098\n",
            "Iteration 163, loss = 0.08370446\n",
            "Iteration 164, loss = 0.08316199\n",
            "Iteration 165, loss = 0.08255790\n",
            "Iteration 166, loss = 0.08189852\n",
            "Iteration 167, loss = 0.08147819\n",
            "Iteration 168, loss = 0.08072779\n",
            "Iteration 169, loss = 0.08009767\n",
            "Iteration 170, loss = 0.07964451\n",
            "Iteration 171, loss = 0.07915504\n",
            "Iteration 172, loss = 0.07840232\n",
            "Iteration 173, loss = 0.07796951\n",
            "Iteration 174, loss = 0.07753378\n",
            "Iteration 175, loss = 0.07686123\n",
            "Iteration 176, loss = 0.07622231\n",
            "Iteration 177, loss = 0.07581725\n",
            "Iteration 178, loss = 0.07529231\n",
            "Iteration 179, loss = 0.07482392\n",
            "Iteration 180, loss = 0.07427749\n",
            "Iteration 181, loss = 0.07381275\n",
            "Iteration 182, loss = 0.07324530\n",
            "Iteration 183, loss = 0.07273914\n",
            "Iteration 184, loss = 0.07228800\n",
            "Iteration 185, loss = 0.07179854\n",
            "Iteration 186, loss = 0.07126566\n",
            "Iteration 187, loss = 0.07077033\n",
            "Iteration 188, loss = 0.07034778\n",
            "Iteration 189, loss = 0.06993641\n",
            "Iteration 190, loss = 0.06937818\n",
            "Iteration 191, loss = 0.06909883\n",
            "Iteration 192, loss = 0.06852892\n",
            "Iteration 193, loss = 0.06809342\n",
            "Iteration 194, loss = 0.06768033\n",
            "Iteration 195, loss = 0.06723544\n",
            "Iteration 196, loss = 0.06677194\n",
            "Iteration 197, loss = 0.06627223\n",
            "Iteration 198, loss = 0.06589892\n",
            "Iteration 199, loss = 0.06535955\n",
            "Iteration 200, loss = 0.06505877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.34377257\n",
            "Iteration 2, loss = 0.25633151\n",
            "Iteration 3, loss = 0.25149072\n",
            "Iteration 4, loss = 0.24876497\n",
            "Iteration 5, loss = 0.24634436\n",
            "Iteration 6, loss = 0.24292826\n",
            "Iteration 7, loss = 0.24063358\n",
            "Iteration 8, loss = 0.23878755\n",
            "Iteration 9, loss = 0.23551053\n",
            "Iteration 10, loss = 0.23352647\n",
            "Iteration 11, loss = 0.23039625\n",
            "Iteration 12, loss = 0.22752010\n",
            "Iteration 13, loss = 0.22348997\n",
            "Iteration 14, loss = 0.22000487\n",
            "Iteration 15, loss = 0.21613952\n",
            "Iteration 16, loss = 0.21209475\n",
            "Iteration 17, loss = 0.20791884\n",
            "Iteration 18, loss = 0.20396968\n",
            "Iteration 19, loss = 0.20013635\n",
            "Iteration 20, loss = 0.19642108\n",
            "Iteration 21, loss = 0.19269772\n",
            "Iteration 22, loss = 0.18991122\n",
            "Iteration 23, loss = 0.18682815\n",
            "Iteration 24, loss = 0.18364734\n",
            "Iteration 25, loss = 0.18106422\n",
            "Iteration 26, loss = 0.17837826\n",
            "Iteration 27, loss = 0.17549020\n",
            "Iteration 28, loss = 0.17316377\n",
            "Iteration 29, loss = 0.17066306\n",
            "Iteration 30, loss = 0.16887459\n",
            "Iteration 31, loss = 0.16621997\n",
            "Iteration 32, loss = 0.16502071\n",
            "Iteration 33, loss = 0.16375459\n",
            "Iteration 34, loss = 0.16180843\n",
            "Iteration 35, loss = 0.16041882\n",
            "Iteration 36, loss = 0.15917659\n",
            "Iteration 37, loss = 0.15745478\n",
            "Iteration 38, loss = 0.15746427\n",
            "Iteration 39, loss = 0.15642728\n",
            "Iteration 40, loss = 0.15590887\n",
            "Iteration 41, loss = 0.15522291\n",
            "Iteration 42, loss = 0.15496381\n",
            "Iteration 43, loss = 0.15384287\n",
            "Iteration 44, loss = 0.15388291\n",
            "Iteration 45, loss = 0.15298138\n",
            "Iteration 46, loss = 0.15278680\n",
            "Iteration 47, loss = 0.15305852\n",
            "Iteration 48, loss = 0.15200406\n",
            "Iteration 49, loss = 0.15210010\n",
            "Iteration 50, loss = 0.15192554\n",
            "Iteration 51, loss = 0.15098033\n",
            "Iteration 52, loss = 0.15078301\n",
            "Iteration 53, loss = 0.15118942\n",
            "Iteration 54, loss = 0.15110753\n",
            "Iteration 55, loss = 0.14988305\n",
            "Iteration 56, loss = 0.14996651\n",
            "Iteration 57, loss = 0.15064920\n",
            "Iteration 58, loss = 0.15008017\n",
            "Iteration 59, loss = 0.14943649\n",
            "Iteration 60, loss = 0.14945513\n",
            "Iteration 61, loss = 0.14899183\n",
            "Iteration 62, loss = 0.14902489\n",
            "Iteration 63, loss = 0.14958045\n",
            "Iteration 64, loss = 0.14890172\n",
            "Iteration 65, loss = 0.14905702\n",
            "Iteration 66, loss = 0.14870302\n",
            "Iteration 67, loss = 0.14847412\n",
            "Iteration 68, loss = 0.14858879\n",
            "Iteration 69, loss = 0.14827910\n",
            "Iteration 70, loss = 0.14811425\n",
            "Iteration 71, loss = 0.14805841\n",
            "Iteration 72, loss = 0.14822911\n",
            "Iteration 73, loss = 0.14788834\n",
            "Iteration 74, loss = 0.14766237\n",
            "Iteration 75, loss = 0.14744748\n",
            "Iteration 76, loss = 0.14773607\n",
            "Iteration 77, loss = 0.14773608\n",
            "Iteration 78, loss = 0.14762771\n",
            "Iteration 79, loss = 0.14748830\n",
            "Iteration 80, loss = 0.14786883\n",
            "Iteration 81, loss = 0.14691431\n",
            "Iteration 82, loss = 0.14706539\n",
            "Iteration 83, loss = 0.14732280\n",
            "Iteration 84, loss = 0.14672923\n",
            "Iteration 85, loss = 0.14679226\n",
            "Iteration 86, loss = 0.14677869\n",
            "Iteration 87, loss = 0.14679231\n",
            "Iteration 88, loss = 0.14624127\n",
            "Iteration 89, loss = 0.14662967\n",
            "Iteration 90, loss = 0.14631585\n",
            "Iteration 91, loss = 0.14622813\n",
            "Iteration 92, loss = 0.14691572\n",
            "Iteration 93, loss = 0.14608268\n",
            "Iteration 94, loss = 0.14623163\n",
            "Iteration 95, loss = 0.14623880\n",
            "Iteration 96, loss = 0.14597579\n",
            "Iteration 97, loss = 0.14598870\n",
            "Iteration 98, loss = 0.14530527\n",
            "Iteration 99, loss = 0.14659099\n",
            "Iteration 100, loss = 0.14587595\n",
            "Iteration 101, loss = 0.14605220\n",
            "Iteration 102, loss = 0.14563733\n",
            "Iteration 103, loss = 0.14522454\n",
            "Iteration 104, loss = 0.14547797\n",
            "Iteration 105, loss = 0.14570106\n",
            "Iteration 106, loss = 0.14582441\n",
            "Iteration 107, loss = 0.14556853\n",
            "Iteration 108, loss = 0.14587099\n",
            "Iteration 109, loss = 0.14549725\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.34231006\n",
            "Iteration 2, loss = 0.25487460\n",
            "Iteration 3, loss = 0.25034335\n",
            "Iteration 4, loss = 0.24761788\n",
            "Iteration 5, loss = 0.24455578\n",
            "Iteration 6, loss = 0.24150067\n",
            "Iteration 7, loss = 0.23857630\n",
            "Iteration 8, loss = 0.23570759\n",
            "Iteration 9, loss = 0.23334793\n",
            "Iteration 10, loss = 0.23044726\n",
            "Iteration 11, loss = 0.22683593\n",
            "Iteration 12, loss = 0.22451234\n",
            "Iteration 13, loss = 0.22094310\n",
            "Iteration 14, loss = 0.21723699\n",
            "Iteration 15, loss = 0.21423004\n",
            "Iteration 16, loss = 0.21111233\n",
            "Iteration 17, loss = 0.20795255\n",
            "Iteration 18, loss = 0.20528708\n",
            "Iteration 19, loss = 0.20140921\n",
            "Iteration 20, loss = 0.19855959\n",
            "Iteration 21, loss = 0.19493254\n",
            "Iteration 22, loss = 0.19192294\n",
            "Iteration 23, loss = 0.18867084\n",
            "Iteration 24, loss = 0.18527936\n",
            "Iteration 25, loss = 0.18299598\n",
            "Iteration 26, loss = 0.17980873\n",
            "Iteration 27, loss = 0.17751619\n",
            "Iteration 28, loss = 0.17513611\n",
            "Iteration 29, loss = 0.17331526\n",
            "Iteration 30, loss = 0.17121971\n",
            "Iteration 31, loss = 0.16943736\n",
            "Iteration 32, loss = 0.16815313\n",
            "Iteration 33, loss = 0.16653329\n",
            "Iteration 34, loss = 0.16446588\n",
            "Iteration 35, loss = 0.16292507\n",
            "Iteration 36, loss = 0.16211802\n",
            "Iteration 37, loss = 0.16030643\n",
            "Iteration 38, loss = 0.15932185\n",
            "Iteration 39, loss = 0.15846788\n",
            "Iteration 40, loss = 0.15755268\n",
            "Iteration 41, loss = 0.15709851\n",
            "Iteration 42, loss = 0.15654182\n",
            "Iteration 43, loss = 0.15557614\n",
            "Iteration 44, loss = 0.15503195\n",
            "Iteration 45, loss = 0.15500588\n",
            "Iteration 46, loss = 0.15430593\n",
            "Iteration 47, loss = 0.15385562\n",
            "Iteration 48, loss = 0.15358833\n",
            "Iteration 49, loss = 0.15275074\n",
            "Iteration 50, loss = 0.15301461\n",
            "Iteration 51, loss = 0.15258031\n",
            "Iteration 52, loss = 0.15171443\n",
            "Iteration 53, loss = 0.15195506\n",
            "Iteration 54, loss = 0.15175774\n",
            "Iteration 55, loss = 0.15109710\n",
            "Iteration 56, loss = 0.15105709\n",
            "Iteration 57, loss = 0.15079891\n",
            "Iteration 58, loss = 0.15039962\n",
            "Iteration 59, loss = 0.15048002\n",
            "Iteration 60, loss = 0.15096981\n",
            "Iteration 61, loss = 0.14955634\n",
            "Iteration 62, loss = 0.14991755\n",
            "Iteration 63, loss = 0.15015878\n",
            "Iteration 64, loss = 0.15002543\n",
            "Iteration 65, loss = 0.14902679\n",
            "Iteration 66, loss = 0.14941092\n",
            "Iteration 67, loss = 0.14903726\n",
            "Iteration 68, loss = 0.14901031\n",
            "Iteration 69, loss = 0.14911526\n",
            "Iteration 70, loss = 0.14927698\n",
            "Iteration 71, loss = 0.14830654\n",
            "Iteration 72, loss = 0.14867216\n",
            "Iteration 73, loss = 0.14867375\n",
            "Iteration 74, loss = 0.14837419\n",
            "Iteration 75, loss = 0.14848620\n",
            "Iteration 76, loss = 0.14858153\n",
            "Iteration 77, loss = 0.14802357\n",
            "Iteration 78, loss = 0.14828839\n",
            "Iteration 79, loss = 0.14800432\n",
            "Iteration 80, loss = 0.14787558\n",
            "Iteration 81, loss = 0.14776344\n",
            "Iteration 82, loss = 0.14766092\n",
            "Iteration 83, loss = 0.14732041\n",
            "Iteration 84, loss = 0.14758630\n",
            "Iteration 85, loss = 0.14758700\n",
            "Iteration 86, loss = 0.14716158\n",
            "Iteration 87, loss = 0.14740765\n",
            "Iteration 88, loss = 0.14712913\n",
            "Iteration 89, loss = 0.14719138\n",
            "Iteration 90, loss = 0.14746640\n",
            "Iteration 91, loss = 0.14696444\n",
            "Iteration 92, loss = 0.14675713\n",
            "Iteration 93, loss = 0.14661770\n",
            "Iteration 94, loss = 0.14676334\n",
            "Iteration 95, loss = 0.14672095\n",
            "Iteration 96, loss = 0.14651143\n",
            "Iteration 97, loss = 0.14676450\n",
            "Iteration 98, loss = 0.14626732\n",
            "Iteration 99, loss = 0.14705963\n",
            "Iteration 100, loss = 0.14602754\n",
            "Iteration 101, loss = 0.14649551\n",
            "Iteration 102, loss = 0.14651666\n",
            "Iteration 103, loss = 0.14632425\n",
            "Iteration 104, loss = 0.14619325\n",
            "Iteration 105, loss = 0.14588180\n",
            "Iteration 106, loss = 0.14599010\n",
            "Iteration 107, loss = 0.14594498\n",
            "Iteration 108, loss = 0.14620302\n",
            "Iteration 109, loss = 0.14580772\n",
            "Iteration 110, loss = 0.14583258\n",
            "Iteration 111, loss = 0.14599725\n",
            "Iteration 112, loss = 0.14567512\n",
            "Iteration 113, loss = 0.14601762\n",
            "Iteration 114, loss = 0.14522817\n",
            "Iteration 115, loss = 0.14573371\n",
            "Iteration 116, loss = 0.14544016\n",
            "Iteration 117, loss = 0.14560666\n",
            "Iteration 118, loss = 0.14548984\n",
            "Iteration 119, loss = 0.14546537\n",
            "Iteration 120, loss = 0.14520104\n",
            "Iteration 121, loss = 0.14540429\n",
            "Iteration 122, loss = 0.14566642\n",
            "Iteration 123, loss = 0.14556635\n",
            "Iteration 124, loss = 0.14543525\n",
            "Iteration 125, loss = 0.14512581\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.34275496\n",
            "Iteration 2, loss = 0.25539105\n",
            "Iteration 3, loss = 0.25068078\n",
            "Iteration 4, loss = 0.24779365\n",
            "Iteration 5, loss = 0.24506877\n",
            "Iteration 6, loss = 0.24243317\n",
            "Iteration 7, loss = 0.23929973\n",
            "Iteration 8, loss = 0.23674016\n",
            "Iteration 9, loss = 0.23377466\n",
            "Iteration 10, loss = 0.23071042\n",
            "Iteration 11, loss = 0.22809389\n",
            "Iteration 12, loss = 0.22440159\n",
            "Iteration 13, loss = 0.22131838\n",
            "Iteration 14, loss = 0.21756969\n",
            "Iteration 15, loss = 0.21409657\n",
            "Iteration 16, loss = 0.21016226\n",
            "Iteration 17, loss = 0.20645432\n",
            "Iteration 18, loss = 0.20249237\n",
            "Iteration 19, loss = 0.19858698\n",
            "Iteration 20, loss = 0.19454918\n",
            "Iteration 21, loss = 0.19083005\n",
            "Iteration 22, loss = 0.18681719\n",
            "Iteration 23, loss = 0.18364683\n",
            "Iteration 24, loss = 0.18033262\n",
            "Iteration 25, loss = 0.17713120\n",
            "Iteration 26, loss = 0.17459393\n",
            "Iteration 27, loss = 0.17196418\n",
            "Iteration 28, loss = 0.16951608\n",
            "Iteration 29, loss = 0.16782272\n",
            "Iteration 30, loss = 0.16610084\n",
            "Iteration 31, loss = 0.16521394\n",
            "Iteration 32, loss = 0.16335456\n",
            "Iteration 33, loss = 0.16182682\n",
            "Iteration 34, loss = 0.16067068\n",
            "Iteration 35, loss = 0.15960721\n",
            "Iteration 36, loss = 0.15806552\n",
            "Iteration 37, loss = 0.15761702\n",
            "Iteration 38, loss = 0.15636229\n",
            "Iteration 39, loss = 0.15550655\n",
            "Iteration 40, loss = 0.15437354\n",
            "Iteration 41, loss = 0.15373826\n",
            "Iteration 42, loss = 0.15343430\n",
            "Iteration 43, loss = 0.15265388\n",
            "Iteration 44, loss = 0.15241033\n",
            "Iteration 45, loss = 0.15139082\n",
            "Iteration 46, loss = 0.15080228\n",
            "Iteration 47, loss = 0.15064863\n",
            "Iteration 48, loss = 0.15031791\n",
            "Iteration 49, loss = 0.14967510\n",
            "Iteration 50, loss = 0.14981058\n",
            "Iteration 51, loss = 0.14933400\n",
            "Iteration 52, loss = 0.14889487\n",
            "Iteration 53, loss = 0.14882372\n",
            "Iteration 54, loss = 0.14889514\n",
            "Iteration 55, loss = 0.14771055\n",
            "Iteration 56, loss = 0.14817521\n",
            "Iteration 57, loss = 0.14786456\n",
            "Iteration 58, loss = 0.14825888\n",
            "Iteration 59, loss = 0.14736937\n",
            "Iteration 60, loss = 0.14817002\n",
            "Iteration 61, loss = 0.14712081\n",
            "Iteration 62, loss = 0.14770223\n",
            "Iteration 63, loss = 0.14681646\n",
            "Iteration 64, loss = 0.14727153\n",
            "Iteration 65, loss = 0.14683054\n",
            "Iteration 66, loss = 0.14669624\n",
            "Iteration 67, loss = 0.14646884\n",
            "Iteration 68, loss = 0.14609652\n",
            "Iteration 69, loss = 0.14632857\n",
            "Iteration 70, loss = 0.14590776\n",
            "Iteration 71, loss = 0.14611266\n",
            "Iteration 72, loss = 0.14602192\n",
            "Iteration 73, loss = 0.14605631\n",
            "Iteration 74, loss = 0.14565756\n",
            "Iteration 75, loss = 0.14569635\n",
            "Iteration 76, loss = 0.14567343\n",
            "Iteration 77, loss = 0.14538794\n",
            "Iteration 78, loss = 0.14547393\n",
            "Iteration 79, loss = 0.14479115\n",
            "Iteration 80, loss = 0.14474634\n",
            "Iteration 81, loss = 0.14508645\n",
            "Iteration 82, loss = 0.14522758\n",
            "Iteration 83, loss = 0.14533338\n",
            "Iteration 84, loss = 0.14487149\n",
            "Iteration 85, loss = 0.14507166\n",
            "Iteration 86, loss = 0.14465002\n",
            "Iteration 87, loss = 0.14457961\n",
            "Iteration 88, loss = 0.14460379\n",
            "Iteration 89, loss = 0.14431882\n",
            "Iteration 90, loss = 0.14471072\n",
            "Iteration 91, loss = 0.14459076\n",
            "Iteration 92, loss = 0.14448863\n",
            "Iteration 93, loss = 0.14415611\n",
            "Iteration 94, loss = 0.14413056\n",
            "Iteration 95, loss = 0.14390724\n",
            "Iteration 96, loss = 0.14392573\n",
            "Iteration 97, loss = 0.14447262\n",
            "Iteration 98, loss = 0.14383747\n",
            "Iteration 99, loss = 0.14368312\n",
            "Iteration 100, loss = 0.14431975\n",
            "Iteration 101, loss = 0.14399997\n",
            "Iteration 102, loss = 0.14362662\n",
            "Iteration 103, loss = 0.14379638\n",
            "Iteration 104, loss = 0.14350211\n",
            "Iteration 105, loss = 0.14373085\n",
            "Iteration 106, loss = 0.14385928\n",
            "Iteration 107, loss = 0.14388765\n",
            "Iteration 108, loss = 0.14347971\n",
            "Iteration 109, loss = 0.14369939\n",
            "Iteration 110, loss = 0.14384804\n",
            "Iteration 111, loss = 0.14299926\n",
            "Iteration 112, loss = 0.14333487\n",
            "Iteration 113, loss = 0.14350155\n",
            "Iteration 114, loss = 0.14297589\n",
            "Iteration 115, loss = 0.14335140\n",
            "Iteration 116, loss = 0.14366282\n",
            "Iteration 117, loss = 0.14330450\n",
            "Iteration 118, loss = 0.14286889\n",
            "Iteration 119, loss = 0.14302227\n",
            "Iteration 120, loss = 0.14361568\n",
            "Iteration 121, loss = 0.14306623\n",
            "Iteration 122, loss = 0.14273465\n",
            "Iteration 123, loss = 0.14293733\n",
            "Iteration 124, loss = 0.14283487\n",
            "Iteration 125, loss = 0.14297903\n",
            "Iteration 126, loss = 0.14314681\n",
            "Iteration 127, loss = 0.14309777\n",
            "Iteration 128, loss = 0.14282441\n",
            "Iteration 129, loss = 0.14216857\n",
            "Iteration 130, loss = 0.14290301\n",
            "Iteration 131, loss = 0.14241400\n",
            "Iteration 132, loss = 0.14306481\n",
            "Iteration 133, loss = 0.14258973\n",
            "Iteration 134, loss = 0.14245280\n",
            "Iteration 135, loss = 0.14264078\n",
            "Iteration 136, loss = 0.14276645\n",
            "Iteration 137, loss = 0.14242453\n",
            "Iteration 138, loss = 0.14252114\n",
            "Iteration 139, loss = 0.14198209\n",
            "Iteration 140, loss = 0.14254103\n",
            "Iteration 141, loss = 0.14237490\n",
            "Iteration 142, loss = 0.14240887\n",
            "Iteration 143, loss = 0.14230697\n",
            "Iteration 144, loss = 0.14264899\n",
            "Iteration 145, loss = 0.14202720\n",
            "Iteration 146, loss = 0.14235403\n",
            "Iteration 147, loss = 0.14200846\n",
            "Iteration 148, loss = 0.14197483\n",
            "Iteration 149, loss = 0.14222121\n",
            "Iteration 150, loss = 0.14177440\n",
            "Iteration 151, loss = 0.14158795\n",
            "Iteration 152, loss = 0.14209427\n",
            "Iteration 153, loss = 0.14255468\n",
            "Iteration 154, loss = 0.14194123\n",
            "Iteration 155, loss = 0.14198585\n",
            "Iteration 156, loss = 0.14235249\n",
            "Iteration 157, loss = 0.14187836\n",
            "Iteration 158, loss = 0.14137256\n",
            "Iteration 159, loss = 0.14205365\n",
            "Iteration 160, loss = 0.14232479\n",
            "Iteration 161, loss = 0.14167753\n",
            "Iteration 162, loss = 0.14220360\n",
            "Iteration 163, loss = 0.14182233\n",
            "Iteration 164, loss = 0.14149517\n",
            "Iteration 165, loss = 0.14148733\n",
            "Iteration 166, loss = 0.14208323\n",
            "Iteration 167, loss = 0.14119584\n",
            "Iteration 168, loss = 0.14202273\n",
            "Iteration 169, loss = 0.14182007\n",
            "Iteration 170, loss = 0.14224438\n",
            "Iteration 171, loss = 0.14171536\n",
            "Iteration 172, loss = 0.14179812\n",
            "Iteration 173, loss = 0.14151158\n",
            "Iteration 174, loss = 0.14153481\n",
            "Iteration 175, loss = 0.14159350\n",
            "Iteration 176, loss = 0.14183003\n",
            "Iteration 177, loss = 0.14134863\n",
            "Iteration 178, loss = 0.14185135\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.30542467\n",
            "Iteration 2, loss = 0.21908467\n",
            "Iteration 3, loss = 0.21340741\n",
            "Iteration 4, loss = 0.20894164\n",
            "Iteration 5, loss = 0.20382031\n",
            "Iteration 6, loss = 0.19784826\n",
            "Iteration 7, loss = 0.19112466\n",
            "Iteration 8, loss = 0.18294809\n",
            "Iteration 9, loss = 0.17191473\n",
            "Iteration 10, loss = 0.15888190\n",
            "Iteration 11, loss = 0.14251659\n",
            "Iteration 12, loss = 0.12615583\n",
            "Iteration 13, loss = 0.10828295\n",
            "Iteration 14, loss = 0.09125482\n",
            "Iteration 15, loss = 0.07572480\n",
            "Iteration 16, loss = 0.06087776\n",
            "Iteration 17, loss = 0.04881357\n",
            "Iteration 18, loss = 0.03784768\n",
            "Iteration 19, loss = 0.02907853\n",
            "Iteration 20, loss = 0.02291706\n",
            "Iteration 21, loss = 0.01868948\n",
            "Iteration 22, loss = 0.01534331\n",
            "Iteration 23, loss = 0.01370921\n",
            "Iteration 24, loss = 0.01281352\n",
            "Iteration 25, loss = 0.01218422\n",
            "Iteration 26, loss = 0.01158791\n",
            "Iteration 27, loss = 0.00874046\n",
            "Iteration 28, loss = 0.00674652\n",
            "Iteration 29, loss = 0.00495383\n",
            "Iteration 30, loss = 0.00403080\n",
            "Iteration 31, loss = 0.00349615\n",
            "Iteration 32, loss = 0.00309784\n",
            "Iteration 33, loss = 0.00508760\n",
            "Iteration 34, loss = 0.01477261\n",
            "Iteration 35, loss = 0.01319507\n",
            "Iteration 36, loss = 0.00539320\n",
            "Iteration 37, loss = 0.00299763\n",
            "Iteration 38, loss = 0.00217993\n",
            "Iteration 39, loss = 0.00195730\n",
            "Iteration 40, loss = 0.00196012\n",
            "Iteration 41, loss = 0.00181539\n",
            "Iteration 42, loss = 0.00185877\n",
            "Iteration 43, loss = 0.00178422\n",
            "Iteration 44, loss = 0.00159017\n",
            "Iteration 45, loss = 0.00244059\n",
            "Iteration 46, loss = 0.01713186\n",
            "Iteration 47, loss = 0.01044919\n",
            "Iteration 48, loss = 0.00350310\n",
            "Iteration 49, loss = 0.00197424\n",
            "Iteration 50, loss = 0.00164172\n",
            "Iteration 51, loss = 0.00161890\n",
            "Iteration 52, loss = 0.00133499\n",
            "Iteration 53, loss = 0.00141774\n",
            "Iteration 54, loss = 0.00141817\n",
            "Iteration 55, loss = 0.00161326\n",
            "Iteration 56, loss = 0.00160427\n",
            "Iteration 57, loss = 0.00192322\n",
            "Iteration 58, loss = 0.00875828\n",
            "Iteration 59, loss = 0.00797443\n",
            "Iteration 60, loss = 0.00363059\n",
            "Iteration 61, loss = 0.00149692\n",
            "Iteration 62, loss = 0.00128143\n",
            "Iteration 63, loss = 0.00126378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.30436193\n",
            "Iteration 2, loss = 0.21731693\n",
            "Iteration 3, loss = 0.21208951\n",
            "Iteration 4, loss = 0.20644009\n",
            "Iteration 5, loss = 0.19887670\n",
            "Iteration 6, loss = 0.19013711\n",
            "Iteration 7, loss = 0.18042992\n",
            "Iteration 8, loss = 0.16897704\n",
            "Iteration 9, loss = 0.15646308\n",
            "Iteration 10, loss = 0.14183553\n",
            "Iteration 11, loss = 0.12531209\n",
            "Iteration 12, loss = 0.10941188\n",
            "Iteration 13, loss = 0.09247337\n",
            "Iteration 14, loss = 0.07630773\n",
            "Iteration 15, loss = 0.06200552\n",
            "Iteration 16, loss = 0.04946247\n",
            "Iteration 17, loss = 0.03905890\n",
            "Iteration 18, loss = 0.03056333\n",
            "Iteration 19, loss = 0.02374117\n",
            "Iteration 20, loss = 0.01919576\n",
            "Iteration 21, loss = 0.01682718\n",
            "Iteration 22, loss = 0.01520611\n",
            "Iteration 23, loss = 0.01298823\n",
            "Iteration 24, loss = 0.01063491\n",
            "Iteration 25, loss = 0.00864595\n",
            "Iteration 26, loss = 0.00678786\n",
            "Iteration 27, loss = 0.00602427\n",
            "Iteration 28, loss = 0.00574074\n",
            "Iteration 29, loss = 0.01207213\n",
            "Iteration 30, loss = 0.01262521\n",
            "Iteration 31, loss = 0.00757903\n",
            "Iteration 32, loss = 0.00358716\n",
            "Iteration 33, loss = 0.00250715\n",
            "Iteration 34, loss = 0.00192460\n",
            "Iteration 35, loss = 0.00175585\n",
            "Iteration 36, loss = 0.00162016\n",
            "Iteration 37, loss = 0.00161591\n",
            "Iteration 38, loss = 0.00148500\n",
            "Iteration 39, loss = 0.00180115\n",
            "Iteration 40, loss = 0.00645206\n",
            "Iteration 41, loss = 0.02046564\n",
            "Iteration 42, loss = 0.00835578\n",
            "Iteration 43, loss = 0.00287205\n",
            "Iteration 44, loss = 0.00143828\n",
            "Iteration 45, loss = 0.00121548\n",
            "Iteration 46, loss = 0.00112506\n",
            "Iteration 47, loss = 0.00118860\n",
            "Iteration 48, loss = 0.00109789\n",
            "Iteration 49, loss = 0.00112109\n",
            "Iteration 50, loss = 0.00098297\n",
            "Iteration 51, loss = 0.00104406\n",
            "Iteration 52, loss = 0.00103991\n",
            "Iteration 53, loss = 0.00102123\n",
            "Iteration 54, loss = 0.00372539\n",
            "Iteration 55, loss = 0.01619543\n",
            "Iteration 56, loss = 0.00819715\n",
            "Iteration 57, loss = 0.00239234\n",
            "Iteration 58, loss = 0.00111396\n",
            "Iteration 59, loss = 0.00083442\n",
            "Iteration 60, loss = 0.00084812\n",
            "Iteration 61, loss = 0.00086980\n",
            "Iteration 62, loss = 0.00090674\n",
            "Iteration 63, loss = 0.00090232\n",
            "Iteration 64, loss = 0.00087554\n",
            "Iteration 65, loss = 0.00098803\n",
            "Iteration 66, loss = 0.00090266\n",
            "Iteration 67, loss = 0.00088866\n",
            "Iteration 68, loss = 0.00088646\n",
            "Iteration 69, loss = 0.00097905\n",
            "Iteration 70, loss = 0.00618971\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.30381983\n",
            "Iteration 2, loss = 0.21741119\n",
            "Iteration 3, loss = 0.21210745\n",
            "Iteration 4, loss = 0.20753246\n",
            "Iteration 5, loss = 0.20222824\n",
            "Iteration 6, loss = 0.19628787\n",
            "Iteration 7, loss = 0.18900489\n",
            "Iteration 8, loss = 0.18053535\n",
            "Iteration 9, loss = 0.16925458\n",
            "Iteration 10, loss = 0.15617906\n",
            "Iteration 11, loss = 0.14061033\n",
            "Iteration 12, loss = 0.12306395\n",
            "Iteration 13, loss = 0.10459391\n",
            "Iteration 14, loss = 0.08645707\n",
            "Iteration 15, loss = 0.06913738\n",
            "Iteration 16, loss = 0.05453075\n",
            "Iteration 17, loss = 0.04250977\n",
            "Iteration 18, loss = 0.03237628\n",
            "Iteration 19, loss = 0.02574567\n",
            "Iteration 20, loss = 0.02006948\n",
            "Iteration 21, loss = 0.01604188\n",
            "Iteration 22, loss = 0.01371480\n",
            "Iteration 23, loss = 0.01257688\n",
            "Iteration 24, loss = 0.01126060\n",
            "Iteration 25, loss = 0.00944983\n",
            "Iteration 26, loss = 0.00900707\n",
            "Iteration 27, loss = 0.00762663\n",
            "Iteration 28, loss = 0.00666857\n",
            "Iteration 29, loss = 0.00553574\n",
            "Iteration 30, loss = 0.00564942\n",
            "Iteration 31, loss = 0.00656862\n",
            "Iteration 32, loss = 0.00520505\n",
            "Iteration 33, loss = 0.00381035\n",
            "Iteration 34, loss = 0.00284482\n",
            "Iteration 35, loss = 0.00258786\n",
            "Iteration 36, loss = 0.00285763\n",
            "Iteration 37, loss = 0.00546714\n",
            "Iteration 38, loss = 0.00708299\n",
            "Iteration 39, loss = 0.00712331\n",
            "Iteration 40, loss = 0.00435831\n",
            "Iteration 41, loss = 0.00240204\n",
            "Iteration 42, loss = 0.00178217\n",
            "Iteration 43, loss = 0.00151673\n",
            "Iteration 44, loss = 0.00132221\n",
            "Iteration 45, loss = 0.00145195\n",
            "Iteration 46, loss = 0.00142728\n",
            "Iteration 47, loss = 0.00116468\n",
            "Iteration 48, loss = 0.00165125\n",
            "Iteration 49, loss = 0.00325335\n",
            "Iteration 50, loss = 0.01164335\n",
            "Iteration 51, loss = 0.00641777\n",
            "Iteration 52, loss = 0.00333755\n",
            "Iteration 53, loss = 0.00164944\n",
            "Iteration 54, loss = 0.00109006\n",
            "Iteration 55, loss = 0.00115213\n",
            "Iteration 56, loss = 0.00110957\n",
            "Iteration 57, loss = 0.00107274\n",
            "Iteration 58, loss = 0.00117752\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.49758526\n",
            "Iteration 2, loss = 0.26086918\n",
            "Iteration 3, loss = 0.22321173\n",
            "Iteration 4, loss = 0.21527171\n",
            "Iteration 5, loss = 0.21213999\n",
            "Iteration 6, loss = 0.21030623\n",
            "Iteration 7, loss = 0.20897055\n",
            "Iteration 8, loss = 0.20784462\n",
            "Iteration 9, loss = 0.20668646\n",
            "Iteration 10, loss = 0.20560047\n",
            "Iteration 11, loss = 0.20453726\n",
            "Iteration 12, loss = 0.20331543\n",
            "Iteration 13, loss = 0.20202211\n",
            "Iteration 14, loss = 0.20087198\n",
            "Iteration 15, loss = 0.19948075\n",
            "Iteration 16, loss = 0.19808402\n",
            "Iteration 17, loss = 0.19675000\n",
            "Iteration 18, loss = 0.19528284\n",
            "Iteration 19, loss = 0.19392166\n",
            "Iteration 20, loss = 0.19252082\n",
            "Iteration 21, loss = 0.19110912\n",
            "Iteration 22, loss = 0.18971258\n",
            "Iteration 23, loss = 0.18838959\n",
            "Iteration 24, loss = 0.18708786\n",
            "Iteration 25, loss = 0.18568133\n",
            "Iteration 26, loss = 0.18419042\n",
            "Iteration 27, loss = 0.18291869\n",
            "Iteration 28, loss = 0.18165124\n",
            "Iteration 29, loss = 0.18036319\n",
            "Iteration 30, loss = 0.17935049\n",
            "Iteration 31, loss = 0.17804265\n",
            "Iteration 32, loss = 0.17703149\n",
            "Iteration 33, loss = 0.17592088\n",
            "Iteration 34, loss = 0.17469511\n",
            "Iteration 35, loss = 0.17359945\n",
            "Iteration 36, loss = 0.17269995\n",
            "Iteration 37, loss = 0.17164055\n",
            "Iteration 38, loss = 0.17065354\n",
            "Iteration 39, loss = 0.16972640\n",
            "Iteration 40, loss = 0.16868847\n",
            "Iteration 41, loss = 0.16785884\n",
            "Iteration 42, loss = 0.16694074\n",
            "Iteration 43, loss = 0.16601374\n",
            "Iteration 44, loss = 0.16500777\n",
            "Iteration 45, loss = 0.16429639\n",
            "Iteration 46, loss = 0.16343080\n",
            "Iteration 47, loss = 0.16256432\n",
            "Iteration 48, loss = 0.16158951\n",
            "Iteration 49, loss = 0.16066809\n",
            "Iteration 50, loss = 0.16000136\n",
            "Iteration 51, loss = 0.15902740\n",
            "Iteration 52, loss = 0.15817113\n",
            "Iteration 53, loss = 0.15752033\n",
            "Iteration 54, loss = 0.15665510\n",
            "Iteration 55, loss = 0.15590012\n",
            "Iteration 56, loss = 0.15524683\n",
            "Iteration 57, loss = 0.15447323\n",
            "Iteration 58, loss = 0.15376346\n",
            "Iteration 59, loss = 0.15292463\n",
            "Iteration 60, loss = 0.15213221\n",
            "Iteration 61, loss = 0.15144123\n",
            "Iteration 62, loss = 0.15080569\n",
            "Iteration 63, loss = 0.15013704\n",
            "Iteration 64, loss = 0.14946298\n",
            "Iteration 65, loss = 0.14885423\n",
            "Iteration 66, loss = 0.14818354\n",
            "Iteration 67, loss = 0.14749343\n",
            "Iteration 68, loss = 0.14693845\n",
            "Iteration 69, loss = 0.14631444\n",
            "Iteration 70, loss = 0.14563162\n",
            "Iteration 71, loss = 0.14503844\n",
            "Iteration 72, loss = 0.14460363\n",
            "Iteration 73, loss = 0.14402793\n",
            "Iteration 74, loss = 0.14350090\n",
            "Iteration 75, loss = 0.14282878\n",
            "Iteration 76, loss = 0.14226121\n",
            "Iteration 77, loss = 0.14172395\n",
            "Iteration 78, loss = 0.14144699\n",
            "Iteration 79, loss = 0.14079981\n",
            "Iteration 80, loss = 0.14019516\n",
            "Iteration 81, loss = 0.13982328\n",
            "Iteration 82, loss = 0.13929109\n",
            "Iteration 83, loss = 0.13885947\n",
            "Iteration 84, loss = 0.13850597\n",
            "Iteration 85, loss = 0.13816186\n",
            "Iteration 86, loss = 0.13747967\n",
            "Iteration 87, loss = 0.13713407\n",
            "Iteration 88, loss = 0.13659627\n",
            "Iteration 89, loss = 0.13651747\n",
            "Iteration 90, loss = 0.13614127\n",
            "Iteration 91, loss = 0.13551670\n",
            "Iteration 92, loss = 0.13519300\n",
            "Iteration 93, loss = 0.13484103\n",
            "Iteration 94, loss = 0.13444373\n",
            "Iteration 95, loss = 0.13422770\n",
            "Iteration 96, loss = 0.13373894\n",
            "Iteration 97, loss = 0.13324209\n",
            "Iteration 98, loss = 0.13306299\n",
            "Iteration 99, loss = 0.13277600\n",
            "Iteration 100, loss = 0.13238205\n",
            "Iteration 101, loss = 0.13209876\n",
            "Iteration 102, loss = 0.13160144\n",
            "Iteration 103, loss = 0.13122199\n",
            "Iteration 104, loss = 0.13115929\n",
            "Iteration 105, loss = 0.13081539\n",
            "Iteration 106, loss = 0.13052958\n",
            "Iteration 107, loss = 0.13000047\n",
            "Iteration 108, loss = 0.12990035\n",
            "Iteration 109, loss = 0.12967241\n",
            "Iteration 110, loss = 0.12918979\n",
            "Iteration 111, loss = 0.12893704\n",
            "Iteration 112, loss = 0.12886734\n",
            "Iteration 113, loss = 0.12860179\n",
            "Iteration 114, loss = 0.12821578\n",
            "Iteration 115, loss = 0.12813692\n",
            "Iteration 116, loss = 0.12760175\n",
            "Iteration 117, loss = 0.12767269\n",
            "Iteration 118, loss = 0.12715046\n",
            "Iteration 119, loss = 0.12691216\n",
            "Iteration 120, loss = 0.12673659\n",
            "Iteration 121, loss = 0.12660143\n",
            "Iteration 122, loss = 0.12603578\n",
            "Iteration 123, loss = 0.12612433\n",
            "Iteration 124, loss = 0.12572911\n",
            "Iteration 125, loss = 0.12535058\n",
            "Iteration 126, loss = 0.12541437\n",
            "Iteration 127, loss = 0.12521397\n",
            "Iteration 128, loss = 0.12471451\n",
            "Iteration 129, loss = 0.12465497\n",
            "Iteration 130, loss = 0.12445226\n",
            "Iteration 131, loss = 0.12423088\n",
            "Iteration 132, loss = 0.12408846\n",
            "Iteration 133, loss = 0.12395397\n",
            "Iteration 134, loss = 0.12351105\n",
            "Iteration 135, loss = 0.12343472\n",
            "Iteration 136, loss = 0.12320712\n",
            "Iteration 137, loss = 0.12350717\n",
            "Iteration 138, loss = 0.12284477\n",
            "Iteration 139, loss = 0.12290707\n",
            "Iteration 140, loss = 0.12266048\n",
            "Iteration 141, loss = 0.12238226\n",
            "Iteration 142, loss = 0.12232214\n",
            "Iteration 143, loss = 0.12224850\n",
            "Iteration 144, loss = 0.12187785\n",
            "Iteration 145, loss = 0.12199539\n",
            "Iteration 146, loss = 0.12194101\n",
            "Iteration 147, loss = 0.12159340\n",
            "Iteration 148, loss = 0.12149031\n",
            "Iteration 149, loss = 0.12137769\n",
            "Iteration 150, loss = 0.12108363\n",
            "Iteration 151, loss = 0.12096276\n",
            "Iteration 152, loss = 0.12071594\n",
            "Iteration 153, loss = 0.12067783\n",
            "Iteration 154, loss = 0.12082927\n",
            "Iteration 155, loss = 0.12042987\n",
            "Iteration 156, loss = 0.12042955\n",
            "Iteration 157, loss = 0.12011950\n",
            "Iteration 158, loss = 0.12002780\n",
            "Iteration 159, loss = 0.12019405\n",
            "Iteration 160, loss = 0.11972175\n",
            "Iteration 161, loss = 0.11965591\n",
            "Iteration 162, loss = 0.11969550\n",
            "Iteration 163, loss = 0.11947389\n",
            "Iteration 164, loss = 0.11926100\n",
            "Iteration 165, loss = 0.11910581\n",
            "Iteration 166, loss = 0.11908536\n",
            "Iteration 167, loss = 0.11899191\n",
            "Iteration 168, loss = 0.11895300\n",
            "Iteration 169, loss = 0.11869411\n",
            "Iteration 170, loss = 0.11873804\n",
            "Iteration 171, loss = 0.11868071\n",
            "Iteration 172, loss = 0.11851714\n",
            "Iteration 173, loss = 0.11840211\n",
            "Iteration 174, loss = 0.11825391\n",
            "Iteration 175, loss = 0.11832168\n",
            "Iteration 176, loss = 0.11791841\n",
            "Iteration 177, loss = 0.11822488\n",
            "Iteration 178, loss = 0.11788070\n",
            "Iteration 179, loss = 0.11797696\n",
            "Iteration 180, loss = 0.11773313\n",
            "Iteration 181, loss = 0.11747847\n",
            "Iteration 182, loss = 0.11745225\n",
            "Iteration 183, loss = 0.11738218\n",
            "Iteration 184, loss = 0.11731299\n",
            "Iteration 185, loss = 0.11734646\n",
            "Iteration 186, loss = 0.11701287\n",
            "Iteration 187, loss = 0.11707218\n",
            "Iteration 188, loss = 0.11689779\n",
            "Iteration 189, loss = 0.11685740\n",
            "Iteration 190, loss = 0.11657392\n",
            "Iteration 191, loss = 0.11677660\n",
            "Iteration 192, loss = 0.11654876\n",
            "Iteration 193, loss = 0.11641610\n",
            "Iteration 194, loss = 0.11647256\n",
            "Iteration 195, loss = 0.11612286\n",
            "Iteration 196, loss = 0.11618888\n",
            "Iteration 197, loss = 0.11614658\n",
            "Iteration 198, loss = 0.11609156\n",
            "Iteration 199, loss = 0.11606218\n",
            "Iteration 200, loss = 0.11600887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.49632564\n",
            "Iteration 2, loss = 0.25934812\n",
            "Iteration 3, loss = 0.22183018\n",
            "Iteration 4, loss = 0.21395309\n",
            "Iteration 5, loss = 0.21066386\n",
            "Iteration 6, loss = 0.20892141\n",
            "Iteration 7, loss = 0.20746579\n",
            "Iteration 8, loss = 0.20645036\n",
            "Iteration 9, loss = 0.20519590\n",
            "Iteration 10, loss = 0.20419996\n",
            "Iteration 11, loss = 0.20298307\n",
            "Iteration 12, loss = 0.20188762\n",
            "Iteration 13, loss = 0.20070806\n",
            "Iteration 14, loss = 0.19950963\n",
            "Iteration 15, loss = 0.19844567\n",
            "Iteration 16, loss = 0.19726201\n",
            "Iteration 17, loss = 0.19610316\n",
            "Iteration 18, loss = 0.19481220\n",
            "Iteration 19, loss = 0.19357787\n",
            "Iteration 20, loss = 0.19236900\n",
            "Iteration 21, loss = 0.19107729\n",
            "Iteration 22, loss = 0.19009696\n",
            "Iteration 23, loss = 0.18869645\n",
            "Iteration 24, loss = 0.18766803\n",
            "Iteration 25, loss = 0.18647947\n",
            "Iteration 26, loss = 0.18526337\n",
            "Iteration 27, loss = 0.18409474\n",
            "Iteration 28, loss = 0.18302903\n",
            "Iteration 29, loss = 0.18196713\n",
            "Iteration 30, loss = 0.18078454\n",
            "Iteration 31, loss = 0.17975082\n",
            "Iteration 32, loss = 0.17871720\n",
            "Iteration 33, loss = 0.17779123\n",
            "Iteration 34, loss = 0.17678309\n",
            "Iteration 35, loss = 0.17578478\n",
            "Iteration 36, loss = 0.17470337\n",
            "Iteration 37, loss = 0.17396007\n",
            "Iteration 38, loss = 0.17296111\n",
            "Iteration 39, loss = 0.17194140\n",
            "Iteration 40, loss = 0.17115115\n",
            "Iteration 41, loss = 0.17022290\n",
            "Iteration 42, loss = 0.16938656\n",
            "Iteration 43, loss = 0.16860835\n",
            "Iteration 44, loss = 0.16768189\n",
            "Iteration 45, loss = 0.16689478\n",
            "Iteration 46, loss = 0.16620522\n",
            "Iteration 47, loss = 0.16523874\n",
            "Iteration 48, loss = 0.16468140\n",
            "Iteration 49, loss = 0.16375131\n",
            "Iteration 50, loss = 0.16314305\n",
            "Iteration 51, loss = 0.16240933\n",
            "Iteration 52, loss = 0.16174373\n",
            "Iteration 53, loss = 0.16098373\n",
            "Iteration 54, loss = 0.16030053\n",
            "Iteration 55, loss = 0.15963922\n",
            "Iteration 56, loss = 0.15889670\n",
            "Iteration 57, loss = 0.15831519\n",
            "Iteration 58, loss = 0.15761916\n",
            "Iteration 59, loss = 0.15679627\n",
            "Iteration 60, loss = 0.15625398\n",
            "Iteration 61, loss = 0.15554827\n",
            "Iteration 62, loss = 0.15485658\n",
            "Iteration 63, loss = 0.15437246\n",
            "Iteration 64, loss = 0.15372232\n",
            "Iteration 65, loss = 0.15322535\n",
            "Iteration 66, loss = 0.15276216\n",
            "Iteration 67, loss = 0.15193553\n",
            "Iteration 68, loss = 0.15126199\n",
            "Iteration 69, loss = 0.15078391\n",
            "Iteration 70, loss = 0.15023661\n",
            "Iteration 71, loss = 0.14968177\n",
            "Iteration 72, loss = 0.14916725\n",
            "Iteration 73, loss = 0.14869085\n",
            "Iteration 74, loss = 0.14791627\n",
            "Iteration 75, loss = 0.14742400\n",
            "Iteration 76, loss = 0.14703610\n",
            "Iteration 77, loss = 0.14672264\n",
            "Iteration 78, loss = 0.14606346\n",
            "Iteration 79, loss = 0.14568163\n",
            "Iteration 80, loss = 0.14503593\n",
            "Iteration 81, loss = 0.14469456\n",
            "Iteration 82, loss = 0.14436211\n",
            "Iteration 83, loss = 0.14382479\n",
            "Iteration 84, loss = 0.14341492\n",
            "Iteration 85, loss = 0.14318248\n",
            "Iteration 86, loss = 0.14267189\n",
            "Iteration 87, loss = 0.14242067\n",
            "Iteration 88, loss = 0.14206311\n",
            "Iteration 89, loss = 0.14159008\n",
            "Iteration 90, loss = 0.14121825\n",
            "Iteration 91, loss = 0.14081096\n",
            "Iteration 92, loss = 0.14034827\n",
            "Iteration 93, loss = 0.13995821\n",
            "Iteration 94, loss = 0.13957595\n",
            "Iteration 95, loss = 0.13944027\n",
            "Iteration 96, loss = 0.13877853\n",
            "Iteration 97, loss = 0.13849454\n",
            "Iteration 98, loss = 0.13831312\n",
            "Iteration 99, loss = 0.13798152\n",
            "Iteration 100, loss = 0.13747708\n",
            "Iteration 101, loss = 0.13723690\n",
            "Iteration 102, loss = 0.13685814\n",
            "Iteration 103, loss = 0.13663880\n",
            "Iteration 104, loss = 0.13632501\n",
            "Iteration 105, loss = 0.13597324\n",
            "Iteration 106, loss = 0.13575073\n",
            "Iteration 107, loss = 0.13508579\n",
            "Iteration 108, loss = 0.13498962\n",
            "Iteration 109, loss = 0.13458307\n",
            "Iteration 110, loss = 0.13411964\n",
            "Iteration 111, loss = 0.13408423\n",
            "Iteration 112, loss = 0.13364998\n",
            "Iteration 113, loss = 0.13350456\n",
            "Iteration 114, loss = 0.13307363\n",
            "Iteration 115, loss = 0.13281032\n",
            "Iteration 116, loss = 0.13244470\n",
            "Iteration 117, loss = 0.13250613\n",
            "Iteration 118, loss = 0.13185934\n",
            "Iteration 119, loss = 0.13152103\n",
            "Iteration 120, loss = 0.13151351\n",
            "Iteration 121, loss = 0.13116450\n",
            "Iteration 122, loss = 0.13124591\n",
            "Iteration 123, loss = 0.13085565\n",
            "Iteration 124, loss = 0.13079659\n",
            "Iteration 125, loss = 0.13033077\n",
            "Iteration 126, loss = 0.13037269\n",
            "Iteration 127, loss = 0.13032377\n",
            "Iteration 128, loss = 0.12976726\n",
            "Iteration 129, loss = 0.12958072\n",
            "Iteration 130, loss = 0.12917217\n",
            "Iteration 131, loss = 0.12917583\n",
            "Iteration 132, loss = 0.12887050\n",
            "Iteration 133, loss = 0.12882216\n",
            "Iteration 134, loss = 0.12847383\n",
            "Iteration 135, loss = 0.12810616\n",
            "Iteration 136, loss = 0.12786006\n",
            "Iteration 137, loss = 0.12799377\n",
            "Iteration 138, loss = 0.12775752\n",
            "Iteration 139, loss = 0.12754758\n",
            "Iteration 140, loss = 0.12721748\n",
            "Iteration 141, loss = 0.12713416\n",
            "Iteration 142, loss = 0.12668941\n",
            "Iteration 143, loss = 0.12676549\n",
            "Iteration 144, loss = 0.12664156\n",
            "Iteration 145, loss = 0.12658125\n",
            "Iteration 146, loss = 0.12602454\n",
            "Iteration 147, loss = 0.12613756\n",
            "Iteration 148, loss = 0.12607688\n",
            "Iteration 149, loss = 0.12601382\n",
            "Iteration 150, loss = 0.12592482\n",
            "Iteration 151, loss = 0.12517406\n",
            "Iteration 152, loss = 0.12528544\n",
            "Iteration 153, loss = 0.12539182\n",
            "Iteration 154, loss = 0.12496579\n",
            "Iteration 155, loss = 0.12501671\n",
            "Iteration 156, loss = 0.12446965\n",
            "Iteration 157, loss = 0.12451305\n",
            "Iteration 158, loss = 0.12463498\n",
            "Iteration 159, loss = 0.12415265\n",
            "Iteration 160, loss = 0.12401338\n",
            "Iteration 161, loss = 0.12383771\n",
            "Iteration 162, loss = 0.12386770\n",
            "Iteration 163, loss = 0.12349945\n",
            "Iteration 164, loss = 0.12312779\n",
            "Iteration 165, loss = 0.12343253\n",
            "Iteration 166, loss = 0.12314878\n",
            "Iteration 167, loss = 0.12350868\n",
            "Iteration 168, loss = 0.12247791\n",
            "Iteration 169, loss = 0.12281572\n",
            "Iteration 170, loss = 0.12252846\n",
            "Iteration 171, loss = 0.12248966\n",
            "Iteration 172, loss = 0.12224500\n",
            "Iteration 173, loss = 0.12194348\n",
            "Iteration 174, loss = 0.12162531\n",
            "Iteration 175, loss = 0.12163416\n",
            "Iteration 176, loss = 0.12179290\n",
            "Iteration 177, loss = 0.12156290\n",
            "Iteration 178, loss = 0.12115480\n",
            "Iteration 179, loss = 0.12118901\n",
            "Iteration 180, loss = 0.12130192\n",
            "Iteration 181, loss = 0.12081137\n",
            "Iteration 182, loss = 0.12084761\n",
            "Iteration 183, loss = 0.12095099\n",
            "Iteration 184, loss = 0.12035817\n",
            "Iteration 185, loss = 0.12051464\n",
            "Iteration 186, loss = 0.12003589\n",
            "Iteration 187, loss = 0.12010155\n",
            "Iteration 188, loss = 0.12020664\n",
            "Iteration 189, loss = 0.11988992\n",
            "Iteration 190, loss = 0.11961821\n",
            "Iteration 191, loss = 0.11958027\n",
            "Iteration 192, loss = 0.11956289\n",
            "Iteration 193, loss = 0.11937303\n",
            "Iteration 194, loss = 0.11923874\n",
            "Iteration 195, loss = 0.11889148\n",
            "Iteration 196, loss = 0.11885640\n",
            "Iteration 197, loss = 0.11870130\n",
            "Iteration 198, loss = 0.11845358\n",
            "Iteration 199, loss = 0.11865050\n",
            "Iteration 200, loss = 0.11873923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.49843184\n",
            "Iteration 2, loss = 0.25943685\n",
            "Iteration 3, loss = 0.22143466\n",
            "Iteration 4, loss = 0.21349737\n",
            "Iteration 5, loss = 0.21026955\n",
            "Iteration 6, loss = 0.20834933\n",
            "Iteration 7, loss = 0.20699353\n",
            "Iteration 8, loss = 0.20587828\n",
            "Iteration 9, loss = 0.20490228\n",
            "Iteration 10, loss = 0.20373742\n",
            "Iteration 11, loss = 0.20278487\n",
            "Iteration 12, loss = 0.20176723\n",
            "Iteration 13, loss = 0.20061510\n",
            "Iteration 14, loss = 0.19945537\n",
            "Iteration 15, loss = 0.19831655\n",
            "Iteration 16, loss = 0.19722213\n",
            "Iteration 17, loss = 0.19603688\n",
            "Iteration 18, loss = 0.19484452\n",
            "Iteration 19, loss = 0.19372660\n",
            "Iteration 20, loss = 0.19243571\n",
            "Iteration 21, loss = 0.19125445\n",
            "Iteration 22, loss = 0.19010431\n",
            "Iteration 23, loss = 0.18903262\n",
            "Iteration 24, loss = 0.18797140\n",
            "Iteration 25, loss = 0.18679765\n",
            "Iteration 26, loss = 0.18565380\n",
            "Iteration 27, loss = 0.18459432\n",
            "Iteration 28, loss = 0.18359264\n",
            "Iteration 29, loss = 0.18251003\n",
            "Iteration 30, loss = 0.18161372\n",
            "Iteration 31, loss = 0.18060297\n",
            "Iteration 32, loss = 0.17968271\n",
            "Iteration 33, loss = 0.17869357\n",
            "Iteration 34, loss = 0.17776229\n",
            "Iteration 35, loss = 0.17688934\n",
            "Iteration 36, loss = 0.17600836\n",
            "Iteration 37, loss = 0.17514300\n",
            "Iteration 38, loss = 0.17443476\n",
            "Iteration 39, loss = 0.17357120\n",
            "Iteration 40, loss = 0.17272323\n",
            "Iteration 41, loss = 0.17172170\n",
            "Iteration 42, loss = 0.17107973\n",
            "Iteration 43, loss = 0.17044484\n",
            "Iteration 44, loss = 0.16966791\n",
            "Iteration 45, loss = 0.16903916\n",
            "Iteration 46, loss = 0.16828503\n",
            "Iteration 47, loss = 0.16756382\n",
            "Iteration 48, loss = 0.16696979\n",
            "Iteration 49, loss = 0.16625082\n",
            "Iteration 50, loss = 0.16568313\n",
            "Iteration 51, loss = 0.16500775\n",
            "Iteration 52, loss = 0.16438912\n",
            "Iteration 53, loss = 0.16384809\n",
            "Iteration 54, loss = 0.16324887\n",
            "Iteration 55, loss = 0.16270261\n",
            "Iteration 56, loss = 0.16209292\n",
            "Iteration 57, loss = 0.16163439\n",
            "Iteration 58, loss = 0.16102423\n",
            "Iteration 59, loss = 0.16028586\n",
            "Iteration 60, loss = 0.15984974\n",
            "Iteration 61, loss = 0.15945339\n",
            "Iteration 62, loss = 0.15890009\n",
            "Iteration 63, loss = 0.15851359\n",
            "Iteration 64, loss = 0.15789937\n",
            "Iteration 65, loss = 0.15723044\n",
            "Iteration 66, loss = 0.15676236\n",
            "Iteration 67, loss = 0.15638237\n",
            "Iteration 68, loss = 0.15565925\n",
            "Iteration 69, loss = 0.15519426\n",
            "Iteration 70, loss = 0.15471594\n",
            "Iteration 71, loss = 0.15412229\n",
            "Iteration 72, loss = 0.15373189\n",
            "Iteration 73, loss = 0.15325865\n",
            "Iteration 74, loss = 0.15281759\n",
            "Iteration 75, loss = 0.15224243\n",
            "Iteration 76, loss = 0.15163566\n",
            "Iteration 77, loss = 0.15127497\n",
            "Iteration 78, loss = 0.15084972\n",
            "Iteration 79, loss = 0.15031328\n",
            "Iteration 80, loss = 0.14986034\n",
            "Iteration 81, loss = 0.14930728\n",
            "Iteration 82, loss = 0.14904917\n",
            "Iteration 83, loss = 0.14838810\n",
            "Iteration 84, loss = 0.14796044\n",
            "Iteration 85, loss = 0.14762799\n",
            "Iteration 86, loss = 0.14726508\n",
            "Iteration 87, loss = 0.14677127\n",
            "Iteration 88, loss = 0.14622806\n",
            "Iteration 89, loss = 0.14579788\n",
            "Iteration 90, loss = 0.14543055\n",
            "Iteration 91, loss = 0.14509469\n",
            "Iteration 92, loss = 0.14455710\n",
            "Iteration 93, loss = 0.14409292\n",
            "Iteration 94, loss = 0.14390696\n",
            "Iteration 95, loss = 0.14346346\n",
            "Iteration 96, loss = 0.14310552\n",
            "Iteration 97, loss = 0.14273597\n",
            "Iteration 98, loss = 0.14226305\n",
            "Iteration 99, loss = 0.14209392\n",
            "Iteration 100, loss = 0.14155968\n",
            "Iteration 101, loss = 0.14106539\n",
            "Iteration 102, loss = 0.14085562\n",
            "Iteration 103, loss = 0.14055869\n",
            "Iteration 104, loss = 0.14015404\n",
            "Iteration 105, loss = 0.13978594\n",
            "Iteration 106, loss = 0.13941572\n",
            "Iteration 107, loss = 0.13915216\n",
            "Iteration 108, loss = 0.13887664\n",
            "Iteration 109, loss = 0.13846459\n",
            "Iteration 110, loss = 0.13799194\n",
            "Iteration 111, loss = 0.13794093\n",
            "Iteration 112, loss = 0.13746398\n",
            "Iteration 113, loss = 0.13727128\n",
            "Iteration 114, loss = 0.13676130\n",
            "Iteration 115, loss = 0.13661717\n",
            "Iteration 116, loss = 0.13643113\n",
            "Iteration 117, loss = 0.13618293\n",
            "Iteration 118, loss = 0.13570144\n",
            "Iteration 119, loss = 0.13548435\n",
            "Iteration 120, loss = 0.13519596\n",
            "Iteration 121, loss = 0.13508852\n",
            "Iteration 122, loss = 0.13465052\n",
            "Iteration 123, loss = 0.13456625\n",
            "Iteration 124, loss = 0.13423574\n",
            "Iteration 125, loss = 0.13404131\n",
            "Iteration 126, loss = 0.13370771\n",
            "Iteration 127, loss = 0.13341882\n",
            "Iteration 128, loss = 0.13302497\n",
            "Iteration 129, loss = 0.13292627\n",
            "Iteration 130, loss = 0.13272381\n",
            "Iteration 131, loss = 0.13234760\n",
            "Iteration 132, loss = 0.13234631\n",
            "Iteration 133, loss = 0.13206448\n",
            "Iteration 134, loss = 0.13166813\n",
            "Iteration 135, loss = 0.13158158\n",
            "Iteration 136, loss = 0.13134432\n",
            "Iteration 137, loss = 0.13120829\n",
            "Iteration 138, loss = 0.13110358\n",
            "Iteration 139, loss = 0.13056118\n",
            "Iteration 140, loss = 0.13061410\n",
            "Iteration 141, loss = 0.13021394\n",
            "Iteration 142, loss = 0.13009174\n",
            "Iteration 143, loss = 0.12999756\n",
            "Iteration 144, loss = 0.12963412\n",
            "Iteration 145, loss = 0.12944408\n",
            "Iteration 146, loss = 0.12939995\n",
            "Iteration 147, loss = 0.12929331\n",
            "Iteration 148, loss = 0.12913594\n",
            "Iteration 149, loss = 0.12883029\n",
            "Iteration 150, loss = 0.12856933\n",
            "Iteration 151, loss = 0.12843627\n",
            "Iteration 152, loss = 0.12803691\n",
            "Iteration 153, loss = 0.12793412\n",
            "Iteration 154, loss = 0.12783986\n",
            "Iteration 155, loss = 0.12763921\n",
            "Iteration 156, loss = 0.12747043\n",
            "Iteration 157, loss = 0.12722717\n",
            "Iteration 158, loss = 0.12733569\n",
            "Iteration 159, loss = 0.12708193\n",
            "Iteration 160, loss = 0.12666465\n",
            "Iteration 161, loss = 0.12662876\n",
            "Iteration 162, loss = 0.12669299\n",
            "Iteration 163, loss = 0.12628482\n",
            "Iteration 164, loss = 0.12616419\n",
            "Iteration 165, loss = 0.12602438\n",
            "Iteration 166, loss = 0.12598555\n",
            "Iteration 167, loss = 0.12541862\n",
            "Iteration 168, loss = 0.12569148\n",
            "Iteration 169, loss = 0.12551918\n",
            "Iteration 170, loss = 0.12520612\n",
            "Iteration 171, loss = 0.12531765\n",
            "Iteration 172, loss = 0.12495972\n",
            "Iteration 173, loss = 0.12482152\n",
            "Iteration 174, loss = 0.12473007\n",
            "Iteration 175, loss = 0.12473678\n",
            "Iteration 176, loss = 0.12478322\n",
            "Iteration 177, loss = 0.12413497\n",
            "Iteration 178, loss = 0.12456820\n",
            "Iteration 179, loss = 0.12407900\n",
            "Iteration 180, loss = 0.12401709\n",
            "Iteration 181, loss = 0.12381128\n",
            "Iteration 182, loss = 0.12366773\n",
            "Iteration 183, loss = 0.12380347\n",
            "Iteration 184, loss = 0.12375801\n",
            "Iteration 185, loss = 0.12330984\n",
            "Iteration 186, loss = 0.12306447\n",
            "Iteration 187, loss = 0.12302980\n",
            "Iteration 188, loss = 0.12305454\n",
            "Iteration 189, loss = 0.12292169\n",
            "Iteration 190, loss = 0.12301917\n",
            "Iteration 191, loss = 0.12264327\n",
            "Iteration 192, loss = 0.12261237\n",
            "Iteration 193, loss = 0.12224133\n",
            "Iteration 194, loss = 0.12249036\n",
            "Iteration 195, loss = 0.12259527\n",
            "Iteration 196, loss = 0.12201377\n",
            "Iteration 197, loss = 0.12214897\n",
            "Iteration 198, loss = 0.12213973\n",
            "Iteration 199, loss = 0.12205723\n",
            "Iteration 200, loss = 0.12155932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45845129\n",
            "Iteration 2, loss = 0.25610512\n",
            "Iteration 3, loss = 0.22558264\n",
            "Iteration 4, loss = 0.21672405\n",
            "Iteration 5, loss = 0.21335062\n",
            "Iteration 6, loss = 0.21158228\n",
            "Iteration 7, loss = 0.21049025\n",
            "Iteration 8, loss = 0.20967704\n",
            "Iteration 9, loss = 0.20902402\n",
            "Iteration 10, loss = 0.20822747\n",
            "Iteration 11, loss = 0.20742864\n",
            "Iteration 12, loss = 0.20665033\n",
            "Iteration 13, loss = 0.20579191\n",
            "Iteration 14, loss = 0.20492199\n",
            "Iteration 15, loss = 0.20384610\n",
            "Iteration 16, loss = 0.20287939\n",
            "Iteration 17, loss = 0.20175159\n",
            "Iteration 18, loss = 0.20061554\n",
            "Iteration 19, loss = 0.19961234\n",
            "Iteration 20, loss = 0.19859389\n",
            "Iteration 21, loss = 0.19758248\n",
            "Iteration 22, loss = 0.19660165\n",
            "Iteration 23, loss = 0.19567504\n",
            "Iteration 24, loss = 0.19477275\n",
            "Iteration 25, loss = 0.19384464\n",
            "Iteration 26, loss = 0.19295569\n",
            "Iteration 27, loss = 0.19202541\n",
            "Iteration 28, loss = 0.19117314\n",
            "Iteration 29, loss = 0.19045083\n",
            "Iteration 30, loss = 0.18953111\n",
            "Iteration 31, loss = 0.18874660\n",
            "Iteration 32, loss = 0.18797418\n",
            "Iteration 33, loss = 0.18714642\n",
            "Iteration 34, loss = 0.18643544\n",
            "Iteration 35, loss = 0.18560284\n",
            "Iteration 36, loss = 0.18480045\n",
            "Iteration 37, loss = 0.18417940\n",
            "Iteration 38, loss = 0.18338112\n",
            "Iteration 39, loss = 0.18258966\n",
            "Iteration 40, loss = 0.18189646\n",
            "Iteration 41, loss = 0.18109250\n",
            "Iteration 42, loss = 0.18045114\n",
            "Iteration 43, loss = 0.17962675\n",
            "Iteration 44, loss = 0.17897531\n",
            "Iteration 45, loss = 0.17817410\n",
            "Iteration 46, loss = 0.17746464\n",
            "Iteration 47, loss = 0.17675257\n",
            "Iteration 48, loss = 0.17612072\n",
            "Iteration 49, loss = 0.17542368\n",
            "Iteration 50, loss = 0.17455721\n",
            "Iteration 51, loss = 0.17377720\n",
            "Iteration 52, loss = 0.17315253\n",
            "Iteration 53, loss = 0.17231527\n",
            "Iteration 54, loss = 0.17164701\n",
            "Iteration 55, loss = 0.17084287\n",
            "Iteration 56, loss = 0.17008279\n",
            "Iteration 57, loss = 0.16926762\n",
            "Iteration 58, loss = 0.16855830\n",
            "Iteration 59, loss = 0.16763071\n",
            "Iteration 60, loss = 0.16679667\n",
            "Iteration 61, loss = 0.16587063\n",
            "Iteration 62, loss = 0.16505278\n",
            "Iteration 63, loss = 0.16412662\n",
            "Iteration 64, loss = 0.16337929\n",
            "Iteration 65, loss = 0.16244177\n",
            "Iteration 66, loss = 0.16142524\n",
            "Iteration 67, loss = 0.16058812\n",
            "Iteration 68, loss = 0.15951471\n",
            "Iteration 69, loss = 0.15851897\n",
            "Iteration 70, loss = 0.15764369\n",
            "Iteration 71, loss = 0.15650413\n",
            "Iteration 72, loss = 0.15554786\n",
            "Iteration 73, loss = 0.15450475\n",
            "Iteration 74, loss = 0.15340579\n",
            "Iteration 75, loss = 0.15229694\n",
            "Iteration 76, loss = 0.15123876\n",
            "Iteration 77, loss = 0.15012955\n",
            "Iteration 78, loss = 0.14905302\n",
            "Iteration 79, loss = 0.14778706\n",
            "Iteration 80, loss = 0.14673731\n",
            "Iteration 81, loss = 0.14558234\n",
            "Iteration 82, loss = 0.14449995\n",
            "Iteration 83, loss = 0.14331387\n",
            "Iteration 84, loss = 0.14214453\n",
            "Iteration 85, loss = 0.14104433\n",
            "Iteration 86, loss = 0.13995575\n",
            "Iteration 87, loss = 0.13872991\n",
            "Iteration 88, loss = 0.13758316\n",
            "Iteration 89, loss = 0.13647642\n",
            "Iteration 90, loss = 0.13530086\n",
            "Iteration 91, loss = 0.13420352\n",
            "Iteration 92, loss = 0.13306629\n",
            "Iteration 93, loss = 0.13190504\n",
            "Iteration 94, loss = 0.13070768\n",
            "Iteration 95, loss = 0.12966939\n",
            "Iteration 96, loss = 0.12851921\n",
            "Iteration 97, loss = 0.12734104\n",
            "Iteration 98, loss = 0.12624184\n",
            "Iteration 99, loss = 0.12524120\n",
            "Iteration 100, loss = 0.12410521\n",
            "Iteration 101, loss = 0.12293888\n",
            "Iteration 102, loss = 0.12180038\n",
            "Iteration 103, loss = 0.12069860\n",
            "Iteration 104, loss = 0.11966052\n",
            "Iteration 105, loss = 0.11855406\n",
            "Iteration 106, loss = 0.11753866\n",
            "Iteration 107, loss = 0.11651600\n",
            "Iteration 108, loss = 0.11547312\n",
            "Iteration 109, loss = 0.11441594\n",
            "Iteration 110, loss = 0.11330103\n",
            "Iteration 111, loss = 0.11231183\n",
            "Iteration 112, loss = 0.11123155\n",
            "Iteration 113, loss = 0.11031694\n",
            "Iteration 114, loss = 0.10945594\n",
            "Iteration 115, loss = 0.10838689\n",
            "Iteration 116, loss = 0.10743929\n",
            "Iteration 117, loss = 0.10648856\n",
            "Iteration 118, loss = 0.10550124\n",
            "Iteration 119, loss = 0.10461705\n",
            "Iteration 120, loss = 0.10359793\n",
            "Iteration 121, loss = 0.10270985\n",
            "Iteration 122, loss = 0.10182416\n",
            "Iteration 123, loss = 0.10099986\n",
            "Iteration 124, loss = 0.09996145\n",
            "Iteration 125, loss = 0.09908275\n",
            "Iteration 126, loss = 0.09825010\n",
            "Iteration 127, loss = 0.09749887\n",
            "Iteration 128, loss = 0.09656040\n",
            "Iteration 129, loss = 0.09572762\n",
            "Iteration 130, loss = 0.09487394\n",
            "Iteration 131, loss = 0.09402410\n",
            "Iteration 132, loss = 0.09327987\n",
            "Iteration 133, loss = 0.09246364\n",
            "Iteration 134, loss = 0.09166860\n",
            "Iteration 135, loss = 0.09094406\n",
            "Iteration 136, loss = 0.09011111\n",
            "Iteration 137, loss = 0.08936466\n",
            "Iteration 138, loss = 0.08856634\n",
            "Iteration 139, loss = 0.08797990\n",
            "Iteration 140, loss = 0.08708717\n",
            "Iteration 141, loss = 0.08648156\n",
            "Iteration 142, loss = 0.08574072\n",
            "Iteration 143, loss = 0.08505057\n",
            "Iteration 144, loss = 0.08438306\n",
            "Iteration 145, loss = 0.08368396\n",
            "Iteration 146, loss = 0.08313559\n",
            "Iteration 147, loss = 0.08233345\n",
            "Iteration 148, loss = 0.08159457\n",
            "Iteration 149, loss = 0.08103313\n",
            "Iteration 150, loss = 0.08023513\n",
            "Iteration 151, loss = 0.07966741\n",
            "Iteration 152, loss = 0.07894403\n",
            "Iteration 153, loss = 0.07842662\n",
            "Iteration 154, loss = 0.07787790\n",
            "Iteration 155, loss = 0.07728381\n",
            "Iteration 156, loss = 0.07666494\n",
            "Iteration 157, loss = 0.07592481\n",
            "Iteration 158, loss = 0.07561398\n",
            "Iteration 159, loss = 0.07489225\n",
            "Iteration 160, loss = 0.07414626\n",
            "Iteration 161, loss = 0.07365675\n",
            "Iteration 162, loss = 0.07305812\n",
            "Iteration 163, loss = 0.07269736\n",
            "Iteration 164, loss = 0.07198193\n",
            "Iteration 165, loss = 0.07165125\n",
            "Iteration 166, loss = 0.07083183\n",
            "Iteration 167, loss = 0.07044003\n",
            "Iteration 168, loss = 0.06992265\n",
            "Iteration 169, loss = 0.06930606\n",
            "Iteration 170, loss = 0.06881084\n",
            "Iteration 171, loss = 0.06836760\n",
            "Iteration 172, loss = 0.06778606\n",
            "Iteration 173, loss = 0.06732709\n",
            "Iteration 174, loss = 0.06675515\n",
            "Iteration 175, loss = 0.06630268\n",
            "Iteration 176, loss = 0.06569883\n",
            "Iteration 177, loss = 0.06527094\n",
            "Iteration 178, loss = 0.06494618\n",
            "Iteration 179, loss = 0.06439168\n",
            "Iteration 180, loss = 0.06386285\n",
            "Iteration 181, loss = 0.06344770\n",
            "Iteration 182, loss = 0.06300751\n",
            "Iteration 183, loss = 0.06254175\n",
            "Iteration 184, loss = 0.06198895\n",
            "Iteration 185, loss = 0.06158876\n",
            "Iteration 186, loss = 0.06111588\n",
            "Iteration 187, loss = 0.06080521\n",
            "Iteration 188, loss = 0.06036204\n",
            "Iteration 189, loss = 0.05989190\n",
            "Iteration 190, loss = 0.05951489\n",
            "Iteration 191, loss = 0.05891193\n",
            "Iteration 192, loss = 0.05859042\n",
            "Iteration 193, loss = 0.05816484\n",
            "Iteration 194, loss = 0.05786576\n",
            "Iteration 195, loss = 0.05731064\n",
            "Iteration 196, loss = 0.05699987\n",
            "Iteration 197, loss = 0.05654955\n",
            "Iteration 198, loss = 0.05624803\n",
            "Iteration 199, loss = 0.05574158\n",
            "Iteration 200, loss = 0.05544213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45773820\n",
            "Iteration 2, loss = 0.25438528\n",
            "Iteration 3, loss = 0.22402895\n",
            "Iteration 4, loss = 0.21526550\n",
            "Iteration 5, loss = 0.21183555\n",
            "Iteration 6, loss = 0.21016339\n",
            "Iteration 7, loss = 0.20911121\n",
            "Iteration 8, loss = 0.20846096\n",
            "Iteration 9, loss = 0.20766158\n",
            "Iteration 10, loss = 0.20697357\n",
            "Iteration 11, loss = 0.20613664\n",
            "Iteration 12, loss = 0.20541816\n",
            "Iteration 13, loss = 0.20459054\n",
            "Iteration 14, loss = 0.20369145\n",
            "Iteration 15, loss = 0.20262369\n",
            "Iteration 16, loss = 0.20163682\n",
            "Iteration 17, loss = 0.20053445\n",
            "Iteration 18, loss = 0.19941134\n",
            "Iteration 19, loss = 0.19837260\n",
            "Iteration 20, loss = 0.19727225\n",
            "Iteration 21, loss = 0.19618637\n",
            "Iteration 22, loss = 0.19520251\n",
            "Iteration 23, loss = 0.19417717\n",
            "Iteration 24, loss = 0.19326056\n",
            "Iteration 25, loss = 0.19225428\n",
            "Iteration 26, loss = 0.19126221\n",
            "Iteration 27, loss = 0.19037548\n",
            "Iteration 28, loss = 0.18949485\n",
            "Iteration 29, loss = 0.18864003\n",
            "Iteration 30, loss = 0.18774719\n",
            "Iteration 31, loss = 0.18693315\n",
            "Iteration 32, loss = 0.18613946\n",
            "Iteration 33, loss = 0.18528407\n",
            "Iteration 34, loss = 0.18452881\n",
            "Iteration 35, loss = 0.18369013\n",
            "Iteration 36, loss = 0.18285147\n",
            "Iteration 37, loss = 0.18221556\n",
            "Iteration 38, loss = 0.18148128\n",
            "Iteration 39, loss = 0.18066864\n",
            "Iteration 40, loss = 0.17999361\n",
            "Iteration 41, loss = 0.17927983\n",
            "Iteration 42, loss = 0.17857627\n",
            "Iteration 43, loss = 0.17786919\n",
            "Iteration 44, loss = 0.17708891\n",
            "Iteration 45, loss = 0.17639626\n",
            "Iteration 46, loss = 0.17574282\n",
            "Iteration 47, loss = 0.17505395\n",
            "Iteration 48, loss = 0.17437225\n",
            "Iteration 49, loss = 0.17359414\n",
            "Iteration 50, loss = 0.17288192\n",
            "Iteration 51, loss = 0.17217125\n",
            "Iteration 52, loss = 0.17149631\n",
            "Iteration 53, loss = 0.17080866\n",
            "Iteration 54, loss = 0.17004895\n",
            "Iteration 55, loss = 0.16932367\n",
            "Iteration 56, loss = 0.16862211\n",
            "Iteration 57, loss = 0.16781198\n",
            "Iteration 58, loss = 0.16713175\n",
            "Iteration 59, loss = 0.16634043\n",
            "Iteration 60, loss = 0.16562644\n",
            "Iteration 61, loss = 0.16492470\n",
            "Iteration 62, loss = 0.16415850\n",
            "Iteration 63, loss = 0.16335108\n",
            "Iteration 64, loss = 0.16254913\n",
            "Iteration 65, loss = 0.16174609\n",
            "Iteration 66, loss = 0.16095973\n",
            "Iteration 67, loss = 0.16025583\n",
            "Iteration 68, loss = 0.15944083\n",
            "Iteration 69, loss = 0.15856736\n",
            "Iteration 70, loss = 0.15777954\n",
            "Iteration 71, loss = 0.15692714\n",
            "Iteration 72, loss = 0.15609880\n",
            "Iteration 73, loss = 0.15518803\n",
            "Iteration 74, loss = 0.15440385\n",
            "Iteration 75, loss = 0.15356280\n",
            "Iteration 76, loss = 0.15267845\n",
            "Iteration 77, loss = 0.15180305\n",
            "Iteration 78, loss = 0.15098959\n",
            "Iteration 79, loss = 0.15011770\n",
            "Iteration 80, loss = 0.14913857\n",
            "Iteration 81, loss = 0.14829972\n",
            "Iteration 82, loss = 0.14730779\n",
            "Iteration 83, loss = 0.14654151\n",
            "Iteration 84, loss = 0.14537175\n",
            "Iteration 85, loss = 0.14458082\n",
            "Iteration 86, loss = 0.14370724\n",
            "Iteration 87, loss = 0.14259837\n",
            "Iteration 88, loss = 0.14167841\n",
            "Iteration 89, loss = 0.14077192\n",
            "Iteration 90, loss = 0.13983869\n",
            "Iteration 91, loss = 0.13883060\n",
            "Iteration 92, loss = 0.13783883\n",
            "Iteration 93, loss = 0.13685425\n",
            "Iteration 94, loss = 0.13582679\n",
            "Iteration 95, loss = 0.13494504\n",
            "Iteration 96, loss = 0.13385677\n",
            "Iteration 97, loss = 0.13295718\n",
            "Iteration 98, loss = 0.13198761\n",
            "Iteration 99, loss = 0.13108946\n",
            "Iteration 100, loss = 0.13001707\n",
            "Iteration 101, loss = 0.12904906\n",
            "Iteration 102, loss = 0.12813888\n",
            "Iteration 103, loss = 0.12704150\n",
            "Iteration 104, loss = 0.12607953\n",
            "Iteration 105, loss = 0.12516661\n",
            "Iteration 106, loss = 0.12421570\n",
            "Iteration 107, loss = 0.12329897\n",
            "Iteration 108, loss = 0.12226718\n",
            "Iteration 109, loss = 0.12128339\n",
            "Iteration 110, loss = 0.12035917\n",
            "Iteration 111, loss = 0.11952628\n",
            "Iteration 112, loss = 0.11852068\n",
            "Iteration 113, loss = 0.11763907\n",
            "Iteration 114, loss = 0.11658866\n",
            "Iteration 115, loss = 0.11567490\n",
            "Iteration 116, loss = 0.11486413\n",
            "Iteration 117, loss = 0.11399118\n",
            "Iteration 118, loss = 0.11296406\n",
            "Iteration 119, loss = 0.11211878\n",
            "Iteration 120, loss = 0.11122030\n",
            "Iteration 121, loss = 0.11027113\n",
            "Iteration 122, loss = 0.10945382\n",
            "Iteration 123, loss = 0.10855236\n",
            "Iteration 124, loss = 0.10762638\n",
            "Iteration 125, loss = 0.10683538\n",
            "Iteration 126, loss = 0.10595540\n",
            "Iteration 127, loss = 0.10510369\n",
            "Iteration 128, loss = 0.10434735\n",
            "Iteration 129, loss = 0.10355304\n",
            "Iteration 130, loss = 0.10255443\n",
            "Iteration 131, loss = 0.10185052\n",
            "Iteration 132, loss = 0.10099334\n",
            "Iteration 133, loss = 0.10020157\n",
            "Iteration 134, loss = 0.09951404\n",
            "Iteration 135, loss = 0.09868816\n",
            "Iteration 136, loss = 0.09793975\n",
            "Iteration 137, loss = 0.09719568\n",
            "Iteration 138, loss = 0.09639861\n",
            "Iteration 139, loss = 0.09577558\n",
            "Iteration 140, loss = 0.09490855\n",
            "Iteration 141, loss = 0.09421257\n",
            "Iteration 142, loss = 0.09341412\n",
            "Iteration 143, loss = 0.09287825\n",
            "Iteration 144, loss = 0.09210453\n",
            "Iteration 145, loss = 0.09131788\n",
            "Iteration 146, loss = 0.09063303\n",
            "Iteration 147, loss = 0.08985951\n",
            "Iteration 148, loss = 0.08930559\n",
            "Iteration 149, loss = 0.08862943\n",
            "Iteration 150, loss = 0.08800000\n",
            "Iteration 151, loss = 0.08723537\n",
            "Iteration 152, loss = 0.08661921\n",
            "Iteration 153, loss = 0.08600387\n",
            "Iteration 154, loss = 0.08528374\n",
            "Iteration 155, loss = 0.08464154\n",
            "Iteration 156, loss = 0.08402982\n",
            "Iteration 157, loss = 0.08350565\n",
            "Iteration 158, loss = 0.08285241\n",
            "Iteration 159, loss = 0.08217184\n",
            "Iteration 160, loss = 0.08164522\n",
            "Iteration 161, loss = 0.08102160\n",
            "Iteration 162, loss = 0.08048345\n",
            "Iteration 163, loss = 0.07990772\n",
            "Iteration 164, loss = 0.07921502\n",
            "Iteration 165, loss = 0.07867434\n",
            "Iteration 166, loss = 0.07816686\n",
            "Iteration 167, loss = 0.07751642\n",
            "Iteration 168, loss = 0.07701153\n",
            "Iteration 169, loss = 0.07647034\n",
            "Iteration 170, loss = 0.07602741\n",
            "Iteration 171, loss = 0.07531848\n",
            "Iteration 172, loss = 0.07486062\n",
            "Iteration 173, loss = 0.07430282\n",
            "Iteration 174, loss = 0.07381483\n",
            "Iteration 175, loss = 0.07337387\n",
            "Iteration 176, loss = 0.07278034\n",
            "Iteration 177, loss = 0.07223923\n",
            "Iteration 178, loss = 0.07174431\n",
            "Iteration 179, loss = 0.07119713\n",
            "Iteration 180, loss = 0.07084903\n",
            "Iteration 181, loss = 0.07029642\n",
            "Iteration 182, loss = 0.06995579\n",
            "Iteration 183, loss = 0.06934732\n",
            "Iteration 184, loss = 0.06875678\n",
            "Iteration 185, loss = 0.06845081\n",
            "Iteration 186, loss = 0.06787152\n",
            "Iteration 187, loss = 0.06737206\n",
            "Iteration 188, loss = 0.06695581\n",
            "Iteration 189, loss = 0.06648227\n",
            "Iteration 190, loss = 0.06599393\n",
            "Iteration 191, loss = 0.06562589\n",
            "Iteration 192, loss = 0.06509196\n",
            "Iteration 193, loss = 0.06475447\n",
            "Iteration 194, loss = 0.06425014\n",
            "Iteration 195, loss = 0.06382776\n",
            "Iteration 196, loss = 0.06333626\n",
            "Iteration 197, loss = 0.06294507\n",
            "Iteration 198, loss = 0.06269957\n",
            "Iteration 199, loss = 0.06210902\n",
            "Iteration 200, loss = 0.06175312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.45782471\n",
            "Iteration 2, loss = 0.25379763\n",
            "Iteration 3, loss = 0.22322835\n",
            "Iteration 4, loss = 0.21450692\n",
            "Iteration 5, loss = 0.21107483\n",
            "Iteration 6, loss = 0.20944806\n",
            "Iteration 7, loss = 0.20837847\n",
            "Iteration 8, loss = 0.20775870\n",
            "Iteration 9, loss = 0.20718672\n",
            "Iteration 10, loss = 0.20653383\n",
            "Iteration 11, loss = 0.20583461\n",
            "Iteration 12, loss = 0.20514379\n",
            "Iteration 13, loss = 0.20440182\n",
            "Iteration 14, loss = 0.20357084\n",
            "Iteration 15, loss = 0.20290347\n",
            "Iteration 16, loss = 0.20192699\n",
            "Iteration 17, loss = 0.20093984\n",
            "Iteration 18, loss = 0.19996397\n",
            "Iteration 19, loss = 0.19887437\n",
            "Iteration 20, loss = 0.19785554\n",
            "Iteration 21, loss = 0.19674470\n",
            "Iteration 22, loss = 0.19576071\n",
            "Iteration 23, loss = 0.19473052\n",
            "Iteration 24, loss = 0.19373813\n",
            "Iteration 25, loss = 0.19263971\n",
            "Iteration 26, loss = 0.19173533\n",
            "Iteration 27, loss = 0.19074296\n",
            "Iteration 28, loss = 0.18982139\n",
            "Iteration 29, loss = 0.18886724\n",
            "Iteration 30, loss = 0.18791544\n",
            "Iteration 31, loss = 0.18710208\n",
            "Iteration 32, loss = 0.18618184\n",
            "Iteration 33, loss = 0.18530288\n",
            "Iteration 34, loss = 0.18457942\n",
            "Iteration 35, loss = 0.18371963\n",
            "Iteration 36, loss = 0.18291769\n",
            "Iteration 37, loss = 0.18222186\n",
            "Iteration 38, loss = 0.18144960\n",
            "Iteration 39, loss = 0.18061181\n",
            "Iteration 40, loss = 0.17994022\n",
            "Iteration 41, loss = 0.17913586\n",
            "Iteration 42, loss = 0.17844782\n",
            "Iteration 43, loss = 0.17777441\n",
            "Iteration 44, loss = 0.17706600\n",
            "Iteration 45, loss = 0.17632953\n",
            "Iteration 46, loss = 0.17569783\n",
            "Iteration 47, loss = 0.17505512\n",
            "Iteration 48, loss = 0.17436705\n",
            "Iteration 49, loss = 0.17377643\n",
            "Iteration 50, loss = 0.17302334\n",
            "Iteration 51, loss = 0.17233541\n",
            "Iteration 52, loss = 0.17178815\n",
            "Iteration 53, loss = 0.17107929\n",
            "Iteration 54, loss = 0.17029473\n",
            "Iteration 55, loss = 0.16971345\n",
            "Iteration 56, loss = 0.16895829\n",
            "Iteration 57, loss = 0.16833053\n",
            "Iteration 58, loss = 0.16757204\n",
            "Iteration 59, loss = 0.16693401\n",
            "Iteration 60, loss = 0.16615823\n",
            "Iteration 61, loss = 0.16560594\n",
            "Iteration 62, loss = 0.16482984\n",
            "Iteration 63, loss = 0.16412024\n",
            "Iteration 64, loss = 0.16351807\n",
            "Iteration 65, loss = 0.16280325\n",
            "Iteration 66, loss = 0.16211938\n",
            "Iteration 67, loss = 0.16139415\n",
            "Iteration 68, loss = 0.16067623\n",
            "Iteration 69, loss = 0.16001581\n",
            "Iteration 70, loss = 0.15918854\n",
            "Iteration 71, loss = 0.15854918\n",
            "Iteration 72, loss = 0.15788631\n",
            "Iteration 73, loss = 0.15709612\n",
            "Iteration 74, loss = 0.15636173\n",
            "Iteration 75, loss = 0.15568082\n",
            "Iteration 76, loss = 0.15483763\n",
            "Iteration 77, loss = 0.15406820\n",
            "Iteration 78, loss = 0.15331762\n",
            "Iteration 79, loss = 0.15261473\n",
            "Iteration 80, loss = 0.15182783\n",
            "Iteration 81, loss = 0.15105567\n",
            "Iteration 82, loss = 0.15027455\n",
            "Iteration 83, loss = 0.14946324\n",
            "Iteration 84, loss = 0.14864055\n",
            "Iteration 85, loss = 0.14787080\n",
            "Iteration 86, loss = 0.14703661\n",
            "Iteration 87, loss = 0.14615572\n",
            "Iteration 88, loss = 0.14537966\n",
            "Iteration 89, loss = 0.14452219\n",
            "Iteration 90, loss = 0.14369118\n",
            "Iteration 91, loss = 0.14285431\n",
            "Iteration 92, loss = 0.14197238\n",
            "Iteration 93, loss = 0.14123331\n",
            "Iteration 94, loss = 0.14038158\n",
            "Iteration 95, loss = 0.13944665\n",
            "Iteration 96, loss = 0.13859416\n",
            "Iteration 97, loss = 0.13766394\n",
            "Iteration 98, loss = 0.13678811\n",
            "Iteration 99, loss = 0.13599246\n",
            "Iteration 100, loss = 0.13507937\n",
            "Iteration 101, loss = 0.13419314\n",
            "Iteration 102, loss = 0.13332945\n",
            "Iteration 103, loss = 0.13235597\n",
            "Iteration 104, loss = 0.13146499\n",
            "Iteration 105, loss = 0.13075654\n",
            "Iteration 106, loss = 0.12970487\n",
            "Iteration 107, loss = 0.12879911\n",
            "Iteration 108, loss = 0.12791712\n",
            "Iteration 109, loss = 0.12707315\n",
            "Iteration 110, loss = 0.12608866\n",
            "Iteration 111, loss = 0.12517459\n",
            "Iteration 112, loss = 0.12427591\n",
            "Iteration 113, loss = 0.12344600\n",
            "Iteration 114, loss = 0.12248722\n",
            "Iteration 115, loss = 0.12163721\n",
            "Iteration 116, loss = 0.12070634\n",
            "Iteration 117, loss = 0.11980537\n",
            "Iteration 118, loss = 0.11892520\n",
            "Iteration 119, loss = 0.11789241\n",
            "Iteration 120, loss = 0.11713496\n",
            "Iteration 121, loss = 0.11609086\n",
            "Iteration 122, loss = 0.11543747\n",
            "Iteration 123, loss = 0.11447277\n",
            "Iteration 124, loss = 0.11358846\n",
            "Iteration 125, loss = 0.11284265\n",
            "Iteration 126, loss = 0.11191492\n",
            "Iteration 127, loss = 0.11099111\n",
            "Iteration 128, loss = 0.11023355\n",
            "Iteration 129, loss = 0.10939154\n",
            "Iteration 130, loss = 0.10855761\n",
            "Iteration 131, loss = 0.10758534\n",
            "Iteration 132, loss = 0.10679140\n",
            "Iteration 133, loss = 0.10601227\n",
            "Iteration 134, loss = 0.10530073\n",
            "Iteration 135, loss = 0.10429751\n",
            "Iteration 136, loss = 0.10359514\n",
            "Iteration 137, loss = 0.10279053\n",
            "Iteration 138, loss = 0.10194098\n",
            "Iteration 139, loss = 0.10104512\n",
            "Iteration 140, loss = 0.10036968\n",
            "Iteration 141, loss = 0.09958754\n",
            "Iteration 142, loss = 0.09880080\n",
            "Iteration 143, loss = 0.09807654\n",
            "Iteration 144, loss = 0.09730717\n",
            "Iteration 145, loss = 0.09653976\n",
            "Iteration 146, loss = 0.09599569\n",
            "Iteration 147, loss = 0.09508142\n",
            "Iteration 148, loss = 0.09427271\n",
            "Iteration 149, loss = 0.09363621\n",
            "Iteration 150, loss = 0.09279705\n",
            "Iteration 151, loss = 0.09217725\n",
            "Iteration 152, loss = 0.09148849\n",
            "Iteration 153, loss = 0.09080732\n",
            "Iteration 154, loss = 0.08999652\n",
            "Iteration 155, loss = 0.08956946\n",
            "Iteration 156, loss = 0.08876362\n",
            "Iteration 157, loss = 0.08810689\n",
            "Iteration 158, loss = 0.08746047\n",
            "Iteration 159, loss = 0.08676163\n",
            "Iteration 160, loss = 0.08614391\n",
            "Iteration 161, loss = 0.08552033\n",
            "Iteration 162, loss = 0.08494150\n",
            "Iteration 163, loss = 0.08425110\n",
            "Iteration 164, loss = 0.08370281\n",
            "Iteration 165, loss = 0.08324149\n",
            "Iteration 166, loss = 0.08247246\n",
            "Iteration 167, loss = 0.08198373\n",
            "Iteration 168, loss = 0.08128959\n",
            "Iteration 169, loss = 0.08075008\n",
            "Iteration 170, loss = 0.08012503\n",
            "Iteration 171, loss = 0.07970579\n",
            "Iteration 172, loss = 0.07904134\n",
            "Iteration 173, loss = 0.07861967\n",
            "Iteration 174, loss = 0.07818180\n",
            "Iteration 175, loss = 0.07746433\n",
            "Iteration 176, loss = 0.07683963\n",
            "Iteration 177, loss = 0.07641966\n",
            "Iteration 178, loss = 0.07592839\n",
            "Iteration 179, loss = 0.07542009\n",
            "Iteration 180, loss = 0.07498623\n",
            "Iteration 181, loss = 0.07440387\n",
            "Iteration 182, loss = 0.07394192\n",
            "Iteration 183, loss = 0.07336395\n",
            "Iteration 184, loss = 0.07302119\n",
            "Iteration 185, loss = 0.07246442\n",
            "Iteration 186, loss = 0.07188389\n",
            "Iteration 187, loss = 0.07142451\n",
            "Iteration 188, loss = 0.07106216\n",
            "Iteration 189, loss = 0.07066170\n",
            "Iteration 190, loss = 0.07006593\n",
            "Iteration 191, loss = 0.06982935\n",
            "Iteration 192, loss = 0.06924392\n",
            "Iteration 193, loss = 0.06891329\n",
            "Iteration 194, loss = 0.06840174\n",
            "Iteration 195, loss = 0.06793572\n",
            "Iteration 196, loss = 0.06750314\n",
            "Iteration 197, loss = 0.06702124\n",
            "Iteration 198, loss = 0.06655722\n",
            "Iteration 199, loss = 0.06617762\n",
            "Iteration 200, loss = 0.06583573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.32802504\n",
            "Iteration 2, loss = 0.23396286\n",
            "Iteration 3, loss = 0.22995874\n",
            "Iteration 4, loss = 0.22728179\n",
            "Iteration 5, loss = 0.22509482\n",
            "Iteration 6, loss = 0.22270864\n",
            "Iteration 7, loss = 0.22042932\n",
            "Iteration 8, loss = 0.21827203\n",
            "Iteration 9, loss = 0.21564186\n",
            "Iteration 10, loss = 0.21329691\n",
            "Iteration 11, loss = 0.21055448\n",
            "Iteration 12, loss = 0.20720788\n",
            "Iteration 13, loss = 0.20395337\n",
            "Iteration 14, loss = 0.19991855\n",
            "Iteration 15, loss = 0.19589399\n",
            "Iteration 16, loss = 0.19137899\n",
            "Iteration 17, loss = 0.18627913\n",
            "Iteration 18, loss = 0.18157410\n",
            "Iteration 19, loss = 0.17577898\n",
            "Iteration 20, loss = 0.17046596\n",
            "Iteration 21, loss = 0.16492652\n",
            "Iteration 22, loss = 0.15929220\n",
            "Iteration 23, loss = 0.15388240\n",
            "Iteration 24, loss = 0.14881995\n",
            "Iteration 25, loss = 0.14340684\n",
            "Iteration 26, loss = 0.13828334\n",
            "Iteration 27, loss = 0.13372775\n",
            "Iteration 28, loss = 0.12954068\n",
            "Iteration 29, loss = 0.12576622\n",
            "Iteration 30, loss = 0.12192536\n",
            "Iteration 31, loss = 0.11895411\n",
            "Iteration 32, loss = 0.11588443\n",
            "Iteration 33, loss = 0.11325149\n",
            "Iteration 34, loss = 0.11116324\n",
            "Iteration 35, loss = 0.10925402\n",
            "Iteration 36, loss = 0.10748823\n",
            "Iteration 37, loss = 0.10588901\n",
            "Iteration 38, loss = 0.10479319\n",
            "Iteration 39, loss = 0.10332912\n",
            "Iteration 40, loss = 0.10272885\n",
            "Iteration 41, loss = 0.10156994\n",
            "Iteration 42, loss = 0.10069585\n",
            "Iteration 43, loss = 0.09993281\n",
            "Iteration 44, loss = 0.09944173\n",
            "Iteration 45, loss = 0.09912176\n",
            "Iteration 46, loss = 0.09820576\n",
            "Iteration 47, loss = 0.09782846\n",
            "Iteration 48, loss = 0.09749697\n",
            "Iteration 49, loss = 0.09696880\n",
            "Iteration 50, loss = 0.09701741\n",
            "Iteration 51, loss = 0.09644278\n",
            "Iteration 52, loss = 0.09611902\n",
            "Iteration 53, loss = 0.09575896\n",
            "Iteration 54, loss = 0.09564621\n",
            "Iteration 55, loss = 0.09537022\n",
            "Iteration 56, loss = 0.09534697\n",
            "Iteration 57, loss = 0.09510685\n",
            "Iteration 58, loss = 0.09458600\n",
            "Iteration 59, loss = 0.09462770\n",
            "Iteration 60, loss = 0.09459541\n",
            "Iteration 61, loss = 0.09415648\n",
            "Iteration 62, loss = 0.09414632\n",
            "Iteration 63, loss = 0.09379799\n",
            "Iteration 64, loss = 0.09370627\n",
            "Iteration 65, loss = 0.09339833\n",
            "Iteration 66, loss = 0.09351776\n",
            "Iteration 67, loss = 0.09343429\n",
            "Iteration 68, loss = 0.09319758\n",
            "Iteration 69, loss = 0.09330894\n",
            "Iteration 70, loss = 0.09299472\n",
            "Iteration 71, loss = 0.09306284\n",
            "Iteration 72, loss = 0.09270971\n",
            "Iteration 73, loss = 0.09280298\n",
            "Iteration 74, loss = 0.09262473\n",
            "Iteration 75, loss = 0.09259776\n",
            "Iteration 76, loss = 0.09235769\n",
            "Iteration 77, loss = 0.09235847\n",
            "Iteration 78, loss = 0.09221020\n",
            "Iteration 79, loss = 0.09212106\n",
            "Iteration 80, loss = 0.09218403\n",
            "Iteration 81, loss = 0.09208064\n",
            "Iteration 82, loss = 0.09195901\n",
            "Iteration 83, loss = 0.09197209\n",
            "Iteration 84, loss = 0.09170463\n",
            "Iteration 85, loss = 0.09165827\n",
            "Iteration 86, loss = 0.09167560\n",
            "Iteration 87, loss = 0.09161322\n",
            "Iteration 88, loss = 0.09165372\n",
            "Iteration 89, loss = 0.09139371\n",
            "Iteration 90, loss = 0.09153647\n",
            "Iteration 91, loss = 0.09129439\n",
            "Iteration 92, loss = 0.09126770\n",
            "Iteration 93, loss = 0.09125972\n",
            "Iteration 94, loss = 0.09103865\n",
            "Iteration 95, loss = 0.09123231\n",
            "Iteration 96, loss = 0.09122123\n",
            "Iteration 97, loss = 0.09103304\n",
            "Iteration 98, loss = 0.09107993\n",
            "Iteration 99, loss = 0.09079367\n",
            "Iteration 100, loss = 0.09096719\n",
            "Iteration 101, loss = 0.09071527\n",
            "Iteration 102, loss = 0.09095425\n",
            "Iteration 103, loss = 0.09066680\n",
            "Iteration 104, loss = 0.09083100\n",
            "Iteration 105, loss = 0.09049213\n",
            "Iteration 106, loss = 0.09060596\n",
            "Iteration 107, loss = 0.09063047\n",
            "Iteration 108, loss = 0.09043985\n",
            "Iteration 109, loss = 0.09038104\n",
            "Iteration 110, loss = 0.09032700\n",
            "Iteration 111, loss = 0.09043646\n",
            "Iteration 112, loss = 0.09057981\n",
            "Iteration 113, loss = 0.09040632\n",
            "Iteration 114, loss = 0.09031492\n",
            "Iteration 115, loss = 0.09027341\n",
            "Iteration 116, loss = 0.09018365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.32723267\n",
            "Iteration 2, loss = 0.23249290\n",
            "Iteration 3, loss = 0.22829338\n",
            "Iteration 4, loss = 0.22560783\n",
            "Iteration 5, loss = 0.22306969\n",
            "Iteration 6, loss = 0.22053387\n",
            "Iteration 7, loss = 0.21815013\n",
            "Iteration 8, loss = 0.21518251\n",
            "Iteration 9, loss = 0.21274285\n",
            "Iteration 10, loss = 0.20991433\n",
            "Iteration 11, loss = 0.20657174\n",
            "Iteration 12, loss = 0.20340003\n",
            "Iteration 13, loss = 0.19935025\n",
            "Iteration 14, loss = 0.19525146\n",
            "Iteration 15, loss = 0.19089977\n",
            "Iteration 16, loss = 0.18625522\n",
            "Iteration 17, loss = 0.18106143\n",
            "Iteration 18, loss = 0.17556306\n",
            "Iteration 19, loss = 0.16964206\n",
            "Iteration 20, loss = 0.16409023\n",
            "Iteration 21, loss = 0.15846842\n",
            "Iteration 22, loss = 0.15300489\n",
            "Iteration 23, loss = 0.14686812\n",
            "Iteration 24, loss = 0.14203769\n",
            "Iteration 25, loss = 0.13671748\n",
            "Iteration 26, loss = 0.13226276\n",
            "Iteration 27, loss = 0.12794997\n",
            "Iteration 28, loss = 0.12352557\n",
            "Iteration 29, loss = 0.12016826\n",
            "Iteration 30, loss = 0.11700832\n",
            "Iteration 31, loss = 0.11378830\n",
            "Iteration 32, loss = 0.11151125\n",
            "Iteration 33, loss = 0.10933748\n",
            "Iteration 34, loss = 0.10738046\n",
            "Iteration 35, loss = 0.10579751\n",
            "Iteration 36, loss = 0.10438874\n",
            "Iteration 37, loss = 0.10288329\n",
            "Iteration 38, loss = 0.10171188\n",
            "Iteration 39, loss = 0.10067928\n",
            "Iteration 40, loss = 0.10000186\n",
            "Iteration 41, loss = 0.09930093\n",
            "Iteration 42, loss = 0.09854880\n",
            "Iteration 43, loss = 0.09775902\n",
            "Iteration 44, loss = 0.09741007\n",
            "Iteration 45, loss = 0.09694919\n",
            "Iteration 46, loss = 0.09631328\n",
            "Iteration 47, loss = 0.09592950\n",
            "Iteration 48, loss = 0.09558289\n",
            "Iteration 49, loss = 0.09511850\n",
            "Iteration 50, loss = 0.09510425\n",
            "Iteration 51, loss = 0.09474957\n",
            "Iteration 52, loss = 0.09446051\n",
            "Iteration 53, loss = 0.09435178\n",
            "Iteration 54, loss = 0.09413134\n",
            "Iteration 55, loss = 0.09376366\n",
            "Iteration 56, loss = 0.09368061\n",
            "Iteration 57, loss = 0.09347014\n",
            "Iteration 58, loss = 0.09317051\n",
            "Iteration 59, loss = 0.09329283\n",
            "Iteration 60, loss = 0.09323765\n",
            "Iteration 61, loss = 0.09271183\n",
            "Iteration 62, loss = 0.09268835\n",
            "Iteration 63, loss = 0.09247350\n",
            "Iteration 64, loss = 0.09237273\n",
            "Iteration 65, loss = 0.09220942\n",
            "Iteration 66, loss = 0.09216095\n",
            "Iteration 67, loss = 0.09195025\n",
            "Iteration 68, loss = 0.09197839\n",
            "Iteration 69, loss = 0.09154710\n",
            "Iteration 70, loss = 0.09172946\n",
            "Iteration 71, loss = 0.09145809\n",
            "Iteration 72, loss = 0.09147078\n",
            "Iteration 73, loss = 0.09132230\n",
            "Iteration 74, loss = 0.09130462\n",
            "Iteration 75, loss = 0.09130609\n",
            "Iteration 76, loss = 0.09117293\n",
            "Iteration 77, loss = 0.09101276\n",
            "Iteration 78, loss = 0.09107569\n",
            "Iteration 79, loss = 0.09076795\n",
            "Iteration 80, loss = 0.09084032\n",
            "Iteration 81, loss = 0.09081109\n",
            "Iteration 82, loss = 0.09073480\n",
            "Iteration 83, loss = 0.09059258\n",
            "Iteration 84, loss = 0.09025606\n",
            "Iteration 85, loss = 0.09032135\n",
            "Iteration 86, loss = 0.09038430\n",
            "Iteration 87, loss = 0.09032851\n",
            "Iteration 88, loss = 0.09024260\n",
            "Iteration 89, loss = 0.09038560\n",
            "Iteration 90, loss = 0.09028028\n",
            "Iteration 91, loss = 0.08986065\n",
            "Iteration 92, loss = 0.09017071\n",
            "Iteration 93, loss = 0.08998659\n",
            "Iteration 94, loss = 0.08964972\n",
            "Iteration 95, loss = 0.08981719\n",
            "Iteration 96, loss = 0.09005841\n",
            "Iteration 97, loss = 0.08957317\n",
            "Iteration 98, loss = 0.08974147\n",
            "Iteration 99, loss = 0.08961979\n",
            "Iteration 100, loss = 0.08963940\n",
            "Iteration 101, loss = 0.08953178\n",
            "Iteration 102, loss = 0.08961393\n",
            "Iteration 103, loss = 0.08945707\n",
            "Iteration 104, loss = 0.08954620\n",
            "Iteration 105, loss = 0.08918059\n",
            "Iteration 106, loss = 0.08935570\n",
            "Iteration 107, loss = 0.08957234\n",
            "Iteration 108, loss = 0.08912735\n",
            "Iteration 109, loss = 0.08913312\n",
            "Iteration 110, loss = 0.08915523\n",
            "Iteration 111, loss = 0.08919806\n",
            "Iteration 112, loss = 0.08916519\n",
            "Iteration 113, loss = 0.08902531\n",
            "Iteration 114, loss = 0.08888877\n",
            "Iteration 115, loss = 0.08923527\n",
            "Iteration 116, loss = 0.08881446\n",
            "Iteration 117, loss = 0.08901859\n",
            "Iteration 118, loss = 0.08891717\n",
            "Iteration 119, loss = 0.08869692\n",
            "Iteration 120, loss = 0.08910168\n",
            "Iteration 121, loss = 0.08873702\n",
            "Iteration 122, loss = 0.08866901\n",
            "Iteration 123, loss = 0.08873762\n",
            "Iteration 124, loss = 0.08866756\n",
            "Iteration 125, loss = 0.08887044\n",
            "Iteration 126, loss = 0.08873859\n",
            "Iteration 127, loss = 0.08858223\n",
            "Iteration 128, loss = 0.08858620\n",
            "Iteration 129, loss = 0.08872545\n",
            "Iteration 130, loss = 0.08850389\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.32811934\n",
            "Iteration 2, loss = 0.23226589\n",
            "Iteration 3, loss = 0.22837347\n",
            "Iteration 4, loss = 0.22602941\n",
            "Iteration 5, loss = 0.22391785\n",
            "Iteration 6, loss = 0.22166626\n",
            "Iteration 7, loss = 0.21897841\n",
            "Iteration 8, loss = 0.21679390\n",
            "Iteration 9, loss = 0.21409583\n",
            "Iteration 10, loss = 0.21120736\n",
            "Iteration 11, loss = 0.20854618\n",
            "Iteration 12, loss = 0.20550057\n",
            "Iteration 13, loss = 0.20218357\n",
            "Iteration 14, loss = 0.19824631\n",
            "Iteration 15, loss = 0.19387881\n",
            "Iteration 16, loss = 0.18949574\n",
            "Iteration 17, loss = 0.18484700\n",
            "Iteration 18, loss = 0.17980464\n",
            "Iteration 19, loss = 0.17409575\n",
            "Iteration 20, loss = 0.16915963\n",
            "Iteration 21, loss = 0.16349514\n",
            "Iteration 22, loss = 0.15821460\n",
            "Iteration 23, loss = 0.15226865\n",
            "Iteration 24, loss = 0.14713040\n",
            "Iteration 25, loss = 0.14186413\n",
            "Iteration 26, loss = 0.13704725\n",
            "Iteration 27, loss = 0.13241812\n",
            "Iteration 28, loss = 0.12839473\n",
            "Iteration 29, loss = 0.12432693\n",
            "Iteration 30, loss = 0.12102231\n",
            "Iteration 31, loss = 0.11748495\n",
            "Iteration 32, loss = 0.11524712\n",
            "Iteration 33, loss = 0.11248637\n",
            "Iteration 34, loss = 0.11030925\n",
            "Iteration 35, loss = 0.10836082\n",
            "Iteration 36, loss = 0.10656810\n",
            "Iteration 37, loss = 0.10539869\n",
            "Iteration 38, loss = 0.10386482\n",
            "Iteration 39, loss = 0.10292697\n",
            "Iteration 40, loss = 0.10212530\n",
            "Iteration 41, loss = 0.10081818\n",
            "Iteration 42, loss = 0.10037098\n",
            "Iteration 43, loss = 0.09933320\n",
            "Iteration 44, loss = 0.09890107\n",
            "Iteration 45, loss = 0.09845199\n",
            "Iteration 46, loss = 0.09792918\n",
            "Iteration 47, loss = 0.09748022\n",
            "Iteration 48, loss = 0.09736072\n",
            "Iteration 49, loss = 0.09691711\n",
            "Iteration 50, loss = 0.09645217\n",
            "Iteration 51, loss = 0.09603278\n",
            "Iteration 52, loss = 0.09558218\n",
            "Iteration 53, loss = 0.09529147\n",
            "Iteration 54, loss = 0.09516475\n",
            "Iteration 55, loss = 0.09500648\n",
            "Iteration 56, loss = 0.09458844\n",
            "Iteration 57, loss = 0.09446381\n",
            "Iteration 58, loss = 0.09445308\n",
            "Iteration 59, loss = 0.09409717\n",
            "Iteration 60, loss = 0.09403434\n",
            "Iteration 61, loss = 0.09400962\n",
            "Iteration 62, loss = 0.09344171\n",
            "Iteration 63, loss = 0.09355119\n",
            "Iteration 64, loss = 0.09336381\n",
            "Iteration 65, loss = 0.09323356\n",
            "Iteration 66, loss = 0.09292663\n",
            "Iteration 67, loss = 0.09311621\n",
            "Iteration 68, loss = 0.09284663\n",
            "Iteration 69, loss = 0.09263678\n",
            "Iteration 70, loss = 0.09260945\n",
            "Iteration 71, loss = 0.09258857\n",
            "Iteration 72, loss = 0.09269788\n",
            "Iteration 73, loss = 0.09206569\n",
            "Iteration 74, loss = 0.09217236\n",
            "Iteration 75, loss = 0.09218360\n",
            "Iteration 76, loss = 0.09206085\n",
            "Iteration 77, loss = 0.09187040\n",
            "Iteration 78, loss = 0.09193238\n",
            "Iteration 79, loss = 0.09192141\n",
            "Iteration 80, loss = 0.09169963\n",
            "Iteration 81, loss = 0.09156933\n",
            "Iteration 82, loss = 0.09148709\n",
            "Iteration 83, loss = 0.09146540\n",
            "Iteration 84, loss = 0.09139249\n",
            "Iteration 85, loss = 0.09128451\n",
            "Iteration 86, loss = 0.09110379\n",
            "Iteration 87, loss = 0.09128326\n",
            "Iteration 88, loss = 0.09124382\n",
            "Iteration 89, loss = 0.09089210\n",
            "Iteration 90, loss = 0.09102165\n",
            "Iteration 91, loss = 0.09099327\n",
            "Iteration 92, loss = 0.09083781\n",
            "Iteration 93, loss = 0.09070185\n",
            "Iteration 94, loss = 0.09088194\n",
            "Iteration 95, loss = 0.09107342\n",
            "Iteration 96, loss = 0.09057369\n",
            "Iteration 97, loss = 0.09057444\n",
            "Iteration 98, loss = 0.09036175\n",
            "Iteration 99, loss = 0.09065425\n",
            "Iteration 100, loss = 0.09027082\n",
            "Iteration 101, loss = 0.09052170\n",
            "Iteration 102, loss = 0.09028395\n",
            "Iteration 103, loss = 0.09018992\n",
            "Iteration 104, loss = 0.09016354\n",
            "Iteration 105, loss = 0.09030853\n",
            "Iteration 106, loss = 0.09005546\n",
            "Iteration 107, loss = 0.09008715\n",
            "Iteration 108, loss = 0.09003748\n",
            "Iteration 109, loss = 0.09019469\n",
            "Iteration 110, loss = 0.09001471\n",
            "Iteration 111, loss = 0.08996893\n",
            "Iteration 112, loss = 0.09010241\n",
            "Iteration 113, loss = 0.08999510\n",
            "Iteration 114, loss = 0.08990387\n",
            "Iteration 115, loss = 0.08987937\n",
            "Iteration 116, loss = 0.08966930\n",
            "Iteration 117, loss = 0.08969364\n",
            "Iteration 118, loss = 0.08989282\n",
            "Iteration 119, loss = 0.08962453\n",
            "Iteration 120, loss = 0.08964083\n",
            "Iteration 121, loss = 0.08961879\n",
            "Iteration 122, loss = 0.08972958\n",
            "Iteration 123, loss = 0.08956513\n",
            "Iteration 124, loss = 0.08954707\n",
            "Iteration 125, loss = 0.08958012\n",
            "Iteration 126, loss = 0.08959725\n",
            "Iteration 127, loss = 0.08960295\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.31844074\n",
            "Iteration 2, loss = 0.21974226\n",
            "Iteration 3, loss = 0.21401214\n",
            "Iteration 4, loss = 0.20996896\n",
            "Iteration 5, loss = 0.20642721\n",
            "Iteration 6, loss = 0.20271204\n",
            "Iteration 7, loss = 0.19899119\n",
            "Iteration 8, loss = 0.19535509\n",
            "Iteration 9, loss = 0.19112106\n",
            "Iteration 10, loss = 0.18675958\n",
            "Iteration 11, loss = 0.18181882\n",
            "Iteration 12, loss = 0.17576023\n",
            "Iteration 13, loss = 0.16942161\n",
            "Iteration 14, loss = 0.16182359\n",
            "Iteration 15, loss = 0.15353648\n",
            "Iteration 16, loss = 0.14438553\n",
            "Iteration 17, loss = 0.13431217\n",
            "Iteration 18, loss = 0.12404729\n",
            "Iteration 19, loss = 0.11291072\n",
            "Iteration 20, loss = 0.10204746\n",
            "Iteration 21, loss = 0.09101332\n",
            "Iteration 22, loss = 0.08051659\n",
            "Iteration 23, loss = 0.07076127\n",
            "Iteration 24, loss = 0.06169537\n",
            "Iteration 25, loss = 0.05351242\n",
            "Iteration 26, loss = 0.04589009\n",
            "Iteration 27, loss = 0.03954849\n",
            "Iteration 28, loss = 0.03392390\n",
            "Iteration 29, loss = 0.02900256\n",
            "Iteration 30, loss = 0.02477098\n",
            "Iteration 31, loss = 0.02125839\n",
            "Iteration 32, loss = 0.01808434\n",
            "Iteration 33, loss = 0.01539755\n",
            "Iteration 34, loss = 0.01324168\n",
            "Iteration 35, loss = 0.01142336\n",
            "Iteration 36, loss = 0.00981720\n",
            "Iteration 37, loss = 0.00849864\n",
            "Iteration 38, loss = 0.00743478\n",
            "Iteration 39, loss = 0.00650018\n",
            "Iteration 40, loss = 0.00578480\n",
            "Iteration 41, loss = 0.00509731\n",
            "Iteration 42, loss = 0.00453194\n",
            "Iteration 43, loss = 0.00409990\n",
            "Iteration 44, loss = 0.00364706\n",
            "Iteration 45, loss = 0.00334164\n",
            "Iteration 46, loss = 0.00300533\n",
            "Iteration 47, loss = 0.00279734\n",
            "Iteration 48, loss = 0.00258241\n",
            "Iteration 49, loss = 0.00251464\n",
            "Iteration 50, loss = 0.00231866\n",
            "Iteration 51, loss = 0.00218860\n",
            "Iteration 52, loss = 0.00205503\n",
            "Iteration 53, loss = 0.00194356\n",
            "Iteration 54, loss = 0.00192948\n",
            "Iteration 55, loss = 0.00186970\n",
            "Iteration 56, loss = 0.00186060\n",
            "Iteration 57, loss = 0.00174120\n",
            "Iteration 58, loss = 0.00167469\n",
            "Iteration 59, loss = 0.00173669\n",
            "Iteration 60, loss = 0.00167136\n",
            "Iteration 61, loss = 0.00161351\n",
            "Iteration 62, loss = 0.00158991\n",
            "Iteration 63, loss = 0.00163267\n",
            "Iteration 64, loss = 0.00156660\n",
            "Iteration 65, loss = 0.00162938\n",
            "Iteration 66, loss = 0.00151926\n",
            "Iteration 67, loss = 0.00155684\n",
            "Iteration 68, loss = 0.00147472\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.31768693\n",
            "Iteration 2, loss = 0.21816413\n",
            "Iteration 3, loss = 0.21222637\n",
            "Iteration 4, loss = 0.20819704\n",
            "Iteration 5, loss = 0.20431711\n",
            "Iteration 6, loss = 0.20045801\n",
            "Iteration 7, loss = 0.19662013\n",
            "Iteration 8, loss = 0.19226461\n",
            "Iteration 9, loss = 0.18813136\n",
            "Iteration 10, loss = 0.18330996\n",
            "Iteration 11, loss = 0.17783339\n",
            "Iteration 12, loss = 0.17184770\n",
            "Iteration 13, loss = 0.16472890\n",
            "Iteration 14, loss = 0.15704144\n",
            "Iteration 15, loss = 0.14840748\n",
            "Iteration 16, loss = 0.13900728\n",
            "Iteration 17, loss = 0.12877884\n",
            "Iteration 18, loss = 0.11793821\n",
            "Iteration 19, loss = 0.10680095\n",
            "Iteration 20, loss = 0.09595621\n",
            "Iteration 21, loss = 0.08524343\n",
            "Iteration 22, loss = 0.07524488\n",
            "Iteration 23, loss = 0.06531062\n",
            "Iteration 24, loss = 0.05697875\n",
            "Iteration 25, loss = 0.04905232\n",
            "Iteration 26, loss = 0.04213215\n",
            "Iteration 27, loss = 0.03611426\n",
            "Iteration 28, loss = 0.03064552\n",
            "Iteration 29, loss = 0.02615864\n",
            "Iteration 30, loss = 0.02225247\n",
            "Iteration 31, loss = 0.01882695\n",
            "Iteration 32, loss = 0.01603986\n",
            "Iteration 33, loss = 0.01361855\n",
            "Iteration 34, loss = 0.01154937\n",
            "Iteration 35, loss = 0.00993010\n",
            "Iteration 36, loss = 0.00852969\n",
            "Iteration 37, loss = 0.00737386\n",
            "Iteration 38, loss = 0.00630850\n",
            "Iteration 39, loss = 0.00546451\n",
            "Iteration 40, loss = 0.00478820\n",
            "Iteration 41, loss = 0.00430725\n",
            "Iteration 42, loss = 0.00390916\n",
            "Iteration 43, loss = 0.00344614\n",
            "Iteration 44, loss = 0.00317523\n",
            "Iteration 45, loss = 0.00292384\n",
            "Iteration 46, loss = 0.00261099\n",
            "Iteration 47, loss = 0.00241338\n",
            "Iteration 48, loss = 0.00228669\n",
            "Iteration 49, loss = 0.00220811\n",
            "Iteration 50, loss = 0.00205883\n",
            "Iteration 51, loss = 0.00191511\n",
            "Iteration 52, loss = 0.00176420\n",
            "Iteration 53, loss = 0.00166205\n",
            "Iteration 54, loss = 0.00172244\n",
            "Iteration 55, loss = 0.00163606\n",
            "Iteration 56, loss = 0.00154331\n",
            "Iteration 57, loss = 0.00157474\n",
            "Iteration 58, loss = 0.00144158\n",
            "Iteration 59, loss = 0.00140371\n",
            "Iteration 60, loss = 0.00139883\n",
            "Iteration 61, loss = 0.00143316\n",
            "Iteration 62, loss = 0.00136523\n",
            "Iteration 63, loss = 0.00127783\n",
            "Iteration 64, loss = 0.00127905\n",
            "Iteration 65, loss = 0.00137643\n",
            "Iteration 66, loss = 0.00123955\n",
            "Iteration 67, loss = 0.00128244\n",
            "Iteration 68, loss = 0.00124682\n",
            "Iteration 69, loss = 0.00124247\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.31833387\n",
            "Iteration 2, loss = 0.21776902\n",
            "Iteration 3, loss = 0.21215620\n",
            "Iteration 4, loss = 0.20846580\n",
            "Iteration 5, loss = 0.20503944\n",
            "Iteration 6, loss = 0.20146492\n",
            "Iteration 7, loss = 0.19757621\n",
            "Iteration 8, loss = 0.19393342\n",
            "Iteration 9, loss = 0.18974423\n",
            "Iteration 10, loss = 0.18511850\n",
            "Iteration 11, loss = 0.18023868\n",
            "Iteration 12, loss = 0.17454803\n",
            "Iteration 13, loss = 0.16815509\n",
            "Iteration 14, loss = 0.16073703\n",
            "Iteration 15, loss = 0.15225331\n",
            "Iteration 16, loss = 0.14325897\n",
            "Iteration 17, loss = 0.13337800\n",
            "Iteration 18, loss = 0.12287866\n",
            "Iteration 19, loss = 0.11169909\n",
            "Iteration 20, loss = 0.10094210\n",
            "Iteration 21, loss = 0.09004216\n",
            "Iteration 22, loss = 0.07965966\n",
            "Iteration 23, loss = 0.06958880\n",
            "Iteration 24, loss = 0.06060866\n",
            "Iteration 25, loss = 0.05252499\n",
            "Iteration 26, loss = 0.04535666\n",
            "Iteration 27, loss = 0.03882090\n",
            "Iteration 28, loss = 0.03330814\n",
            "Iteration 29, loss = 0.02836580\n",
            "Iteration 30, loss = 0.02431888\n",
            "Iteration 31, loss = 0.02054590\n",
            "Iteration 32, loss = 0.01763873\n",
            "Iteration 33, loss = 0.01511703\n",
            "Iteration 34, loss = 0.01284640\n",
            "Iteration 35, loss = 0.01105941\n",
            "Iteration 36, loss = 0.00955119\n",
            "Iteration 37, loss = 0.00822013\n",
            "Iteration 38, loss = 0.00711286\n",
            "Iteration 39, loss = 0.00622449\n",
            "Iteration 40, loss = 0.00550562\n",
            "Iteration 41, loss = 0.00482180\n",
            "Iteration 42, loss = 0.00430205\n",
            "Iteration 43, loss = 0.00381693\n",
            "Iteration 44, loss = 0.00337385\n",
            "Iteration 45, loss = 0.00305862\n",
            "Iteration 46, loss = 0.00275933\n",
            "Iteration 47, loss = 0.00255065\n",
            "Iteration 48, loss = 0.00231535\n",
            "Iteration 49, loss = 0.00218273\n",
            "Iteration 50, loss = 0.00209862\n",
            "Iteration 51, loss = 0.00190597\n",
            "Iteration 52, loss = 0.00181115\n",
            "Iteration 53, loss = 0.00167871\n",
            "Iteration 54, loss = 0.00162919\n",
            "Iteration 55, loss = 0.00157127\n",
            "Iteration 56, loss = 0.00155723\n",
            "Iteration 57, loss = 0.00153388\n",
            "Iteration 58, loss = 0.00144662\n",
            "Iteration 59, loss = 0.00142818\n",
            "Iteration 60, loss = 0.00140994\n",
            "Iteration 61, loss = 0.00137063\n",
            "Iteration 62, loss = 0.00139863\n",
            "Iteration 63, loss = 0.00128413\n",
            "Iteration 64, loss = 0.00128804\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.30655661\n",
            "Iteration 2, loss = 0.22001002\n",
            "Iteration 3, loss = 0.21473340\n",
            "Iteration 4, loss = 0.21078501\n",
            "Iteration 5, loss = 0.20605750\n",
            "Iteration 6, loss = 0.20036039\n",
            "Iteration 7, loss = 0.19381628\n",
            "Iteration 8, loss = 0.18586704\n",
            "Iteration 9, loss = 0.17518460\n",
            "Iteration 10, loss = 0.16270351\n",
            "Iteration 11, loss = 0.14672969\n",
            "Iteration 12, loss = 0.13034911\n",
            "Iteration 13, loss = 0.11201173\n",
            "Iteration 14, loss = 0.09492275\n",
            "Iteration 15, loss = 0.07914416\n",
            "Iteration 16, loss = 0.06418400\n",
            "Iteration 17, loss = 0.05212690\n",
            "Iteration 18, loss = 0.04194205\n",
            "Iteration 19, loss = 0.03407079\n",
            "Iteration 20, loss = 0.02751130\n",
            "Iteration 21, loss = 0.02364861\n",
            "Iteration 22, loss = 0.02018028\n",
            "Iteration 23, loss = 0.01824963\n",
            "Iteration 24, loss = 0.01714469\n",
            "Iteration 25, loss = 0.01555766\n",
            "Iteration 26, loss = 0.01456431\n",
            "Iteration 27, loss = 0.01424286\n",
            "Iteration 28, loss = 0.01369174\n",
            "Iteration 29, loss = 0.01327219\n",
            "Iteration 30, loss = 0.01219308\n",
            "Iteration 31, loss = 0.01217252\n",
            "Iteration 32, loss = 0.01111077\n",
            "Iteration 33, loss = 0.01011324\n",
            "Iteration 34, loss = 0.00924989\n",
            "Iteration 35, loss = 0.00846003\n",
            "Iteration 36, loss = 0.00783243\n",
            "Iteration 37, loss = 0.00877968\n",
            "Iteration 38, loss = 0.01743323\n",
            "Iteration 39, loss = 0.01976306\n",
            "Iteration 40, loss = 0.01214808\n",
            "Iteration 41, loss = 0.00878999\n",
            "Iteration 42, loss = 0.00764657\n",
            "Iteration 43, loss = 0.00719746\n",
            "Iteration 44, loss = 0.00701864\n",
            "Iteration 45, loss = 0.00713328\n",
            "Iteration 46, loss = 0.00687896\n",
            "Iteration 47, loss = 0.00688975\n",
            "Iteration 48, loss = 0.00701599\n",
            "Iteration 49, loss = 0.02364621\n",
            "Iteration 50, loss = 0.02082399\n",
            "Iteration 51, loss = 0.01068323\n",
            "Iteration 52, loss = 0.00750252\n",
            "Iteration 53, loss = 0.00688918\n",
            "Iteration 54, loss = 0.00672575\n",
            "Iteration 55, loss = 0.00684420\n",
            "Iteration 56, loss = 0.00661251\n",
            "Iteration 57, loss = 0.00652178\n",
            "Iteration 58, loss = 0.00648428\n",
            "Iteration 59, loss = 0.01014448\n",
            "Iteration 60, loss = 0.02458973\n",
            "Iteration 61, loss = 0.01328254\n",
            "Iteration 62, loss = 0.00837915\n",
            "Iteration 63, loss = 0.00700783\n",
            "Iteration 64, loss = 0.00656702\n",
            "Iteration 65, loss = 0.00659366\n",
            "Iteration 66, loss = 0.00646676\n",
            "Iteration 67, loss = 0.00634617\n",
            "Iteration 68, loss = 0.00638026\n",
            "Iteration 69, loss = 0.00636383\n",
            "Iteration 70, loss = 0.00646191\n",
            "Iteration 71, loss = 0.01680936\n",
            "Iteration 72, loss = 0.01984518\n",
            "Iteration 73, loss = 0.01021285\n",
            "Iteration 74, loss = 0.00715348\n",
            "Iteration 75, loss = 0.00658367\n",
            "Iteration 76, loss = 0.00636598\n",
            "Iteration 77, loss = 0.00627286\n",
            "Iteration 78, loss = 0.00632086\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.30567897\n",
            "Iteration 2, loss = 0.21811136\n",
            "Iteration 3, loss = 0.21306021\n",
            "Iteration 4, loss = 0.20751522\n",
            "Iteration 5, loss = 0.20001981\n",
            "Iteration 6, loss = 0.19143190\n",
            "Iteration 7, loss = 0.18221608\n",
            "Iteration 8, loss = 0.17153810\n",
            "Iteration 9, loss = 0.15970513\n",
            "Iteration 10, loss = 0.14576319\n",
            "Iteration 11, loss = 0.12956409\n",
            "Iteration 12, loss = 0.11380187\n",
            "Iteration 13, loss = 0.09681559\n",
            "Iteration 14, loss = 0.08121777\n",
            "Iteration 15, loss = 0.06645990\n",
            "Iteration 16, loss = 0.05378851\n",
            "Iteration 17, loss = 0.04408590\n",
            "Iteration 18, loss = 0.03605414\n",
            "Iteration 19, loss = 0.02909148\n",
            "Iteration 20, loss = 0.02482929\n",
            "Iteration 21, loss = 0.02264686\n",
            "Iteration 22, loss = 0.01978188\n",
            "Iteration 23, loss = 0.01890415\n",
            "Iteration 24, loss = 0.01788843\n",
            "Iteration 25, loss = 0.01620740\n",
            "Iteration 26, loss = 0.01433966\n",
            "Iteration 27, loss = 0.01371704\n",
            "Iteration 28, loss = 0.01265143\n",
            "Iteration 29, loss = 0.01261543\n",
            "Iteration 30, loss = 0.01085584\n",
            "Iteration 31, loss = 0.00959447\n",
            "Iteration 32, loss = 0.00934770\n",
            "Iteration 33, loss = 0.00929211\n",
            "Iteration 34, loss = 0.01447726\n",
            "Iteration 35, loss = 0.01719565\n",
            "Iteration 36, loss = 0.01328324\n",
            "Iteration 37, loss = 0.00935725\n",
            "Iteration 38, loss = 0.00791674\n",
            "Iteration 39, loss = 0.00729082\n",
            "Iteration 40, loss = 0.00718133\n",
            "Iteration 41, loss = 0.00680193\n",
            "Iteration 42, loss = 0.00661867\n",
            "Iteration 43, loss = 0.00661988\n",
            "Iteration 44, loss = 0.00887854\n",
            "Iteration 45, loss = 0.03190396\n",
            "Iteration 46, loss = 0.01669477\n",
            "Iteration 47, loss = 0.00917648\n",
            "Iteration 48, loss = 0.00711017\n",
            "Iteration 49, loss = 0.00666961\n",
            "Iteration 50, loss = 0.00644123\n",
            "Iteration 51, loss = 0.00631928\n",
            "Iteration 52, loss = 0.00628410\n",
            "Iteration 53, loss = 0.00614429\n",
            "Iteration 54, loss = 0.00615332\n",
            "Iteration 55, loss = 0.00642092\n",
            "Iteration 56, loss = 0.02301504\n",
            "Iteration 57, loss = 0.01948161\n",
            "Iteration 58, loss = 0.00958563\n",
            "Iteration 59, loss = 0.00697755\n",
            "Iteration 60, loss = 0.00632060\n",
            "Iteration 61, loss = 0.00614801\n",
            "Iteration 62, loss = 0.00604859\n",
            "Iteration 63, loss = 0.00592361\n",
            "Iteration 64, loss = 0.00593753\n",
            "Iteration 65, loss = 0.00595137\n",
            "Iteration 66, loss = 0.00582637\n",
            "Iteration 67, loss = 0.00568611\n",
            "Iteration 68, loss = 0.01067689\n",
            "Iteration 69, loss = 0.02734990\n",
            "Iteration 70, loss = 0.01256195\n",
            "Iteration 71, loss = 0.00726684\n",
            "Iteration 72, loss = 0.00617852\n",
            "Iteration 73, loss = 0.00590108\n",
            "Iteration 74, loss = 0.00577753\n",
            "Iteration 75, loss = 0.00569708\n",
            "Iteration 76, loss = 0.00575269\n",
            "Iteration 77, loss = 0.00554978\n",
            "Iteration 78, loss = 0.00554237\n",
            "Iteration 79, loss = 0.00558103\n",
            "Iteration 80, loss = 0.01607166\n",
            "Iteration 81, loss = 0.02079987\n",
            "Iteration 82, loss = 0.00991617\n",
            "Iteration 83, loss = 0.00652121\n",
            "Iteration 84, loss = 0.00580469\n",
            "Iteration 85, loss = 0.00574281\n",
            "Iteration 86, loss = 0.00564623\n",
            "Iteration 87, loss = 0.00549213\n",
            "Iteration 88, loss = 0.00553638\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.30573445\n",
            "Iteration 2, loss = 0.21831436\n",
            "Iteration 3, loss = 0.21340455\n",
            "Iteration 4, loss = 0.20934982\n",
            "Iteration 5, loss = 0.20459737\n",
            "Iteration 6, loss = 0.19927994\n",
            "Iteration 7, loss = 0.19259245\n",
            "Iteration 8, loss = 0.18514120\n",
            "Iteration 9, loss = 0.17501241\n",
            "Iteration 10, loss = 0.16311336\n",
            "Iteration 11, loss = 0.14885790\n",
            "Iteration 12, loss = 0.13315286\n",
            "Iteration 13, loss = 0.11670051\n",
            "Iteration 14, loss = 0.09986438\n",
            "Iteration 15, loss = 0.08361620\n",
            "Iteration 16, loss = 0.06877311\n",
            "Iteration 17, loss = 0.05627077\n",
            "Iteration 18, loss = 0.04506447\n",
            "Iteration 19, loss = 0.03611075\n",
            "Iteration 20, loss = 0.02932091\n",
            "Iteration 21, loss = 0.02601654\n",
            "Iteration 22, loss = 0.02298991\n",
            "Iteration 23, loss = 0.02059558\n",
            "Iteration 24, loss = 0.01785106\n",
            "Iteration 25, loss = 0.01529245\n",
            "Iteration 26, loss = 0.01346108\n",
            "Iteration 27, loss = 0.01258816\n",
            "Iteration 28, loss = 0.01606820\n",
            "Iteration 29, loss = 0.01865823\n",
            "Iteration 30, loss = 0.01445834\n",
            "Iteration 31, loss = 0.01194953\n",
            "Iteration 32, loss = 0.00968010\n",
            "Iteration 33, loss = 0.00870886\n",
            "Iteration 34, loss = 0.00819449\n",
            "Iteration 35, loss = 0.00780617\n",
            "Iteration 36, loss = 0.00769868\n",
            "Iteration 37, loss = 0.01163962\n",
            "Iteration 38, loss = 0.03145175\n",
            "Iteration 39, loss = 0.01558746\n",
            "Iteration 40, loss = 0.00934108\n",
            "Iteration 41, loss = 0.00773455\n",
            "Iteration 42, loss = 0.00742396\n",
            "Iteration 43, loss = 0.00734064\n",
            "Iteration 44, loss = 0.00697464\n",
            "Iteration 45, loss = 0.00710548\n",
            "Iteration 46, loss = 0.00705801\n",
            "Iteration 47, loss = 0.01809860\n",
            "Iteration 48, loss = 0.02324823\n",
            "Iteration 49, loss = 0.01196477\n",
            "Iteration 50, loss = 0.00812986\n",
            "Iteration 51, loss = 0.00716383\n",
            "Iteration 52, loss = 0.00687877\n",
            "Iteration 53, loss = 0.00699187\n",
            "Iteration 54, loss = 0.00675729\n",
            "Iteration 55, loss = 0.00682084\n",
            "Iteration 56, loss = 0.00663699\n",
            "Iteration 57, loss = 0.00656713\n",
            "Iteration 58, loss = 0.00858235\n",
            "Iteration 59, loss = 0.02637170\n",
            "Iteration 60, loss = 0.01533535\n",
            "Iteration 61, loss = 0.00865806\n",
            "Iteration 62, loss = 0.00708389\n",
            "Iteration 63, loss = 0.00670724\n",
            "Iteration 64, loss = 0.00645603\n",
            "Iteration 65, loss = 0.00639159\n",
            "Iteration 66, loss = 0.00641071\n",
            "Iteration 67, loss = 0.00654100\n",
            "Iteration 68, loss = 0.00650443\n",
            "Iteration 69, loss = 0.00643842\n",
            "Iteration 70, loss = 0.01253975\n",
            "Iteration 71, loss = 0.02236481\n",
            "Iteration 72, loss = 0.01052014\n",
            "Iteration 73, loss = 0.00710668\n",
            "Iteration 74, loss = 0.00656224\n",
            "Iteration 75, loss = 0.00621871\n",
            "Iteration 76, loss = 0.00627157\n",
            "Iteration 77, loss = 0.00611362\n",
            "Iteration 78, loss = 0.00606690\n",
            "Iteration 79, loss = 0.00607856\n",
            "Iteration 80, loss = 0.00614365\n",
            "Iteration 81, loss = 0.00624533\n",
            "Iteration 82, loss = 0.02154804\n",
            "Iteration 83, loss = 0.01452771\n",
            "Iteration 84, loss = 0.00831464\n",
            "Iteration 85, loss = 0.00650325\n",
            "Iteration 86, loss = 0.00609255\n",
            "Iteration 87, loss = 0.00606807\n",
            "Iteration 88, loss = 0.00606028\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.29797264\n",
            "Iteration 2, loss = 0.23258674\n",
            "Iteration 3, loss = 0.22901469\n",
            "Iteration 4, loss = 0.22612057\n",
            "Iteration 5, loss = 0.22356697\n",
            "Iteration 6, loss = 0.22103602\n",
            "Iteration 7, loss = 0.21834235\n",
            "Iteration 8, loss = 0.21532637\n",
            "Iteration 9, loss = 0.21211577\n",
            "Iteration 10, loss = 0.20859647\n",
            "Iteration 11, loss = 0.20436971\n",
            "Iteration 12, loss = 0.19989881\n",
            "Iteration 13, loss = 0.19512405\n",
            "Iteration 14, loss = 0.19009352\n",
            "Iteration 15, loss = 0.18512652\n",
            "Iteration 16, loss = 0.17989275\n",
            "Iteration 17, loss = 0.17497942\n",
            "Iteration 18, loss = 0.16939793\n",
            "Iteration 19, loss = 0.16521599\n",
            "Iteration 20, loss = 0.16013826\n",
            "Iteration 21, loss = 0.15527730\n",
            "Iteration 22, loss = 0.15107107\n",
            "Iteration 23, loss = 0.14685120\n",
            "Iteration 24, loss = 0.14309561\n",
            "Iteration 25, loss = 0.13974802\n",
            "Iteration 26, loss = 0.13667786\n",
            "Iteration 27, loss = 0.13365210\n",
            "Iteration 28, loss = 0.13096592\n",
            "Iteration 29, loss = 0.12846054\n",
            "Iteration 30, loss = 0.12667299\n",
            "Iteration 31, loss = 0.12501912\n",
            "Iteration 32, loss = 0.12325375\n",
            "Iteration 33, loss = 0.12183935\n",
            "Iteration 34, loss = 0.12066177\n",
            "Iteration 35, loss = 0.11955402\n",
            "Iteration 36, loss = 0.11854457\n",
            "Iteration 37, loss = 0.11797215\n",
            "Iteration 38, loss = 0.11703537\n",
            "Iteration 39, loss = 0.11625460\n",
            "Iteration 40, loss = 0.11575214\n",
            "Iteration 41, loss = 0.11540960\n",
            "Iteration 42, loss = 0.11454929\n",
            "Iteration 43, loss = 0.11444319\n",
            "Iteration 44, loss = 0.11392015\n",
            "Iteration 45, loss = 0.11345612\n",
            "Iteration 46, loss = 0.11321031\n",
            "Iteration 47, loss = 0.11298379\n",
            "Iteration 48, loss = 0.11263801\n",
            "Iteration 49, loss = 0.11234814\n",
            "Iteration 50, loss = 0.11205120\n",
            "Iteration 51, loss = 0.11202005\n",
            "Iteration 52, loss = 0.11154123\n",
            "Iteration 53, loss = 0.11160068\n",
            "Iteration 54, loss = 0.11128908\n",
            "Iteration 55, loss = 0.11122849\n",
            "Iteration 56, loss = 0.11089289\n",
            "Iteration 57, loss = 0.11080159\n",
            "Iteration 58, loss = 0.11050983\n",
            "Iteration 59, loss = 0.11031115\n",
            "Iteration 60, loss = 0.11027642\n",
            "Iteration 61, loss = 0.11014193\n",
            "Iteration 62, loss = 0.10997492\n",
            "Iteration 63, loss = 0.10978354\n",
            "Iteration 64, loss = 0.10981482\n",
            "Iteration 65, loss = 0.10974693\n",
            "Iteration 66, loss = 0.10939711\n",
            "Iteration 67, loss = 0.10937695\n",
            "Iteration 68, loss = 0.10931384\n",
            "Iteration 69, loss = 0.10928122\n",
            "Iteration 70, loss = 0.10918855\n",
            "Iteration 71, loss = 0.10896062\n",
            "Iteration 72, loss = 0.10908349\n",
            "Iteration 73, loss = 0.10887796\n",
            "Iteration 74, loss = 0.10879968\n",
            "Iteration 75, loss = 0.10863505\n",
            "Iteration 76, loss = 0.10848055\n",
            "Iteration 77, loss = 0.10861227\n",
            "Iteration 78, loss = 0.10860782\n",
            "Iteration 79, loss = 0.10817868\n",
            "Iteration 80, loss = 0.10838706\n",
            "Iteration 81, loss = 0.10824567\n",
            "Iteration 82, loss = 0.10812350\n",
            "Iteration 83, loss = 0.10825358\n",
            "Iteration 84, loss = 0.10783784\n",
            "Iteration 85, loss = 0.10809627\n",
            "Iteration 86, loss = 0.10770111\n",
            "Iteration 87, loss = 0.10791178\n",
            "Iteration 88, loss = 0.10779503\n",
            "Iteration 89, loss = 0.10787652\n",
            "Iteration 90, loss = 0.10767568\n",
            "Iteration 91, loss = 0.10769621\n",
            "Iteration 92, loss = 0.10744599\n",
            "Iteration 93, loss = 0.10758771\n",
            "Iteration 94, loss = 0.10743036\n",
            "Iteration 95, loss = 0.10735956\n",
            "Iteration 96, loss = 0.10764612\n",
            "Iteration 97, loss = 0.10728541\n",
            "Iteration 98, loss = 0.10721779\n",
            "Iteration 99, loss = 0.10733475\n",
            "Iteration 100, loss = 0.10714973\n",
            "Iteration 101, loss = 0.10730335\n",
            "Iteration 102, loss = 0.10709168\n",
            "Iteration 103, loss = 0.10702383\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Mejores parametros: {'hidden_layer_sizes': (100,), 'alpha': 0.01}\n",
            "Mejor Score: 0.9094374954112725\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NX3R_8X8cma"
      },
      "source": [
        "Mejores parametros: {'hidden_layer_sizes': (100,), 'alpha': 0.01}\n",
        "\n",
        "Mejor Score: 0.9094374954112725\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "oqP_GAp-YumY",
        "outputId": "d6093704-c4ad-4859-c43f-5f6b7c1ac904"
      },
      "source": [
        "print(\"Score: \"+str(mlp_optimo.score(xtest,ytest))+'\\n')\n",
        "y_pred = mlp_optimo.predict(xtest)\n",
        "confusion(ytest,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.9115625\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-y6s7-9_xrS"
      },
      "source": [
        "Nota: Con la optimización de parámetros, no logramos una mejora considerable del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CLeXYwIkh09"
      },
      "source": [
        "#Respuesta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V5wLzp5lss7"
      },
      "source": [
        "El uso de una red neuronal (en este caso el perceptrón multicapa) para entrenar un modelo de aprendizaje automático para clasificación de revisiones de productos (problema de clasificación binaria) resultó igualmente efectivo que un algoritmo de clasificación \"tradicional\" como el LinearSVC.  Al momento de decidir por uno de los dos modelos, elegiría el LinearSVC por su velocidad de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3VY6laenJSe"
      },
      "source": [
        "#Conclusiones:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIcNNz9_qTRX"
      },
      "source": [
        "1. El **preprocesamiento de datos** es una etapa crucial a la hora de querer resolver un problema de aprendizaje automático.  A lo largo del curso, vimos  cómo un buen preprocesamiento de datos puede promover, en gran manera, el ajuste o la generalización que realizan los algoritmos: Rellenar valores faltantes, identificar y eliminar datos que pueden considerarse ruido, resolver redundancia, corregir inconsistencias, escalar datos, realizar transformaciones, convertir variables categóricas en variables numéricas, entre otros.\n",
        "\n",
        "  Cuando recibimos un conjunto de datos, además de encontrar valores numéricos, en muchos casos, debemos trabajar también con datos no estructurados como texto, imágenes o vídeos.  El problema de aprendizaje automático que enfrentamos en este caso de uso, requirió realizar preprocesamiento del texto de las revisiones de los productos:\n",
        "\n",
        "- Normalización: limpiar caracteres especiales y no deseados, pasar a minúsculas, tokenizar (separar el texto en palabras), lemmatizar (llevar las palabras a su raíz o a la forma base o diccionario de una palabra).\n",
        "\n",
        "- Vectorización: convertir los textos a números (Encoders o word embeddings). Estrategias como eliminación de stopwords y tf-idf pueden ayudar en muchos casos a determinar la relevancia de las palabras para trabajar con aquellas que ayuden a extraer la información contenida en los textos.\n",
        "\n",
        "  En nuestro caso de uso, conocer los datos nos permitió tomar la decisión de **agregar los títulos a las revisiones** para enriquecer el conjunto de palabras usadas para el entrenamiento, decidir **dejar la palabra \"no\"**, ya que las negaciones, en muchos casos, pueden dar el sentido negativo o positivo en una revisión y vectorizar las palabras usando **n-grams** para configurar expresiones de pares de palabras que fueran comunes en revisiones, tanto positivas como negativas.\n",
        "\n",
        "2. La **experimentación** es una buena alternativa para lograr obtener un buen modelo de aprendizaje automático.  Usar diferentes algoritmos y técnicas, permite considerar diferentes opciones para buscar la mejor aproximación a la solución de un problema.\n",
        "\n",
        "  En este caso de uso, pudimos seleccionar entre varios algoritmos para buscar el mejor modelo para clasificar las revisiones de productos, además, experimentamos con una red neuronal para validar su capacidad en el mencionado problema de clasificación.\n",
        "\n",
        "3. Como solución al problema de clasificación de las revisiones de películas en español, seleccionamos el modelo LinearSVC por su velocidad de entrenamiento.\n",
        "\n",
        "4. Existen aproximaciones mas complejas para abordar el problema de clasificación de nuestro caso de uso, sin embargo, preferimos una solución sencilla bien abordada, de acuerdo con los recursos de cómputo disponibles y los conocimientos adquiridos durante el curso, que otras mas ambiciosas y difíciles de sustentar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Wt4L-Sf6VV"
      },
      "source": [
        "---\n",
        "/// Para mejorar el modelo en una próxima iteración ///\n",
        "\n",
        "Para mejorar el modelo en una próxima iteración podríamos explorar las siguientes alternativas:\n",
        "\n",
        "1. **Experimentar otras arquitecturas de redes neuronales** (las RNN: redes neuronales recurrentes son ampliamente usadas para trabajar con texto), incluyendo modelos con mas capas ocultas.\n",
        "   Implementar las redes neuronales con Keras, que es una librería ampliamente usada, amigable para el usuario, modular y extensible.\n",
        "\n",
        "   El diseño de las redes neuronales puede no ser una tarea trivial, no es posible resumir el proceso de diseño de las capas ocultas con unas pocas reglas básicas. Investigadores han desarrollado muchas heurísticas de diseño para las capas ocultas, que ayudan a las personas a obtener el comportamiento deseado de sus redes, sin embargo, para nosotros, esto requiere un gran esfuerzo en investigación y en algunos casos, contar con equipos de cómputo preparados para procesar redes profundas o un esfuerzo adicional para determinar cómo compensar el número de capas ocultas con el tiempo necesario para entrenar las redes.\n",
        "\n",
        "   Lo que esperamos con esta experimentación es poder hallar una arquitectura de red que pueda generalizar mejor nuestro problema de clasificación y aumentar el accuracy del modelo actual.\n",
        "\n",
        "2. Para seguir evaluando el uso de embeddings, **probar otras incrustaciones** pre-entrenadas para español o entrenar nuestro propio embedding para verificar si nuestro algoritmo puede mejorar el score reemplazando las bolsas de palabras por incrustaciones.\n",
        "\n",
        "3. **Aumentar la cantidad de datos** de entrenamiento, para lo cuál, debemos contar con equipos de cómputo de mayores características.  Entrenar con mas datos, podría hacer que nuestro algoritmo generalice mejor.\n",
        "\n",
        "4. **Probar con ensambles** para validar si mejora el proceso de inferencia.\n",
        "\n",
        "  Combinar varios clasificadores en un ensamble, podría otorgarnos un modelo mas robusto y \"lucrarnos\" del esquema de votación del ensamble para mejorar la exactitud en la clasificación de los reviews.\n",
        "\n",
        "  Para esto, podríamos usar boosting con XGBoost (Extreme Gradient Boosting), que es un modelo eficiente y que genera múltiples modelos de predicción, los cuales se ejecutan secuencialmente y van mejorando progresivamente con base en los resultados del modelo anterior para mejorar y hallar mayor estabilidad en sus resultados.\n",
        "\n",
        "  Lo que esperamos con esta estrategia es mejorar la exactitud en las predicciones.\n",
        "\n",
        "5. Experimentar con modelos que corresponden mas al estado del arte en el procesamiento de lenguaje natural como son los **transformers**.  El modelo **BERT** (Bidirectional Encoder Representations from Transformers), ampliamente usado para tareas de análisis de sentimiento y Question answering (QA), podría ser una aproximación de mayor vanguardia para obtener un modelo mas robusto que generalice mejor ante nuevas revisiones nunca antes vistas.\n"
      ]
    }
  ]
}